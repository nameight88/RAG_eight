"""
ê¸ˆìœµê°ë…ì› ì œì¬ JSON ì½”í¼ìŠ¤ë¥¼ ì‹œë§¨í‹± ì²­í‚¹í•˜ê³  ë²¡í„° ì €ì¥ì†Œì— ì„ë² ë”©í•˜ëŠ” ëª¨ë“ˆ
- ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ì²­í¬ë¡œ ë¶„í• 
- ë²¡í„° ì„ë² ë”© ìƒì„±
- ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ì†Œ êµ¬ì¶•
"""

import os
import json
import re
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma, FAISS
# ì„ë² ë”© ëª¨ë¸ ì„í¬íŠ¸
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
import chromadb
from langchain.docstore.document import Document
import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class FSSSemanticChunker:
    """ê¸ˆìœµê°ë…ì› ì œì¬ ì •ë³´ë¥¼ ì‹œë§¨í‹± ì²­í‚¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        input_json: str,
        output_dir: str,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        use_openai_embeddings: bool = True,  # ê¸°ë³¸ê°’ì„ Trueë¡œ ë³€ê²½
        chunk_size: int = 512,
        chunk_overlap: int = 50,
        use_faiss: bool = False  # FAISS ì‚¬ìš© ì—¬ë¶€ ì¶”ê°€
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            input_json: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            model_name: ì‹œë§¨í‹± ì²­í‚¹ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„ (use_openai_embeddings=Falseì¸ ê²½ìš°)
            use_openai_embeddings: OpenAI ì„ë² ë”© API ì‚¬ìš© ì—¬ë¶€
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê²¹ì¹¨ í¬ê¸°
            use_faiss: FAISS ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš© ì—¬ë¶€
        """
        self.input_json = input_json
        self.output_dir = output_dir
        self.model_name = model_name
        self.use_openai_embeddings = use_openai_embeddings
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_faiss = use_faiss
        
        # OpenAI API í‚¤ ë¡œë“œ
        if use_openai_embeddings:
            self.openai_api_key = os.getenv('OPENAI_API_KEY')
            if not self.openai_api_key:
                print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. HuggingFace ì„ë² ë”©ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_openai_embeddings = False

        # Langchain í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        if use_openai_embeddings and self.openai_api_key:
            print(f"ğŸ§  OpenAI ì„ë² ë”© API ì´ˆê¸°í™” ì¤‘...")
            self.embeddings = OpenAIEmbeddings(
                model="text-embedding-3-large",
                openai_api_key=self.openai_api_key
            )
            print(f"âœ… OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            print(f"ğŸ§  HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {self.model_name}")
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.model_name,
                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            print(f"âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë°ì´í„° ë° ë²¡í„° ì €ì¥ì†Œ
        self.corpus = None
        self.db = None
    
    def load_corpus(self) -> List[Dict[str, Any]]:
        """JSON ì½”í¼ìŠ¤ ë¡œë“œ"""
        try:
            print(f"ğŸ“„ JSON ì½”í¼ìŠ¤ ë¡œë“œ ì¤‘: {self.input_json}")
            
            # ë””ë ‰í† ë¦¬ í™•ì¸
            input_dir = os.path.dirname(self.input_json)
            if not os.path.exists(input_dir):
                print(f"âš ï¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒì„±í•©ë‹ˆë‹¤: {input_dir}")
                os.makedirs(input_dir, exist_ok=True)
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(self.input_json):
                print(f"âš ï¸ JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.input_json}")
                print("í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                return self.create_sample_corpus()
            
            with open(self.input_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if "documents" in data:
                documents = data["documents"]
                print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
                self.corpus = documents
                return documents
            else:
                print("âŒ JSON íŒŒì¼ì— 'documents' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return self.create_sample_corpus()
        
        except Exception as e:
            print(f"âŒ ì½”í¼ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            return self.create_sample_corpus()
    
    def create_sample_corpus(self) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì½”í¼ìŠ¤ ìƒì„±"""
        print("ğŸ”„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        
        sample_documents = [
            {
                "doc_id": "SAMPLE_001",
                "source_file": "sample_file_1.pdf",
                "institution": "í…ŒìŠ¤íŠ¸ê¸ˆìœµì£¼ì‹íšŒì‚¬",
                "sanction_date": "2023.01.15",
                "content": {
                    "sanction_type": "ê³¼íƒœë£Œ",
                    "fine": {"amount": 5000000, "unit": "ì›", "text": "ê³¼íƒœë£Œ 5ë°±ë§Œì›"},
                    "executive_sanction": "CEO ì£¼ì˜ì  ê²½ê³ ",
                    "sanction_facts": [
                        {"title": "ì „ìê¸ˆìœµ ì•ˆì „ì¡°ì¹˜ ë¯¸í¡", "content": "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•¨"}
                    ],
                    "full_text": "í…ŒìŠ¤íŠ¸ê¸ˆìœµ(ì£¼)ëŠ” ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•˜ì˜€ìœ¼ë¯€ë¡œ ê³¼íƒœë£Œ 5ë°±ë§Œì›ì˜ ì œì¬ë¥¼ ê²°ì •í•¨"
                },
                "metadata": {
                    "doc_type": "ì œì¬ë‚´ìš©ê³µê°œ",
                    "char_count": 200,
                    "regulations": ["ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°", "ì „ìê¸ˆìœµê°ë…ê·œì • ì œ13ì¡°"]
                },
                "quality_score": 4
            },
            {
                "doc_id": "SAMPLE_002",
                "source_file": "sample_file_2.pdf",
                "institution": "ìƒ˜í”Œì€í–‰",
                "sanction_date": "2023.02.20",
                "content": {
                    "sanction_type": "ê¸°ê´€ê²½ê³ ",
                    "fine": {"amount": 0, "unit": "ì›", "text": ""},
                    "executive_sanction": "CIO ë¬¸ì±…ê²½ê³ ",
                    "sanction_facts": [
                        {"title": "ì •ë³´ë³´í˜¸ ì¸ë ¥ ë¶€ì¡±", "content": "ì „ìê¸ˆìœµê°ë…ê·œì •ì— ë”°ë¥¸ ì •ë³´ë³´í˜¸ ì¸ë ¥ì„ ì¶©ë¶„íˆ í™•ë³´í•˜ì§€ ì•Šê³  ìš´ì˜í•¨"},
                        {"title": "ë³´ì•ˆ ì·¨ì•½ì  ë°©ì¹˜", "content": "ì•Œë ¤ì§„ ë³´ì•ˆ ì·¨ì•½ì ì— ëŒ€í•œ ì¡°ì¹˜ë¥¼ 6ê°œì›” ì´ìƒ ì§€ì—°í•˜ì—¬ ì¡°ì¹˜í•¨"}
                    ],
                    "full_text": "ìƒ˜í”Œì€í–‰ì€ ì „ìê¸ˆìœµê°ë…ê·œì •ì— ë”°ë¥¸ ì •ë³´ë³´í˜¸ ì¸ë ¥ì„ ì¶©ë¶„íˆ í™•ë³´í•˜ì§€ ì•Šê³  ìš´ì˜í•˜ì˜€ìœ¼ë©°, ì•Œë ¤ì§„ ë³´ì•ˆ ì·¨ì•½ì ì— ëŒ€í•œ ì¡°ì¹˜ë¥¼ 6ê°œì›” ì´ìƒ ì§€ì—°í•˜ì—¬ ì¡°ì¹˜í•œ ì‚¬ì‹¤ì´ í™•ì¸ë˜ì–´ ê¸°ê´€ê²½ê³  ì¡°ì¹˜í•¨"
                },
                "metadata": {
                    "doc_type": "ì œì¬ë‚´ìš©ê³µê°œ",
                    "char_count": 300,
                    "regulations": ["ì „ìê¸ˆìœµê°ë…ê·œì • ì œ36ì¡°", "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì˜2"]
                },
                "quality_score": 5
            },
            {
                "doc_id": "SAMPLE_003",
                "source_file": "sample_file_3.hwp",
                "institution": "ì˜ˆì‹œë³´í—˜",
                "sanction_date": "2023.03.10",
                "content": {
                    "sanction_type": "ê³¼íƒœë£Œ",
                    "fine": {"amount": 10000000, "unit": "ì›", "text": "ê³¼íƒœë£Œ 1ì²œë§Œì›"},
                    "executive_sanction": "",
                    "sanction_facts": [
                        {"title": "ê°œì¸ì •ë³´ ìœ ì¶œì‚¬ê³ ", "content": "ê³ ê° ê°œì¸ì •ë³´ ê´€ë¦¬ ì†Œí™€ë¡œ ì¸í•´ ì•½ 1,000ëª…ì˜ ê³ ê°ì •ë³´ê°€ ì™¸ë¶€ì— ìœ ì¶œë¨"}
                    ],
                    "full_text": "ì˜ˆì‹œë³´í—˜ì€ ê³ ê° ê°œì¸ì •ë³´ ê´€ë¦¬ ì†Œí™€ë¡œ ì¸í•´ ì•½ 1,000ëª…ì˜ ê³ ê°ì •ë³´ê°€ ì™¸ë¶€ì— ìœ ì¶œëœ ì‚¬ì‹¤ì´ í™•ì¸ë˜ì–´ ê³¼íƒœë£Œ 1ì²œë§Œì›ì„ ë¶€ê³¼í•¨. í•´ë‹¹ ì‚¬ê³ ëŠ” ë‚´ë¶€ ì§ì›ì˜ ê´€ë¦¬ ì†Œí™€ ë° ì‹œìŠ¤í…œ ë³´ì•ˆ ì·¨ì•½ì ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ì˜€ìŒ."
                },
                "metadata": {
                    "doc_type": "ì œì¬ë‚´ìš©ê³µê°œ",
                    "char_count": 250,
                    "regulations": ["ê°œì¸ì •ë³´ë³´í˜¸ë²• ì œ29ì¡°", "ì‹ ìš©ì •ë³´ë²• ì œ19ì¡°"]
                },
                "quality_score": 3
            }
        ]
        
        # ìƒ˜í”Œ ë°ì´í„° ì €ì¥
        os.makedirs(os.path.dirname(self.input_json), exist_ok=True)
        
        with open(self.input_json, 'w', encoding='utf-8') as f:
            json.dump({
                "metadata": {
                    "source": "ìƒ˜í”Œ ë°ì´í„°",
                    "created_at": "2023-06-13"
                },
                "documents": sample_documents
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(sample_documents)}ê°œ ë¬¸ì„œ")
        return sample_documents
    
    def preprocess_document(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì„œ ì „ì²˜ë¦¬"""
        # í•„ìš”í•œ í•„ë“œ ì¶”ì¶œ
        processed = {
            "id": doc.get("doc_id", ""),
            "institution": doc.get("institution", ""),
            "sanction_date": doc.get("sanction_date", ""),
            "sanction_type": doc.get("content", {}).get("sanction_type", ""),
            "fine_amount": doc.get("content", {}).get("fine", {}).get("amount", 0),
            "fine_text": doc.get("content", {}).get("fine", {}).get("text", ""),
            "sanction_facts": doc.get("content", {}).get("sanction_facts", []),
            "full_text": doc.get("content", {}).get("full_text", ""),
            "regulations": doc.get("metadata", {}).get("regulations", []),
            "quality_score": doc.get("quality_score", 0),
        }
        
        # ì œì¬ ì‚¬ì‹¤ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        facts_text = ""
        for fact in processed["sanction_facts"]:
            if isinstance(fact, dict):
                title = fact.get("title", "")
                content = fact.get("content", "")
                if title and content:
                    facts_text += f"{title}\n{content}\n\n"
                elif title:
                    facts_text += f"{title}\n\n"
                elif content:
                    facts_text += f"{content}\n\n"
        
        # ê·œì • í…ìŠ¤íŠ¸ ìƒì„±
        regulations_text = "\n".join(processed["regulations"]) if processed["regulations"] else ""
        
        # í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
        combined_text = (
            f"ê¸ˆìœµê¸°ê´€: {processed['institution']}\n"
            f"ì œì¬ì¼ì: {processed['sanction_date']}\n"
            f"ì œì¬ìœ í˜•: {processed['sanction_type']}\n"
            f"ê³¼íƒœë£Œ: {processed['fine_text']}\n\n"
            f"ì œì¬ì‚¬ì‹¤:\n{facts_text}\n"
            f"ê´€ë ¨ê·œì •:\n{regulations_text}\n\n"
            f"{processed['full_text']}"
        )
        
        processed["combined_text"] = combined_text
        return processed
    
    def create_semantic_chunks(self, preprocessed_docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì‹œë§¨í‹± ì²­í‚¹ ìˆ˜í–‰"""
        all_chunks = []
        
        print(f"ğŸ”ª {len(preprocessed_docs)}ê°œ ë¬¸ì„œ ì²­í‚¹ ì‹œì‘...")
        for doc in tqdm(preprocessed_docs):
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.text_splitter.split_text(doc["combined_text"])
            
            # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            doc_chunks = []
            for i, chunk_text in enumerate(chunks):
                chunk = {
                    "id": f"{doc['id']}-chunk-{i}",
                    "doc_id": doc["id"],
                    "institution": doc["institution"],
                    "sanction_date": doc["sanction_date"],
                    "sanction_type": doc["sanction_type"],
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "chunk_text": chunk_text,
                    "quality_score": doc["quality_score"]
                }
                doc_chunks.append(chunk)
            
            all_chunks.extend(doc_chunks)
        
        print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks
    
    def create_vector_store(self, batch_size: int = 100):
        """ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ ì¶”ê°€)"""
        print("ğŸ”¢ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        output_json = os.path.join(self.output_dir, "fss_sanctions_parsed.json")
        
        try:
            # ë°ì´í„° ë¡œë“œ
            with open(output_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë¬¸ì„œ ë°°ì—´ ìƒì„±
            documents = []
            for item in data:
                for chunk in item['chunks']:
                    doc = Document(
                        page_content=chunk['content'],
                        metadata={
                            'id': chunk['id'],
                            'institution': item['institution'],
                            'sanction_date': item['sanction_date'],
                            'sanction_type': item['sanction_type'],
                            'violation_regulation': item.get('violation_regulation', '')
                        }
                    )
                    documents.append(doc)
            
            print(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
            
            # ì„ë² ë”© ì´ˆê¸°í™”
            if self.use_openai_embeddings:
                print("ğŸ§  OpenAI ì„ë² ë”© API ì´ˆê¸°í™” ì¤‘...")
                embeddings = OpenAIEmbeddings(
                    #model="text-embedding-3-small",
                    model="text-embedding-3-large",
                    openai_api_key=self.openai_api_key
                )
            else:
                print(f"ğŸ§  HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {self.model_name}")
                embeddings = HuggingFaceEmbeddings(
                    model_name=self.model_name,
                    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
            
            # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            if self.use_faiss:
                # FAISS ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬
                faiss_dir = os.path.join(self.output_dir, "faiss")
                os.makedirs(faiss_dir, exist_ok=True)
                
                print(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘: {faiss_dir}")
                
                # ì´ˆê¸° ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ì²« ë°°ì¹˜ ì‚¬ìš©)
                first_batch = documents[:batch_size]
                vectorstore = FAISS.from_documents(
                    documents=first_batch,
                    embedding=embeddings
                )
                
                # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì¶”ê°€
                for i in tqdm(range(batch_size, len(documents), batch_size), desc="FAISSì— ë¬¸ì„œ ì¶”ê°€ ì¤‘"):
                    batch = documents[i:i+batch_size]
                    vectorstore.add_documents(batch)
                
                # ë¡œì»¬ íŒŒì¼ì— ì €ì¥
                vectorstore.save_local(faiss_dir)
                self.db = vectorstore
                print(f"âœ… FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")
                
                # ë²¡í„° ì €ì¥ì†Œ ì •ë³´ ì €ì¥
                vector_store_info = {
                    'created_at': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'document_count': len(documents),
                    'embed_model': self.model_name if not self.use_openai_embeddings else 'text-embedding-3-large',
                    'use_openai': self.use_openai_embeddings,
                    'vector_store_type': 'FAISS'
                }
                
                with open(os.path.join(self.output_dir, 'vector_store_info.json'), 'w', encoding='utf-8') as f:
                    json.dump(vector_store_info, f, ensure_ascii=False, indent=2)
                
            else:
                # Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                print(f"Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘: {self.output_dir}")
                vectorstore = Chroma.from_documents(
                    documents=documents,
                    embedding=embeddings,
                    persist_directory=self.output_dir
                )
                vectorstore.persist()
                
                print(f"âœ… Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")
                
                # ë²¡í„° ì €ì¥ì†Œ ì •ë³´ ì €ì¥
                vector_store_info = {
                    'created_at': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'document_count': len(documents),
                    'embed_model': self.model_name if not self.use_openai_embeddings else 'text-embedding-3-large',
                    'use_openai': self.use_openai_embeddings,
                    'vector_store_type': 'Chroma'
                }
                
                with open(os.path.join(self.output_dir, 'vector_store_info.json'), 'w', encoding='utf-8') as f:
                    json.dump(vector_store_info, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š ë²¡í„° ì €ì¥ì†Œ ì •ë³´: {len(documents)}ê°œ ë¬¸ì„œ ì„ë² ë”©")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def test_query(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜í–‰"""
        if not self.db:
            print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ” ì¿¼ë¦¬: '{query}'")
        results = self.db.similarity_search_with_score(query, k=k)
        
        formatted_results = []
        for doc, score in results:
            result = {
                "content": doc.page_content,
                "metadata": doc.metadata,
                "score": float(score)
            }
            formatted_results.append(result)
        
        return formatted_results
    
    def process(self) -> None:
        """ì „ì²´ ì²˜ë¦¬ ê³¼ì • ì‹¤í–‰"""
        # 1. ì½”í¼ìŠ¤ ë¡œë“œ
        documents = self.load_corpus()
        if not documents:
            return
        
        # 2. ë¬¸ì„œ ì „ì²˜ë¦¬
        print("ğŸ”„ ë¬¸ì„œ ì „ì²˜ë¦¬ ì¤‘...")
        preprocessed_docs = [self.preprocess_document(doc) for doc in tqdm(documents)]
        
        # 3. ì‹œë§¨í‹± ì²­í‚¹
        chunks = self.create_semantic_chunks(preprocessed_docs)
        # 3.5. ì²­í¬ ê²°ê³¼ ì €ì¥
        output_json_path = os.path.join(self.output_dir, "fss_sanctions_parsed.json")
        os.makedirs(self.output_dir, exist_ok=True)
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(
                [
                    {
                        "institution": doc["institution"],
                        "sanction_date": doc["sanction_date"],
                        "sanction_type": doc["sanction_type"],
                        "chunks": [
                            {
                                "id": chunk["id"],
                                "content": chunk["chunk_text"]
                            }
                            for chunk in chunks if chunk["doc_id"] == doc["id"]
                        ]
                    }
                    for doc in preprocessed_docs
                ],
                f,
                ensure_ascii=False,
                indent=2
            )
        print("âœ… ì²­í¬ JSON ì €ì¥ ì™„ë£Œ")
        # 4. ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
        self.create_vector_store(batch_size=100)
        
        print(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
        
        # 5. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
        test_queries = [
            "ì „ìê¸ˆìœµ ê´€ë ¨ ê³¼íƒœë£Œ ë¶€ê³¼ ì‚¬ë¡€",
            "ê°œì¸ì •ë³´ ìœ ì¶œ ê´€ë ¨ ì œì¬",
            "ì •ë³´ë³´í˜¸ ìœ„ë°˜ìœ¼ë¡œ ì¸í•œ ì œì¬",
        ]
        
        for query in test_queries:
            results = self.test_query(query)
            print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ '{query}' ê²°ê³¼:")
            for i, result in enumerate(results[:2]):  # ìƒìœ„ 2ê°œë§Œ ì¶œë ¥
                print(f"  #{i+1} (ì ìˆ˜: {result['score']:.4f})")
                print(f"  ê¸°ê´€: {result['metadata']['institution']}")
                print(f"  ì œì¬ì¼: {result['metadata']['sanction_date']}")
                print(f"  ë‚´ìš©: {result['content'][:150]}...\n")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    chunker = FSSSemanticChunker(
        input_json="./data/FSS_SANCTION/fss_sanctions_parsed.json",  # ìƒëŒ€ ê²½ë¡œ ìˆ˜ì •
        output_dir="./data/vector_db/fss_sanctions",  # ìƒëŒ€ ê²½ë¡œ ìˆ˜ì •
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # í•œêµ­ì–´ ì§€ì› ë‹¤êµ­ì–´ ëª¨ë¸ë¡œ ë³€ê²½
        #use_openai_embeddings=True, # OpenAI ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€ë¥¼ Trueë¡œ ë³€ê²½
        use_openai_embeddings=False,
        chunk_size=512,
        chunk_overlap=50,
        use_faiss=True # FAISS ì‚¬ìš© ì—¬ë¶€
    )
    
    chunker.process() 