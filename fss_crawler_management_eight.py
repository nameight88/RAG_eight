"""
ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ ì‚¬í•­ ë“± ê³µì‹œ ì „ìê¸ˆìœµ ê´€ë ¨ ë¬¸ì„œ í¬ë¡¤ëŸ¬
- ëŒ€ìƒ: https://www.fss.or.kr/fss/job/openInfoImpr/list.do
- ê¸°ê°„: 2014-01-01 ~ í˜„ì¬
- í•„í„°: ì „ìê¸ˆìœµ ê´€ë ¨ ë¶€ì„œ ë° í‚¤ì›Œë“œ
"""

import requests
import time
import os
import json
import re
import urllib.parse
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any


def is_electronic_finance_related(institution: str = "", content: str = "", filename: str = "") -> bool:
    """ë¬¸ì„œ ë‚´ìš©ì—ì„œ ì§€ì •ëœ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ íŒë‹¨"""
    
    # ì‚¬ìš©ì ì§€ì • í‚¤ì›Œë“œ
    keywords = [
        'ì „ìê¸ˆìœµ',
        'ì •ë³´ì²˜ë¦¬ìœ„íƒ',
        'ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì •',
        'ì‹ ìš©ì •ë³´ë²•',
        'ì‹ ìš©ì •ë³´ì˜ ì´ìš© ë° ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ',
        'ì‹ ìš©ì •ë³´ì—…ê°ë…ê·œì •',
        'ì‹ ìš©ì •ë³´ì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™'
    ]
    
    # ë¬¸ì„œ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ í™•ì¸ (ê°€ì¥ ì¤‘ìš”)
    if content:
        for keyword in keywords:
            if keyword in content:
                return True
    
    # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    if filename:
        for keyword in keywords:
            if keyword in filename:
                return True
    
    # ê¸°ê´€ëª…ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    if institution:
        for keyword in keywords:
            if keyword in institution:
                return True
    
    return False


def is_likely_electronic_finance_related(institution: str = "", filename: str = "") -> bool:
    """ë‹¤ìš´ë¡œë“œ ì „ ì‚¬ì „ í•„í„°ë§ - ê¸°ê´€ëª…ê³¼ íŒŒì¼ëª…ìœ¼ë¡œë§Œ íŒë‹¨"""
    
    # ì „ìê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ
    keywords = [
        'ì „ìê¸ˆìœµ',
        'ì •ë³´ì²˜ë¦¬ìœ„íƒ',
        'ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ',
        'ì‹ ìš©ì •ë³´ë²•',
        'ì‹ ìš©ì •ë³´',
        'ì‹ ìš©ì •ë³´ì—…ê°ë…',
        'í•€í…Œí¬',
        'ê°„í¸ê²°ì œ',
        'ì „ìê²°ì œ',
        'ì˜¨ë¼ì¸',
        'ë””ì§€í„¸',
        'IT',
        'ì •ë³´í†µì‹ ',
        'ì‹œìŠ¤í…œ',
        'ë°ì´í„°',
        'ê°œì¸ì •ë³´'
    ]
    
    # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    if filename:
        filename_lower = filename.lower()
        for keyword in keywords:
            if keyword in filename_lower:
                return True
    
    # ê¸°ê´€ëª…ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    if institution:
        institution_lower = institution.lower()
        for keyword in keywords:
            if keyword in institution_lower:
                return True
    
    return False


def extract_text_from_file(file_path: str) -> str:
    """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        if file_path.lower().endswith('.pdf'):
            import fitz  # PyMuPDF
            doc = fitz.open(file_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        
        elif file_path.lower().endswith(('.hwp', '.hwpx')):
            # HWP íŒŒì¼ ì²˜ë¦¬ (ê°„ë‹¨í•œ ë°©ë²•)
            try:
                import subprocess
                result = subprocess.run(['hwp5txt', file_path], 
                                      capture_output=True, text=True, encoding='utf-8')
                return result.stdout
            except:
                return ""
        
        else:
            return ""
    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì—ëŸ¬ {file_path}: {e}")
        return ""


class FSSManagementCrawler:
    """ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ ê³µì‹œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, base_dir: str = "../../"):
        self.base_url = "https://www.fss.or.kr"
        self.base_dir = Path(base_dir)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
    def download_file(self, file_url: str, filename: str, save_dir: Path) -> Optional[Path]:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        save_dir.mkdir(parents=True, exist_ok=True)
        filepath = save_dir / filename
        
        if filepath.exists():
            print(f"ğŸ“ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨: {filepath}")
            return filepath
        
        try:
            # ì„¸ì…˜ ì‚¬ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œ
            session = requests.Session()
            session.headers.update(self.headers)
            
            resp = session.get(file_url, timeout=30, stream=True)
            if resp.status_code == 200:
                with open(filepath, 'wb') as f:
                    for chunk in resp.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
                return filepath
            else:
                print(f"âŒ íŒŒì¼ ìš”ì²­ ì‹¤íŒ¨: {resp.status_code}")
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None
        
    def get_management_list(self, page: int = 1) -> Dict:
        """ê²½ì˜ìœ ì˜ ê³µì‹œ ëª©ë¡ ì¡°íšŒ"""
        url = f"{self.base_url}/fss/job/openInfoImpr/list.do"
        params = {
            'menuNo': '200483',
            'pageIndex': page,
            'sdate': '2014-01-01',
            'edate': datetime.now().strftime('%Y-%m-%d'),
            'searchCnd': '4',
            'searchWrd': ''
        }
        
        try:
            resp = requests.get(url, params=params, headers=self.headers, timeout=30)
            
            if resp.status_code != 200:
                print(f"âŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: ìƒíƒœì½”ë“œ {resp.status_code}")
                return {}
                
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ì „ì²´ ê±´ìˆ˜ ì¶”ì¶œ
            total_count = 0
            total_elem = soup.find(string=re.compile(r'ì´\s*\d+\s*ê±´'))
            if total_elem:
                match = re.search(r'(\d+)', str(total_elem))
                if match:
                    total_count = int(match.group(1))
            
            # í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            items = []
            
            # ì—¬ëŸ¬ í…Œì´ë¸” í´ë˜ìŠ¤ ì‹œë„
            table = soup.find('table', class_='tbl_list')
            if not table:
                table = soup.find('table', class_='list')
            if not table:
                table = soup.find('table')
            
            if table:
                # tbody ì°¾ê¸°
                tbody = table.find('tbody')
                if not tbody:
                    tbody = table
                    
                rows = tbody.find_all('tr')
                
                for row in rows:
                    tds = row.find_all('td')
                    
                    if len(tds) >= 5:
                        item = {
                            'no': tds[0].get_text(strip=True),
                            'institution': tds[1].get_text(strip=True),
                            'date': tds[2].get_text(strip=True),
                        }
                        
                        # ìƒì„¸ ë§í¬ ì¶”ì¶œ
                        link_tag = tds[3].find('a')
                        if link_tag:
                            onclick = link_tag.get('onclick', '')
                            href = link_tag.get('href', '')
                            
                            if onclick:
                                # JavaScript í•¨ìˆ˜ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                                match = re.search(r"fn_detail\('([^']+)','([^']+)'\)", onclick)
                                if match:
                                    item['mngmCntnNo'] = match.group(1)
                                    item['emOpenSeq'] = match.group(2)
                                    # ìƒì„¸ URL êµ¬ì„±
                                    detail_params = {
                                        'menuNo': '200483',
                                        'mngmCntnNo': item['mngmCntnNo'],
                                        'emOpenSeq': item['emOpenSeq']
                                    }
                                    item['detail_url'] = f"{self.base_url}/fss/job/openInfoImpr/view.do?" + urllib.parse.urlencode(detail_params)
                            elif href:
                                # hrefì—ì„œ ì§ì ‘ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ìƒˆë¡œìš´ ë°©ì‹)
                                item['detail_url'] = urllib.parse.urljoin(self.base_url, href)
                        
                        items.append(item)
            
            return {
                'total_count': total_count,
                'items': items,
                'page': page
            }
            
        except Exception as e:
            print(f"âŒ ëª©ë¡ ì¡°íšŒ ì—ëŸ¬ (í˜ì´ì§€ {page}): {type(e).__name__} - {e}")
            return {}
    
    def get_management_detail_and_download(self, item: Dict, save_dir: str = "data/FSS_MANAGEMENT") -> Dict:
        """ê²½ì˜ìœ ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        file_url = item.get('detail_url')
        if not file_url:
            return item
            
        try:
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„
            save_path = self.base_dir / save_dir
            date_str = re.sub(r'[^\d]', '', item.get('date', ''))[:8] or datetime.now().strftime('%Y%m%d')
            institution = re.sub(r'[^\w\s-]', '', item.get('institution', 'unknown'))
            institution = institution.replace(' ', '_')[:30]
            
            # íŒŒì¼ëª… ì¶”ì¶œ (URLì—ì„œ)
            filename = "document.pdf"  # ê¸°ë³¸ê°’
            if 'file=' in file_url:
                import urllib.parse
                parsed_url = urllib.parse.urlparse(file_url)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                if 'file' in query_params:
                    filename = query_params['file'][0]
                    filename = urllib.parse.unquote(filename)
            
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            if not filename.lower().endswith(('.pdf', '.hwp', '.hwpx')):
                if '.' not in filename:
                    filename += '.pdf'
            
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_filename = f"MGMT_{date_str}_{institution}_{filename}"
            safe_filename = re.sub(r'[^\w\s.-]', '_', safe_filename)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            downloaded_files = []
            filepath = self.download_file(file_url, safe_filename, save_path)
            if filepath:
                downloaded_files.append(str(filepath))
                print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {safe_filename}")
                
                # ê¸°ë³¸ ì •ë³´ë§Œ ì €ì¥ (ë‚´ìš© ë¶„ì„ì€ ë‚˜ì¤‘ì—)
                detail = item.copy()
                detail['title'] = f"{institution} ê²½ì˜ìœ ì˜ì‚¬í•­"
                detail['downloaded_files'] = downloaded_files
                detail['download_count'] = len(downloaded_files)
                detail['file_url'] = file_url
                detail['filename'] = filename
                detail['safe_filename'] = safe_filename
                
                # ê¸°ë³¸ JSONìœ¼ë¡œ ì €ì¥
                json_filename = f"MGMT_{date_str}_{institution}.json"
                json_path = save_path / json_filename
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(detail, f, ensure_ascii=False, indent=2)
                
                return detail
            else:
                print(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {safe_filename}")
                return item
            
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì—ëŸ¬: {type(e).__name__} - {e}")
            return item
    
    def get_existing_managements(self, save_dir: str = "data/FSS_MANAGEMENT") -> set:
        """ê¸°ì¡´ ì €ì¥ëœ ê²½ì˜ìœ ì˜ ì •ë³´ í™•ì¸"""
        existing = set()
        save_path = self.base_dir / save_dir
        
        if save_path.exists():
            for file in save_path.glob("*.json"):
                if file.name.startswith("MGMT_"):
                    # íŒŒì¼ëª…ì—ì„œ mngmCntnNo ì¶”ì¶œ
                    match = re.search(r'_(\d{9,})\.json', file.name)
                    if match:
                        existing.add(match.group(1))
        
        return existing
    
    def crawl_electronic_finance_managements(self) -> List[Dict]:
        """ì „ìê¸ˆìœµ ê´€ë ¨ ê²½ì˜ìœ ì˜ ì •ë³´ í¬ë¡¤ë§"""
        new_files = []
        
        print(f"=== ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ê²½ì˜ìœ ì˜ í¬ë¡¤ë§ ì‹œì‘: {datetime.now()} ===")
        
        try:
            # ê¸°ì¡´ íŒŒì¼ í™•ì¸
            existing = self.get_existing_managements()
            print(f"ê¸°ì¡´ ê²½ì˜ìœ ì˜ ì •ë³´: {len(existing)}ê°œ")
            
            page = 1
            total_electronic = 0
            consecutive_empty = 0
            
            while True:
                print(f"\nğŸ“„ {page}í˜ì´ì§€ ì¡°íšŒ ì¤‘...")
                result = self.get_management_list(page)
                
                if not result or not result.get('items'):
                    consecutive_empty += 1
                    if consecutive_empty >= 3:
                        print("ğŸ“­ ì—°ì† 3í˜ì´ì§€ ë¹ˆ ê²°ê³¼ - í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                    page += 1
                    continue
                
                consecutive_empty = 0
                items = result['items']
                print(f"  - {len(items)}ê°œ í•­ëª© ë°œê²¬")
                
                page_electronic = 0
                page_skipped = 0
                
                for item in items:
                    # ì´ë¯¸ ì €ì¥ëœ í•­ëª©ì€ ê±´ë„ˆë›°ê¸°
                    if item.get('mngmCntnNo') and item.get('mngmCntnNo') in existing:
                        continue
                    
                    institution = item.get('institution', '')
                    file_url = item.get('detail_url', '')
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ
                    filename = ""
                    if 'file=' in file_url:
                        import urllib.parse
                        parsed_url = urllib.parse.urlparse(file_url)
                        query_params = urllib.parse.parse_qs(parsed_url.query)
                        if 'file' in query_params:
                            filename = query_params['file'][0]
                            filename = urllib.parse.unquote(filename)
                    
                    # ì‚¬ì „ í•„í„°ë§ - ì „ìê¸ˆìœµ ê´€ë ¨ì„± í™•ì¸
                    if not is_likely_electronic_finance_related(institution, filename):
                        page_skipped += 1
                        if page <= 3:  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ ìŠ¤í‚¤í•‘ ì •ë³´ ì¶œë ¥
                            print(f"â­ï¸  ìŠ¤í‚¤í•‘: {institution} | {filename}")
                        continue
                    
                    # ë””ë²„ê¹…: ì²˜ìŒ 3í˜ì´ì§€ë§Œ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ í•­ëª© ì¶œë ¥
                    if page <= 3:
                        print(f"ğŸ“‹ ë‹¤ìš´ë¡œë“œ ëŒ€ìƒ: {institution} | {filename}")
                    
                    # ì „ìê¸ˆìœµ ê´€ë ¨ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ
                    detail = self.get_management_detail_and_download(item)
                    
                    if detail and detail.get('download_count', 0) > 0:
                        file_info = {
                            'type': 'ê²½ì˜ìœ ì˜ì‚¬í•­',
                            'institution': detail.get('institution', ''),
                            'date': detail.get('date', ''),
                            'title': detail.get('title', ''),
                            'filename': detail.get('filename', ''),
                            'safe_filename': detail.get('safe_filename', ''),
                            'file_url': detail.get('file_url', ''),
                            'downloaded_files': detail.get('downloaded_files', []),
                            'timestamp': datetime.now().isoformat()
                        }
                        new_files.append(file_info)
                        page_electronic += 1
                        total_electronic += 1
                    
                    time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                
                print(f"  â†’ ì´ í˜ì´ì§€ì—ì„œ {page_electronic}ê°œ ì „ìê¸ˆìœµ ê²½ì˜ìœ ì˜ ë‹¤ìš´ë¡œë“œ, {page_skipped}ê°œ ìŠ¤í‚¤í•‘")
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                total_count = result.get('total_count', 0)
                if total_count > 0 and page * 10 >= total_count:
                    print(f"\nâœ… ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ ì™„ë£Œ (ì´ {total_count}ê±´)")
                    break
                
                page += 1
                time.sleep(1)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                
                # ì•ˆì „ì¥ì¹˜ ì œê±° - ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§
                # if page > 3:  # í…ŒìŠ¤íŠ¸ìš©: 3í˜ì´ì§€ê¹Œì§€ë§Œ
                #     print("âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 3í˜ì´ì§€ê¹Œì§€ë§Œ í¬ë¡¤ë§")
                #     break
            
            
            print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
            print(f"ğŸ”µ ë‹¤ìš´ë¡œë“œëœ ê²½ì˜ìœ ì˜ ì •ë³´: {len(new_files)}ê°œ")
            print(f"ğŸ“Š ì „ì²´ ë‹¤ìš´ë¡œë“œ: {total_electronic}ê°œ")
            
            return new_files
            
        except KeyboardInterrupt:
            print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return new_files
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬: {type(e).__name__} - {e}")
            return new_files
    
    def save_crawl_log(self, new_files: List[Dict]) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ ë¡œê·¸ ì €ì¥"""
        if not new_files:
            return
        
        log_dir = self.base_dir / "crawl_logs"
        log_dir.mkdir(exist_ok=True)
        
        today = datetime.now().strftime("%Y%m%d")
        log_file = log_dir / f"fss_managements_{today}.json"
        
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(new_files, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“‹ í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥: {log_file}")

    def create_json_corpus(self, new_files: List[Dict]) -> None:
        """ë‹¤ìš´ë¡œë“œëœ ëª¨ë“  ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë“¤ë¡œ JSON Corpus ìƒì„±"""
        if not new_files:
            return
        
        print(f"\nğŸ“„ {len(new_files)}ê°œ ë‹¤ìš´ë¡œë“œëœ ë¬¸ì„œ ë‚´ìš© ë¶„ì„ ì¤‘...")
        
        corpus_dir = self.base_dir / "corpus"
        corpus_dir.mkdir(exist_ok=True)
        
        today = datetime.now().strftime("%Y%m%d")
        corpus_file = corpus_dir / f"fss_management_corpus_{today}.json"
        
        corpus_data = {
            "metadata": {
                "source": "ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ì‚¬í•­ ê³µì‹œ",
                "created_date": datetime.now().isoformat(),
                "total_downloaded": len(new_files),
                "keywords": [
                    "ì „ìê¸ˆìœµ",
                    "ì •ë³´ì²˜ë¦¬ìœ„íƒ", 
                    "ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì •",
                    "ì‹ ìš©ì •ë³´ë²•",
                    "ì‹ ìš©ì •ë³´ì˜ ì´ìš© ë° ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ",
                    "ì‹ ìš©ì •ë³´ì—…ê°ë…ê·œì •",
                    "ì‹ ìš©ì •ë³´ì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™"
                ]
            },
            "documents": []
        }
        
        relevant_count = 0
        
        for i, file_info in enumerate(new_files):
            print(f"ğŸ“– ë¶„ì„ ì¤‘ ({i+1}/{len(new_files)}): {file_info.get('institution', '')}")
            
            # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ê²½ë¡œ
            downloaded_files = file_info.get('downloaded_files', [])
            if not downloaded_files:
                continue
                
            file_path = downloaded_files[0]
            
            try:
                # íŒŒì¼ ë‚´ìš© ì¶”ì¶œ
                file_content = extract_text_from_file(file_path)
                
                if not file_content:
                    print(f"âš ï¸  í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {file_info.get('safe_filename', '')}")
                    continue
                
                # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                is_relevant = is_electronic_finance_related(
                    file_info.get('institution', ''), 
                    file_content, 
                    file_info.get('filename', '')
                )
                
                if is_relevant:
                    print(f"âœ… ê´€ë ¨ ë¬¸ì„œ ë°œê²¬: {file_info.get('institution', '')}")
                    
                    # ë°œê²¬ëœ í‚¤ì›Œë“œ ì°¾ê¸°
                    found_keywords = []
                    keywords = corpus_data["metadata"]["keywords"]
                    for keyword in keywords:
                        if keyword in file_content or keyword in file_info.get('filename', '') or keyword in file_info.get('institution', ''):
                            found_keywords.append(keyword)
                    
                    doc_data = {
                        "id": f"FSS_MGMT_{file_info.get('date', '').replace('-', '')}_{file_info.get('institution', '').replace(' ', '_')}",
                        "title": file_info.get('title', ''),
                        "institution": file_info.get('institution', ''),
                        "date": file_info.get('date', ''),
                        "filename": file_info.get('filename', ''),
                        "content": file_content,
                        "content_length": len(file_content),
                        "found_keywords": found_keywords,
                        "file_url": file_info.get('file_url', ''),
                        "processed_date": file_info.get('timestamp', '')
                    }
                    corpus_data["documents"].append(doc_data)
                    relevant_count += 1
                else:
                    print(f"â­ï¸  ê´€ë ¨ ì—†ìŒ: {file_info.get('institution', '')}")
                    
            except Exception as e:
                print(f"âŒ íŒŒì¼ ë¶„ì„ ì—ëŸ¬ {file_path}: {e}")
                continue
        
        # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        corpus_data["metadata"]["total_relevant"] = relevant_count
        
        # Corpus ì €ì¥
        with open(corpus_file, 'w', encoding='utf-8') as f:
            json.dump(corpus_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“š JSON Corpus ìƒì„± ì™„ë£Œ: {corpus_file}")
        print(f"ğŸ“Š ì „ì²´ ë‹¤ìš´ë¡œë“œ: {len(new_files)}ê°œ")
        print(f"ğŸ“Š ê´€ë ¨ ë¬¸ì„œ: {relevant_count}ê°œ")
        print(f"ğŸ“Š Corpusì— í¬í•¨: {len(corpus_data['documents'])}ê°œ")


if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = FSSManagementCrawler()
    
    # ì „ìê¸ˆìœµ ê´€ë ¨ ê²½ì˜ìœ ì˜ ì •ë³´ í¬ë¡¤ë§
    new_files = crawler.crawl_electronic_finance_managements()
    
    # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
    crawler.save_crawl_log(new_files)
    
    # JSON Corpus ìƒì„±
    crawler.create_json_corpus(new_files)
    
    print("\nâœ¨ ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ê²½ì˜ìœ ì˜ í¬ë¡¤ë§ ì™„ë£Œ!")
