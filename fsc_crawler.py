"""
ê¸ˆìœµìœ„ì›íšŒ ì „ìê¸ˆìœµ ê´€ë ¨ ë¬¸ì„œ í¬ë¡¤ëŸ¬
- ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ (NAL)
- ë²•ë ¹í•´ì„ (LAW)
"""

import requests
import time
import os
import json
import re
import urllib.parse
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any


def is_electronic_finance_related(title: str) -> bool:
    """ì œëª©ìœ¼ë¡œ ì „ìê¸ˆìœµ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨"""
    keywords = [
        'ì „ìê¸ˆìœµ', 'í•€í…Œí¬', 'ê°€ìƒìì‚°', 'ì•”í˜¸í™”í', 'ë¹„íŠ¸ì½”ì¸', 
        'ë¸”ë¡ì²´ì¸', 'ì „ìì§€ê¸‰', 'ì „ìê²°ì œ', 'ëª¨ë°”ì¼ê²°ì œ', 'P2P',
        'í¬ë¼ìš°ë“œí€ë”©', 'ë¡œë³´ì–´ë“œë°”ì´ì €', 'ì˜¨ë¼ì¸íˆ¬ì', 'ë””ì§€í„¸',
        'ì „ììƒê±°ë˜', 'API', 'ì˜¤í”ˆë±…í‚¹', 'ë§ˆì´ë°ì´í„°', 'ë¹…í…Œí¬',
        'ì „ìì„œëª…', 'DLT', 'ë¶„ì‚°ì›ì¥', 'CBDC', 'ìŠ¤í…Œì´ë¸”ì½”ì¸', 
        'í´ë¼ìš°ë“œ', 'ì „ì‚°', 'ì„ ë¶ˆ', 'ê°œì¸ì‹ ìš©ì •ë³´', 'ë¹„ë°€ë²ˆí˜¸',
        'ì‹ ìš©ì •ë³´', 'ì¬í•´ë³µêµ¬', 'ì •ë³´ì²˜ë¦¬'
    ]
    return any(keyword in title for keyword in keywords)


def extract_section_text(soup, label):
    """HTMLì—ì„œ íŠ¹ì • ì„¹ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    tag = soup.find(lambda tag: tag.name in ["strong", "th"] and label in tag.get_text())
    if tag:
        if tag.name == "th":
            tr = tag.find_parent("tr")
            if tr:
                td = tr.find("td")
                if td:
                    text = td.get_text(" ", strip=True)
                    if not text:
                        print(f"âš ï¸ [extract_section_text] <th>{label}</th> ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ!")
                    return text
        next_tag = tag.find_next(["div", "p", "td"])
        if next_tag:
            text = next_tag.get_text(" ", strip=True)
            if not text:
                print(f"âš ï¸ [extract_section_text] <strong>{label}</strong> ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ!")
            return text
    print(f"âš ï¸ [extract_section_text] '{label}' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨!")
    return ""


def parse_content(soup, base_url: str, idx: int, title: str, info: Dict) -> Dict:
    """ì›¹í˜ì´ì§€ ë‚´ìš© íŒŒì‹±"""
    summary = extract_section_text(soup, "ì§ˆì˜ìš”ì§€")
    answer = extract_section_text(soup, "íšŒë‹µ")
    reason = extract_section_text(soup, "ì´ìœ ")

    # ì²¨ë¶€íŒŒì¼ ë§í¬ ì¶”ì¶œ
    hwp_links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if href.endswith(".hwp") or href.endswith(".hwpx"):
            file_url = href if href.startswith("http") else base_url + href
            hwp_links.append(file_url)

    return {
        "idx": idx,
        "title": title,
        "info": info,
        "summary": summary,
        "answer": answer,
        "reason": reason,
        "hwp_links": hwp_links
    }


class FSCCrawler:
    """ê¸ˆìœµìœ„ì›íšŒ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, base_dir: str = "../../"):
        self.base_url = "https://better.fsc.go.kr"
        self.base_dir = Path(base_dir)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def parse_fsc_detail(self, idx: int, doc_type: str = "opinion", check_title_only: bool = False) -> Optional[Dict]:
        """ê°œë³„ ë¬¸ì„œ íŒŒì‹±"""
        try:
            if doc_type == "opinion":
                detail_url = f"{self.base_url}/fsc_new/replyCase/OpinionDetail.do"
                data = {"muNo": 86, "stNo": 11, "opinionIdx": idx, "actCd": "R"}
                prefix = "NAL"
            elif doc_type == "lawreq":
                detail_url = f"{self.base_url}/fsc_new/replyCase/LawreqDetail.do"
                data = {"muNo": 85, "stNo": 11, "lawreqIdx": idx, "actCd": "R"}
                prefix = "LAW"
            else:
                raise ValueError("doc_typeì€ 'opinion' ë˜ëŠ” 'lawreq'ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

            resp = requests.post(detail_url, data=data, headers=self.headers, timeout=10)
            
            if resp.status_code != 200:
                print(f"ğŸŒ HTTP ì—ëŸ¬ {doc_type} {idx}: ìƒíƒœì½”ë“œ {resp.status_code}")
                return None
                
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # ì œëª© ì¶”ì¶œ
            title_tag = soup.select_one("table.tbl-view.two td.subject")
            if not title_tag:
                title_tag = soup.select_one("table.tbl-view td.subject")
            if not title_tag:
                title_tag = soup.select_one("table.tbl-write .subject")
            if not title_tag:
                title_tag = soup.select_one(".subject")
            if not title_tag:
                title_tag = soup.find("h3")

            title = title_tag.get_text(strip=True) if title_tag else ""
            
            if not title:
                return None
                
            if check_title_only:
                return {"title": title, "idx": idx}

            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            info = {}
            category = ""
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = {}
            for row in soup.select("table.tbl-view tr"):
                th = row.find("th")
                td = row.find("td")
                if th and td and not td.get("class") == ["subject"]:
                    key = th.get_text(strip=True)
                    val = td.get_text(" ", strip=True)
                    metadata[key] = val
                    
                    if key == "ë¶„ì•¼" or key == "ë¶„ë¥˜":
                        category = val.strip()

            # ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
            content_sections = {}
            for row in soup.select("table.tbl-write tr"):
                th = row.find("th")
                td = row.find("td")
                if th and td and not td.get("class") == ["subject"]:
                    key = th.get_text(strip=True)
                    val = td.get_text(" ", strip=True)
                    content_sections[key] = val

            info.update(metadata)
            info.update(content_sections)

            result = parse_content(soup, self.base_url, idx, title, info)
            result["category"] = category
            return result
            
        except requests.exceptions.Timeout:
            print(f"â° íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ {doc_type} {idx}: 10ì´ˆ ì´ˆê³¼")
            return None
        except requests.exceptions.ConnectionError:
            print(f"ğŸŒ ì—°ê²° ì—ëŸ¬ {doc_type} {idx}: ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ")
            return None
        except requests.exceptions.RequestException as e:
            print(f"ğŸŒ ìš”ì²­ ì—ëŸ¬ {doc_type} {idx}: {e}")
            return None
        except Exception as e:
            print(f"âŒ íŒŒì‹± ì—ëŸ¬ {doc_type} {idx}: {type(e).__name__} - {e}")
            return None

    def save_fsc_detail(self, result: Dict, save_dir: str = "data/FS_NAL", prefix: str = "NAL") -> Dict:
        """ê²°ê³¼ ì €ì¥"""
        save_path = self.base_dir / save_dir
        save_path.mkdir(exist_ok=True)
        
        idx = result["idx"]
        saved_path = None
        
        try:
            if result["hwp_links"]:
                for hwp_link in result["hwp_links"]:
                    try:
                        parsed_url = urllib.parse.urlparse(hwp_link)
                        org_file_name = urllib.parse.parse_qs(parsed_url.query).get('orgFileName', [f'{prefix}_{idx}.hwp'])[0]
                        ext = os.path.splitext(org_file_name)[-1] if '.' in org_file_name else '.hwp'
                        file_name = f'{prefix}_{idx}{ext}'
                        file_path = save_path / file_name
                        
                        if file_path.exists():
                            saved_path = str(file_path)
                            print(f"ğŸ“„ ì´ë¯¸ ì¡´ì¬: {file_path}")
                            continue
                            
                        resp = requests.get(hwp_link, stream=True, headers=self.headers, timeout=30)
                        
                        if resp.status_code == 200:
                            with open(file_path, "wb") as f:
                                for chunk in resp.iter_content(chunk_size=8192):
                                    f.write(chunk)
                            saved_path = str(file_path)
                            print(f"ğŸ’¾ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_path}")
                            time.sleep(2)
                            break
                        else:
                            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {prefix} {idx}: HTTP {resp.status_code}")
                            
                    except requests.exceptions.Timeout:
                        print(f"â° ë‹¤ìš´ë¡œë“œ íƒ€ì„ì•„ì›ƒ {prefix} {idx}: {hwp_link}")
                    except Exception as e:
                        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬ {prefix} {idx}: {type(e).__name__} - {e}")
            else:
                # í…ìŠ¤íŠ¸ë¡œ ì €ì¥
                file_name = f'{prefix}_{idx}.txt'
                file_path = save_path / file_name
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(f"ì œëª©: {result['title']}\n")
                    for k, v in result["info"].items():
                        f.write(f"{k}: {v}\n")
                saved_path = str(file_path)
                print(f"ğŸ“ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {file_path}")
            
            result["saved_path"] = saved_path
            return result
            
        except Exception as e:
            print(f"âŒ ì €ì¥ ì—ëŸ¬ {prefix} {idx}: {type(e).__name__} - {e}")
            result["saved_path"] = None
            return result

    def get_electronic_finance_opinion_list(self) -> List[int]:
        """ì „ìê¸ˆìœµ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ ëª©ë¡ ì¶”ì¶œ"""
        url = f"{self.base_url}/fsc_new/replyCase/selectReplyCaseOpinionList.do"
        
        opinion_indices = []
        page = 0
        
        while True:
            try:
                data = {
                    'draw': page + 1,
                    'start': page * 10,
                    'length': 10,
                    'searchCategory': '4',  # ì „ìê¸ˆìœµ ë¶„ë¥˜ì½”ë“œ
                    'searchKeyword': '',
                    'searchCondition': '',
                    'searchReplyRegDateStart': '',
                    'searchReplyRegDateEnd': '',
                    'searchStatus': '',
                    'searchLawType': '',
                    'searchChartIdx': ''
                }
                
                resp = requests.post(url, data=data, timeout=30)
                if resp.status_code != 200:
                    print(f"âŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: ìƒíƒœì½”ë“œ {resp.status_code}")
                    break
                    
                json_data = resp.json()
                records = json_data.get('data', [])
                
                if not records:
                    break
                    
                for record in records:
                    opinion_idx = record.get('opinionIdx')
                    if opinion_idx:
                        opinion_indices.append(opinion_idx)
                        
                print(f"ğŸ“„ ì „ìê¸ˆìœµ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ {page+1}í˜ì´ì§€: {len(records)}ê°œ ë°œê²¬")
                page += 1
                
                if page > 1000:
                    print("âš ï¸ 1000í˜ì´ì§€ ì´ˆê³¼, ì•ˆì „ ì¤‘ë‹¨")
                    break
                    
            except Exception as e:
                print(f"âŒ ëª©ë¡ ì¡°íšŒ ì—ëŸ¬ (í˜ì´ì§€ {page+1}): {e}")
                break
                
        print(f"âœ… ì „ìê¸ˆìœµ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ ì´ {len(opinion_indices)}ê°œ ë°œê²¬")
        return sorted(opinion_indices)

    def get_electronic_finance_lawreq_list(self) -> List[int]:
        """ë²•ë ¹í•´ì„ ëª©ë¡ì—ì„œ ì „ìê¸ˆìœµ ê´€ë ¨ ì œëª©ë§Œ ì¶”ì¶œ"""
        url = f"{self.base_url}/fsc_new/replyCase/selectReplyCaseLawreqList.do"
        indices = []
        page = 0
        
        while True:
            data = {
                'draw': page + 1,
                'start': page * 10,
                'length': 10,
            }
            try:
                resp = requests.post(url, data=data, timeout=30)
                if resp.status_code != 200:
                    print(f"âŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: ìƒíƒœì½”ë“œ {resp.status_code}")
                    break
                    
                records = resp.json().get('data', [])
                if not records:
                    break
                    
                for record in records:
                    title = record.get('title', '')
                    idx = record.get('lawreqIdx')
                    if idx and is_electronic_finance_related(title):
                        indices.append(idx)
                        
                print(f"ğŸ“„ ë²•ë ¹í•´ì„ {page+1}í˜ì´ì§€: {len(records)}ê°œ ì¤‘ ì „ìê¸ˆìœµ {len([r for r in records if is_electronic_finance_related(r.get('title',''))])}ê°œ")
                page += 1
                
                if page > 1000:
                    print("âš ï¸ 1000í˜ì´ì§€ ì´ˆê³¼, ì•ˆì „ ì¤‘ë‹¨")
                    break
                    
            except Exception as e:
                print(f"âŒ ëª©ë¡ ì¡°íšŒ ì—ëŸ¬ (í˜ì´ì§€ {page+1}): {e}")
                break
                
        print(f"âœ… ì „ìê¸ˆìœµ ë²•ë ¹í•´ì„ ì´ {len(indices)}ê°œ ë°œê²¬")
        return sorted(indices)

    def get_existing_indices(self, save_dir: str, prefix: str) -> set:
        """ê¸°ì¡´ íŒŒì¼ë“¤ì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ"""
        existing_indices = set()
        save_path = self.base_dir / save_dir
        
        if not save_path.exists():
            save_path.mkdir(parents=True, exist_ok=True)
            return existing_indices
        
        for filename in save_path.iterdir():
            if filename.name.startswith(prefix):
                match = re.search(f'{prefix}_(\\d+)', filename.name)
                if match:
                    existing_indices.add(int(match.group(1)))
        
        return existing_indices

    def crawl_new_documents(self) -> List[Dict]:
        """ìƒˆë¡œìš´ ë¬¸ì„œë“¤ í¬ë¡¤ë§"""
        new_files = []
        
        print(f"=== í¬ë¡¤ë§ ì‹œì‘: {datetime.now()} ===")
        
        try:
            # 1. ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ í¬ë¡¤ë§
            print("\n=== ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ í¬ë¡¤ë§ (ì „ìê¸ˆìœµ ë¶„ì•¼ë§Œ) ===")
            existing_nal = self.get_existing_indices("data/FS_NAL", "NAL")
            print(f"ê¸°ì¡´ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ íŒŒì¼: {len(existing_nal)}ê°œ")
            
            electronic_finance_indices = self.get_electronic_finance_opinion_list()
            new_indices = [idx for idx in electronic_finance_indices if idx not in existing_nal]
            
            print(f"ì „ìê¸ˆìœµ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ ì´ {len(electronic_finance_indices)}ê°œ ì¤‘ ìƒˆë¡œìš´ íŒŒì¼ {len(new_indices)}ê°œ ë°œê²¬")
            
            nal_count = 0
            for current_idx in new_indices:
                try:
                    result = self.parse_fsc_detail(current_idx, "opinion")
                    if result and result.get("title"):
                        saved_result = self.save_fsc_detail(result, "data/FS_NAL", "NAL")
                        
                        file_info = {
                            "type": "ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ",
                            "idx": current_idx,
                            "title": result["title"],
                            "category": "ì „ìê¸ˆìœµ",
                            "file_path": saved_result.get("saved_path"),
                            "timestamp": datetime.now().isoformat()
                        }
                        new_files.append(file_info)
                        nal_count += 1
                        print(f"âœ… ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ {current_idx}: [ì „ìê¸ˆìœµ] {result['title'][:40]}...")
                    else:
                        print(f"âŒ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ {current_idx}: ìƒì„¸ í˜ì´ì§€ ì—†ìŒ")
                        
                except Exception as e:
                    print(f"âŒ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ {current_idx} ì²˜ë¦¬ ì—ëŸ¬: {type(e).__name__} - {e}")
            
            # 2. ë²•ë ¹í•´ì„ í¬ë¡¤ë§
            print("\n=== ë²•ë ¹í•´ì„ í¬ë¡¤ë§ (ëª©ë¡ì—ì„œ ì œëª© í•„í„°ë§) ===")
            existing_law = self.get_existing_indices("data/FS_LAW", "LAW")
            lawreq_indices = self.get_electronic_finance_lawreq_list()
            new_indices = [idx for idx in lawreq_indices if idx not in existing_law]
            
            print(f"ì „ìê¸ˆìœµ ë²•ë ¹í•´ì„ ì´ {len(lawreq_indices)}ê°œ ì¤‘ ìƒˆë¡œìš´ íŒŒì¼ {len(new_indices)}ê°œ ë°œê²¬")

            law_count = 0
            for current_idx in new_indices:
                try:
                    result = self.parse_fsc_detail(current_idx, "lawreq")
                    if result and result.get("title"):
                        saved_result = self.save_fsc_detail(result, "data/FS_LAW", "LAW")
                        file_info = {
                            "type": "ë²•ë ¹í•´ì„",
                            "idx": current_idx,
                            "title": result["title"],
                            "file_path": saved_result.get("saved_path"),
                            "timestamp": datetime.now().isoformat()
                        }
                        new_files.append(file_info)
                        law_count += 1
                        print(f"âœ… ë²•ë ¹í•´ì„ {current_idx}: {result['title'][:50]}...")
                    else:
                        print(f"âŒ ë²•ë ¹í•´ì„ {current_idx}: ìƒì„¸ í˜ì´ì§€ ì—†ìŒ")
                except Exception as e:
                    print(f"âŒ ë²•ë ¹í•´ì„ {current_idx} ì²˜ë¦¬ ì—ëŸ¬: {type(e).__name__} - {e}")
                    
            # ê²°ê³¼ ìš”ì•½
            print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
            print(f"ğŸ”µ ìƒˆë¡œìš´ ë¹„ì¡°ì¹˜ì˜ê²¬ì„œ (ì „ìê¸ˆìœµ): {nal_count}ê°œ")
            print(f"ğŸŸ¢ ìƒˆë¡œìš´ ë²•ë ¹í•´ì„ (ì „ìê¸ˆìœµ): {law_count}ê°œ")
            print(f"ğŸ“ ì´ ìƒˆ íŒŒì¼: {len(new_files)}ê°œ")
            
            return new_files
            
        except KeyboardInterrupt:
            print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return new_files
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬: {type(e).__name__} - {e}")
            return new_files

    def save_crawl_log(self, new_files: List[Dict]) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ ë¡œê·¸ ì €ì¥"""
        if not new_files:
            return
            
        log_dir = self.base_dir / "crawl_logs"
        log_dir.mkdir(exist_ok=True)
        
        today = datetime.now().strftime("%Y%m%d")
        log_file = log_dir / f"new_files_{today}.json"
        
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(new_files, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥: {log_file}")

if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = FSCCrawler()
    
    # ìƒˆë¡œìš´ ë¬¸ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    new_files = crawler.crawl_new_documents()
    
    # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
    crawler.save_crawl_log(new_files)
    
    print("í¬ë¡¤ë§ ì™„ë£Œ!")
