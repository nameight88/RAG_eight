"""
ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ ì‚¬í•­ JSON ì½”í¼ìŠ¤ë¥¼ ì ì‘í˜• ì²­í‚¹(Adaptive Chunking)ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆ
- ë¬¸ì„œ êµ¬ì¡°ì™€ ë‚´ìš©ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ ì²­í¬ í¬ê¸° ì¡°ì •
- ë©”íƒ€ë°ì´í„° ë³´ì¡´ ë° ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•œ ìµœì í™”
- ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
"""

import os
import json
import re
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma, FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
import chromadb
import traceback
from langchain.docstore.document import Document
import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class AdaptiveChunkingStrategy:
    """ë¬¸ì„œ íŠ¹ì„±ì— ë”°ë¼ ì²­í‚¹ ì „ëµì„ ì ì‘ì ìœ¼ë¡œ ê²°ì •í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        default_chunk_size: int = 512,
        default_chunk_overlap: int = 50,
        min_chunk_size: int = 100,
        max_chunk_size: int = 1024,
    ):
        """ì´ˆê¸°í™”"""
        self.default_chunk_size = default_chunk_size
        self.default_chunk_overlap = default_chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        
        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.default_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.default_chunk_size,
            chunk_overlap=self.default_chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def determine_chunk_parameters(self, document: Dict[str, Any]) -> Tuple[int, int]:
        """ë¬¸ì„œì˜ íŠ¹ì„±ì— ë”°ë¼ ìµœì ì˜ ì²­í¬ í¬ê¸°ì™€ ê²¹ì¹¨ì„ ê²°ì •"""
        content = document.get('content', {}).get('full_text', '')
        
        # ë¬¸ì„œ ê¸¸ì´ ê¸°ë°˜ ê²°ì •
        content_length = len(content)
        
        # ë§¤ìš° ì§§ì€ ë¬¸ì„œëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ
        if content_length < self.min_chunk_size * 2:
            return content_length, 0
        
        # ë§¤ìš° ê¸´ ë¬¸ì„œëŠ” ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
        if content_length > 10000:
            return min(384, self.default_chunk_size), 30
            
        # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
        return self.default_chunk_size, self.default_chunk_overlap
    
    def get_splitter_for_document(self, document: Dict[str, Any]) -> RecursiveCharacterTextSplitter:
        """ë¬¸ì„œì— ë§ëŠ” ë¶„í• ê¸° ìƒì„±"""
        chunk_size, chunk_overlap = self.determine_chunk_parameters(document)
        
        if chunk_size == self.default_chunk_size and chunk_overlap == self.default_chunk_overlap:
            return self.default_splitter
        
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def split_by_structure(self, document: Dict[str, Any]) -> List[str]:
        """ë¬¸ì„œ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ì²­í‚¹"""
        content = document.get('content', {})
        full_text = content.get('full_text', '')
        
        # ë¬¸ì„œê°€ ì¶©ë¶„íˆ ì§§ìœ¼ë©´ ë¶„í• í•˜ì§€ ì•ŠìŒ
        if len(full_text) < self.min_chunk_size * 2:
            return [full_text]
        
        # ë¬¸ì„œ íŠ¹ì„±ì— ë§ëŠ” ë¶„í• ê¸° ì‚¬ìš©
        splitter = self.get_splitter_for_document(document)
        chunks = splitter.split_text(full_text)
        
        return chunks


class FSSAdaptiveChunker:
    """ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ ë¬¸ì„œ ì ì‘í˜• ì²­í‚¹ ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
    
    def __init__(
        self,
        input_json: str,
        output_dir: str = "./data/vector_db",
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        use_openai_embeddings: bool = True,  # ê¸°ë³¸ê°’ì„ Trueë¡œ ë³€ê²½
        default_chunk_size: int = 512,
        default_chunk_overlap: int = 50,
        use_faiss: bool = False  # FAISS ì‚¬ìš© ì—¬ë¶€ ì¶”ê°€
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            input_json: ì…ë ¥ JSON íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„ (use_openai_embeddings=Falseì¸ ê²½ìš°)
            use_openai_embeddings: OpenAI ì„ë² ë”© API ì‚¬ìš© ì—¬ë¶€
            default_chunk_size: ê¸°ë³¸ ì²­í¬ í¬ê¸°
            default_chunk_overlap: ê¸°ë³¸ ì²­í¬ ê²¹ì¹¨ í¬ê¸°
            use_faiss: FAISS ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš© ì—¬ë¶€
        """
        self.input_json = input_json
        self.output_dir = output_dir
        self.model_name = model_name
        self.use_openai_embeddings = use_openai_embeddings
        self.default_chunk_size = default_chunk_size
        self.default_chunk_overlap = default_chunk_overlap
        self.use_faiss = use_faiss
        
        # OpenAI API í‚¤ ë¡œë“œ
        if use_openai_embeddings:
            self.openai_api_key = os.getenv('OPENAI_API_KEY')
            if not self.openai_api_key:
                print("âš ï¸ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. HuggingFace ì„ë² ë”©ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_openai_embeddings = False
        
        # ì ì‘í˜• ì²­í‚¹ ì „ëµ ì´ˆê¸°í™”
        self.chunking_strategy = AdaptiveChunkingStrategy(
            default_chunk_size=default_chunk_size,
            default_chunk_overlap=default_chunk_overlap
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        if use_openai_embeddings and self.openai_api_key:
            print(f"ğŸ§  OpenAI ì„ë² ë”© API ì´ˆê¸°í™” ì¤‘...")
            self.embeddings = OpenAIEmbeddings(
                model="text-embedding-3-large",
                openai_api_key=self.openai_api_key
            )
            print(f"âœ… OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            print(f"ğŸ§  HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {self.model_name}")
            self.embeddings = HuggingFaceEmbeddings(
                model_name=self.model_name,
                model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            print(f"âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ë°ì´í„° ë° ë²¡í„° ì €ì¥ì†Œ
        self.corpus = None
        self.db = None
        
        # ì²­í¬ í†µê³„
        self.chunk_stats = {
            "total_docs": 0,
            "total_chunks": 0,
            "avg_chunks_per_doc": 0,
            "min_chunks": float('inf'),
            "max_chunks": 0,
        }
    
    def load_corpus(self) -> List[Dict[str, Any]]:
        """JSON ì½”í¼ìŠ¤ ë¡œë“œ"""
        try:
            print(f"ğŸ“„ JSON ì½”í¼ìŠ¤ ë¡œë“œ ì¤‘: {self.input_json}")
            
            # ë””ë ‰í† ë¦¬ í™•ì¸
            input_dir = os.path.dirname(self.input_json)
            if not os.path.exists(input_dir):
                print(f"âš ï¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒì„±í•©ë‹ˆë‹¤: {input_dir}")
                os.makedirs(input_dir, exist_ok=True)
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if not os.path.exists(self.input_json):
                print(f"âš ï¸ JSON íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.input_json}")
                print("í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                return self.create_sample_corpus()
            
            with open(self.input_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if "documents" in data:
                documents = data["documents"]
                print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
                
                # ê´€ë ¨ ë¬¸ì„œë§Œ í•„í„°ë§ (ì „ìê¸ˆìœµ ê´€ë ¨ ë¬¸ì„œ)
                relevant_docs = [doc for doc in documents if doc.get('is_relevant', False)]
                print(f"ğŸ” ì „ìê¸ˆìœµ ê´€ë ¨ ë¬¸ì„œ: {len(relevant_docs)}ê°œ")
                
                self.corpus = relevant_docs
                return relevant_docs
            else:
                print("âŒ JSON íŒŒì¼ì— 'documents' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return self.create_sample_corpus()
        
        except Exception as e:
            print(f"âŒ ì½”í¼ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            return self.create_sample_corpus()
    
    def create_sample_corpus(self) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì½”í¼ìŠ¤ ìƒì„±"""
        print("ğŸ”„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
        
        sample_documents = [
            {
                "doc_id": "SAMPLE_MGMT_001",
                "source_file": "sample_file_1.pdf",
                "institution": "í…ŒìŠ¤íŠ¸ê¸ˆìœµì£¼ì‹íšŒì‚¬",
                "disclosure_date": "2023.01.15",
                "is_relevant": True,
                "found_keywords": ["ì „ìê¸ˆìœµ", "ì •ë³´ì²˜ë¦¬ìœ„íƒ"],
                "content": {
                    "management_type": "ì „ìê¸ˆìœµ ê´€ë ¨ ê²½ì˜ìœ ì˜ì‚¬í•­",
                    "management_details": [
                        {
                            "title": "1. ì „ìê¸ˆìœµ ì•ˆì „ì¡°ì¹˜ ë¯¸í¡",
                            "content": "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•¨. ê¸ˆìœµíšŒì‚¬ëŠ” ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„± í™•ë³´ë¥¼ ìœ„í•˜ì—¬ ì¸ë ¥, ì‹œì„¤, ì „ìì  ì¥ì¹˜ ë“±ì˜ ì •ë³´ê¸°ìˆ ë¶€ë¬¸, ì „ìê¸ˆìœµì—…ë¬´ ì˜ìœ„ë¥¼ ìœ„í•œ ë‚´ë¶€í†µì œì ˆì°¨ ë“±ì— ê´€í•˜ì—¬ ê¸ˆìœµìœ„ì›íšŒê°€ ì •í•˜ëŠ” ê¸°ì¤€ì„ ì¤€ìˆ˜í•´ì•¼ í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ë³´ì•ˆ ì·¨ì•½ì ì„ ë°©ì¹˜í•˜ì˜€ìŒ."
                        },
                        {
                            "title": "2. ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ ë¶€ì‹¤",
                            "content": "ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ê°€ ë¶€ì‹¤í•˜ì—¬ ìœ„íƒì—…ì²´ì— ëŒ€í•œ ê´€ë¦¬Â·ê°ë…ì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŒ. íŠ¹íˆ ì™¸ë¶€ ê°œë°œì—…ì²´ì— ëŒ€í•œ ë³´ì•ˆê´€ë¦¬ê°€ ë¯¸í¡í•˜ì—¬ ê°œë°œ ë‹¨ê³„ì—ì„œ ë³´ì•ˆ ì·¨ì•½ì ì´ ë°œìƒí–ˆìœ¼ë©°, ì´ì— ëŒ€í•œ ì¡°ì¹˜ê°€ ì§€ì—°ë˜ì—ˆìŒ."
                        }
                    ],
                    "full_text": "í…ŒìŠ¤íŠ¸ê¸ˆìœµì£¼ì‹íšŒì‚¬ ê²½ì˜ìœ ì˜ì‚¬í•­ ê³µì‹œ\n\n1. ì „ìê¸ˆìœµ ì•ˆì „ì¡°ì¹˜ ë¯¸í¡\nì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•¨. ê¸ˆìœµíšŒì‚¬ëŠ” ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„± í™•ë³´ë¥¼ ìœ„í•˜ì—¬ ì¸ë ¥, ì‹œì„¤, ì „ìì  ì¥ì¹˜ ë“±ì˜ ì •ë³´ê¸°ìˆ ë¶€ë¬¸, ì „ìê¸ˆìœµì—…ë¬´ ì˜ìœ„ë¥¼ ìœ„í•œ ë‚´ë¶€í†µì œì ˆì°¨ ë“±ì— ê´€í•˜ì—¬ ê¸ˆìœµìœ„ì›íšŒê°€ ì •í•˜ëŠ” ê¸°ì¤€ì„ ì¤€ìˆ˜í•´ì•¼ í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ë³´ì•ˆ ì·¨ì•½ì ì„ ë°©ì¹˜í•˜ì˜€ìŒ.\n\n2. ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ ë¶€ì‹¤\nì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ê°€ ë¶€ì‹¤í•˜ì—¬ ìœ„íƒì—…ì²´ì— ëŒ€í•œ ê´€ë¦¬Â·ê°ë…ì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŒ. íŠ¹íˆ ì™¸ë¶€ ê°œë°œì—…ì²´ì— ëŒ€í•œ ë³´ì•ˆê´€ë¦¬ê°€ ë¯¸í¡í•˜ì—¬ ê°œë°œ ë‹¨ê³„ì—ì„œ ë³´ì•ˆ ì·¨ì•½ì ì´ ë°œìƒí–ˆìœ¼ë©°, ì´ì— ëŒ€í•œ ì¡°ì¹˜ê°€ ì§€ì—°ë˜ì—ˆìŒ."
                },
                "metadata": {
                    "doc_type": "ê²½ì˜ìœ ì˜ì‚¬í•­",
                    "char_count": 500,
                    "regulations": ["ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°", "ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì • ì œ7ì¡°"]
                },
                "quality_score": 4
            },
            {
                "doc_id": "SAMPLE_MGMT_002",
                "source_file": "sample_file_2.pdf",
                "institution": "ìƒ˜í”Œì€í–‰",
                "disclosure_date": "2023.02.20",
                "is_relevant": True,
                "found_keywords": ["ì‹ ìš©ì •ë³´ë²•"],
                "content": {
                    "management_type": "ì‹ ìš©ì •ë³´ ê´€ë ¨ ê²½ì˜ìœ ì˜ì‚¬í•­",
                    "management_details": [
                        {
                            "title": "1. ì‹ ìš©ì •ë³´ ê´€ë¦¬ ì†Œí™€",
                            "content": "ì‹ ìš©ì •ë³´ë²• ì œ19ì¡°ì— ë”°ë¥¸ ì‹ ìš©ì •ë³´ì˜ ê´€ë¦¬ ë° ë³´í˜¸ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ì‹ ìš©ì •ë³´ ìœ ì¶œ ì‚¬ê³ ê°€ ë°œìƒí•¨. ì‹ ìš©ì •ë³´íšŒì‚¬ë“±ì€ ì‹ ìš©ì •ë³´ì˜ ë¶„ì‹¤Â·ë„ë‚œÂ·ìœ ì¶œÂ·ë³€ì¡° ë˜ëŠ” í›¼ì†ì„ ë°©ì§€í•˜ê¸° ìœ„í•˜ì—¬ ë‚´ë¶€ê´€ë¦¬ê·œì • ì¤€ìˆ˜, ì ‘ê·¼í†µì œ ë“± ì•ˆì „ì„± í™•ë³´ì— í•„ìš”í•œ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•˜ë‚˜ ì´ë¥¼ ì´í–‰í•˜ì§€ ì•Šì•˜ìŒ."
                        },
                        {
                            "title": "2. ìœ„íƒì—…ì²´ ê´€ë¦¬ ë¶€ì‹¤",
                            "content": "ì‹ ìš©ì •ë³´ë²• ì œ17ì¡°ì— ë”°ë¥¸ ìœ„íƒì—…ì²´ ê´€ë¦¬ ì˜ë¬´ë¥¼ ì¤€ìˆ˜í•˜ì§€ ì•Šì•˜ìŒ. ì—…ë¬´ ìœ„íƒ ì‹œ ìœ„íƒì—…ì²´ì˜ ê¸°ìˆ ì Â·ë¬¼ë¦¬ì Â·ê´€ë¦¬ì  ë³´ì•ˆ ëŒ€ì±… ìˆ˜ë¦½ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì§€ ì•Šê³  ë³´ì•ˆ ì•½ì •ì„ ì²´ê²°í•˜ì§€ ì•ŠëŠ” ë“± ìœ„íƒì—…ì²´ ê´€ë¦¬ê°€ ë¯¸í¡í•˜ì˜€ìŒ."
                        }
                    ],
                    "full_text": "ìƒ˜í”Œì€í–‰ ê²½ì˜ìœ ì˜ì‚¬í•­ ê³µì‹œ\n\n1. ì‹ ìš©ì •ë³´ ê´€ë¦¬ ì†Œí™€\nì‹ ìš©ì •ë³´ë²• ì œ19ì¡°ì— ë”°ë¥¸ ì‹ ìš©ì •ë³´ì˜ ê´€ë¦¬ ë° ë³´í˜¸ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ì‹ ìš©ì •ë³´ ìœ ì¶œ ì‚¬ê³ ê°€ ë°œìƒí•¨. ì‹ ìš©ì •ë³´íšŒì‚¬ë“±ì€ ì‹ ìš©ì •ë³´ì˜ ë¶„ì‹¤Â·ë„ë‚œÂ·ìœ ì¶œÂ·ë³€ì¡° ë˜ëŠ” í›¼ì†ì„ ë°©ì§€í•˜ê¸° ìœ„í•˜ì—¬ ë‚´ë¶€ê´€ë¦¬ê·œì • ì¤€ìˆ˜, ì ‘ê·¼í†µì œ ë“± ì•ˆì „ì„± í™•ë³´ì— í•„ìš”í•œ ì¡°ì¹˜ë¥¼ ì·¨í•´ì•¼ í•˜ë‚˜ ì´ë¥¼ ì´í–‰í•˜ì§€ ì•Šì•˜ìŒ.\n\n2. ìœ„íƒì—…ì²´ ê´€ë¦¬ ë¶€ì‹¤\nì‹ ìš©ì •ë³´ë²• ì œ17ì¡°ì— ë”°ë¥¸ ìœ„íƒì—…ì²´ ê´€ë¦¬ ì˜ë¬´ë¥¼ ì¤€ìˆ˜í•˜ì§€ ì•Šì•˜ìŒ. ì—…ë¬´ ìœ„íƒ ì‹œ ìœ„íƒì—…ì²´ì˜ ê¸°ìˆ ì Â·ë¬¼ë¦¬ì Â·ê´€ë¦¬ì  ë³´ì•ˆ ëŒ€ì±… ìˆ˜ë¦½ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì§€ ì•Šê³  ë³´ì•ˆ ì•½ì •ì„ ì²´ê²°í•˜ì§€ ì•ŠëŠ” ë“± ìœ„íƒì—…ì²´ ê´€ë¦¬ê°€ ë¯¸í¡í•˜ì˜€ìŒ."
                },
                "metadata": {
                    "doc_type": "ê²½ì˜ìœ ì˜ì‚¬í•­",
                    "char_count": 450,
                    "regulations": ["ì‹ ìš©ì •ë³´ë²• ì œ17ì¡°", "ì‹ ìš©ì •ë³´ë²• ì œ19ì¡°"]
                },
                "quality_score": 5
            }
        ]
        
        # ìƒ˜í”Œ ë°ì´í„° ì €ì¥
        os.makedirs(os.path.dirname(self.input_json), exist_ok=True)
        
        with open(self.input_json, 'w', encoding='utf-8') as f:
            json.dump({
                "metadata": {
                    "source": "ìƒ˜í”Œ ë°ì´í„°",
                    "created_at": "2023-06-13"
                },
                "documents": sample_documents
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(sample_documents)}ê°œ ë¬¸ì„œ")
        return sample_documents
    
    def create_adaptive_chunks(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì ì‘í˜• ì²­í‚¹ ìˆ˜í–‰"""
        all_chunks = []
        chunks_per_doc = []
        
        print(f"ğŸ”ª {len(documents)}ê°œ ë¬¸ì„œ ì ì‘í˜• ì²­í‚¹ ì‹œì‘...")
        
        for doc in tqdm(documents):
            # ì ì‘í˜• ì²­í‚¹ ì „ëµ ì‚¬ìš©
            chunks = self.chunking_strategy.split_by_structure(doc)
            
            # ê° ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            doc_chunks = []
            for i, chunk_text in enumerate(chunks):
                # ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                found_keywords = doc.get('found_keywords', [])
                keywords_str = ', '.join(found_keywords) if found_keywords else ''
                
                # ë¬¸ì„œ íƒ€ì…ì— ë”°ë¼ í•„ë“œëª… ê²°ì •
                doc_type = doc.get('metadata', {}).get('doc_type', '')
                
                chunk = {
                    "id": f"{doc['doc_id']}-chunk-{i}",
                    "doc_id": doc['doc_id'],
                    "institution": doc['institution'],
                    "date": doc.get('date', ''),
                    "doc_type": doc_type,
                    "keywords": keywords_str,  # ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  ë¬¸ìì—´ë¡œ ì €ì¥
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "chunk_text": chunk_text
                }
                doc_chunks.append(chunk)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            if len(doc_chunks) > 0:
                chunks_per_doc.append(len(doc_chunks))
                self.chunk_stats["min_chunks"] = min(self.chunk_stats["min_chunks"], len(doc_chunks))
                self.chunk_stats["max_chunks"] = max(self.chunk_stats["max_chunks"], len(doc_chunks))
            
            all_chunks.extend(doc_chunks)
        
        # ìµœì¢… í†µê³„ ê³„ì‚°
        self.chunk_stats["total_docs"] = len(documents)
        self.chunk_stats["total_chunks"] = len(all_chunks)
        if chunks_per_doc:
            self.chunk_stats["avg_chunks_per_doc"] = sum(chunks_per_doc) / len(chunks_per_doc)
        else:
            self.chunk_stats["min_chunks"] = 0
        
        print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        print(f"ğŸ“Š í‰ê·  ì²­í¬ ìˆ˜ (ë¬¸ì„œë‹¹): {self.chunk_stats['avg_chunks_per_doc']:.2f}ê°œ")
        print(f"ğŸ“Š ìµœì†Œ/ìµœëŒ€ ì²­í¬ ìˆ˜: {self.chunk_stats['min_chunks']}ê°œ/{self.chunk_stats['max_chunks']}ê°œ")
        
        return all_chunks
    
    def build_vector_store(self, chunks: List[Dict[str, Any]]) -> None:
        """ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"""
        try:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(self.output_dir, exist_ok=True)
            
            print(f"ğŸ”„ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì‹œì‘...")
            
            # í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë¶„ë¦¬
            texts = []
            metadatas = []
            
            for chunk in chunks:
                text = chunk.get("chunk_text", "")
                
                # ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ê°€
                if not text.strip():
                    continue
                    
                texts.append(text)
                
                # ì•ˆì „í•œ ë©”íƒ€ë°ì´í„° êµ¬ì„± (ì§ì ‘ í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒ)
                safe_metadata = {
                    "id": chunk.get("id", ""),
                    "doc_id": chunk.get("doc_id", ""),
                    "institution": chunk.get("institution", ""),
                    "date": chunk.get("date", ""),
                    "doc_type": chunk.get("doc_type", ""),
                    "keywords": chunk.get("keywords", ""),
                    "chunk_index": chunk.get("chunk_index", 0),
                    "total_chunks": chunk.get("total_chunks", 1)
                }
                
                metadatas.append(safe_metadata)
            
            if not texts:
                print("âŒ ìœ íš¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # Chroma DB ìƒì„±
            self.db = Chroma.from_texts(
                texts=texts,
                embedding=self.embeddings,
                metadatas=metadatas,
                persist_directory=self.output_dir
            )
            
            # ì €ì¥
            self.db.persist()
            
            print(f"âœ… ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ: {self.output_dir}")
            print(f"ğŸ“Š ì„ë² ë”©ëœ ì²­í¬ ìˆ˜: {len(texts)}")
            
            # ì •ë³´ ì €ì¥
            import datetime
            info = {
                "model_name": self.model_name,
                "chunk_strategy": "adaptive",
                "document_count": self.chunk_stats["total_docs"],
                "chunk_count": len(texts),
                "avg_chunks_per_doc": self.chunk_stats["avg_chunks_per_doc"],
                "min_chunks": self.chunk_stats["min_chunks"],
                "max_chunks": self.chunk_stats["max_chunks"],
                "created_at": datetime.datetime.now().isoformat(),
            }
            
            with open(os.path.join(self.output_dir, "vector_store_info.json"), "w", encoding="utf-8") as f:
                json.dump(info, f, ensure_ascii=False, indent=2)
        
        except Exception as e:
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
    
    def test_query(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜í–‰"""
        if not self.db:
            print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        print(f"ğŸ” ì¿¼ë¦¬: '{query}'")
        
        try:
            results = self.db.similarity_search_with_score(query, k=k)
            
            formatted_results = []
            for doc, score in results:
                result = {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": float(score)
                }
                formatted_results.append(result)
            
            return formatted_results
            
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def process(self):
        """ì „ì²´ ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print(f"ğŸ”„ ê²½ì˜ìœ ì˜ì‚¬í•­ ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ë°ì´í„° ë¡œë“œ
        documents = self.load_corpus()
        
        # 2. ì ì‘í˜• ì²­í‚¹
        chunked_docs = self.create_adaptive_chunks(documents)
        
        # 3. ê²°ê³¼ ì €ì¥
        output_json = os.path.join(self.output_dir, "fss_management_parsed.json")
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(chunked_docs, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ê²½ì˜ìœ ì˜ì‚¬í•­ ì²­í‚¹ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_json}")
        
        # 4. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        self.create_vector_store(batch_size=100)
        
        print(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
    
    def create_vector_store(self, batch_size: int = 100):
        """ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ ì¶”ê°€)"""
        print("ğŸ”¢ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        output_json = os.path.join(self.output_dir, "fss_management_parsed.json")
        
        try:
            # ë°ì´í„° ë¡œë“œ
            with open(output_json, 'r', encoding='utf-8') as f:
                chunks = json.load(f)  # <- ë°”ë¡œ chunk ë¦¬ìŠ¤íŠ¸ì„

                documents = []
                for chunk in chunks:
                    text = chunk.get("chunk_text", "") or chunk.get("content", "")
                    if not text.strip():
                        continue

                    doc = Document(
                        page_content=text,
                        metadata={
                            'id': chunk.get('id', ''),
                            'doc_id': chunk.get('doc_id', ''),
                            'institution': chunk.get('institution', ''),
                            'date': chunk.get('date', ''),
                            'doc_type': chunk.get('doc_type', ''),
                            'keywords': chunk.get('keywords', '')
                        }
                    )
                    documents.append(doc)

            
            print(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
            
            # ì„ë² ë”© ì´ˆê¸°í™”
            if self.use_openai_embeddings:
                print("ğŸ§  OpenAI ì„ë² ë”© API ì´ˆê¸°í™” ì¤‘...")
                embeddings = OpenAIEmbeddings(
                    model="text-embedding-3-large",
                    openai_api_key=self.openai_api_key
                )
            else:
                print(f"ğŸ§  HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {self.model_name}")
                embeddings = HuggingFaceEmbeddings(
                    model_name=self.model_name,
                    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
            
            # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            if self.use_faiss:
                # FAISS ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬
                faiss_dir = os.path.join(self.output_dir, "faiss")
                os.makedirs(faiss_dir, exist_ok=True)
                
                print(f"FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘: {faiss_dir}")
                
                # ì´ˆê¸° ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ì²« ë°°ì¹˜ ì‚¬ìš©)
                first_batch = documents[:batch_size]
                vectorstore = FAISS.from_documents(
                    documents=first_batch,
                    embedding=embeddings
                )
                
                # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì¶”ê°€
                for i in tqdm(range(batch_size, len(documents), batch_size), desc="FAISSì— ë¬¸ì„œ ì¶”ê°€ ì¤‘"):
                    batch = documents[i:i+batch_size]
                    vectorstore.add_documents(batch)
                
                
                # ë¡œì»¬ íŒŒì¼ì— ì €ì¥
                vectorstore.save_local(faiss_dir)
                
                print(f"âœ… FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")
                
                # ë²¡í„° ì €ì¥ì†Œ ì •ë³´ ì €ì¥
                vector_store_info = {
                    'created_at': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'document_count': len(documents),
                    'embed_model': self.model_name if not self.use_openai_embeddings else 'text-embedding-3-large',
                    'use_openai': self.use_openai_embeddings,
                    'vector_store_type': 'FAISS'
                }
                
                with open(os.path.join(self.output_dir, 'vector_store_info.json'), 'w', encoding='utf-8') as f:
                    json.dump(vector_store_info, f, ensure_ascii=False, indent=2)
                
            else:
                # Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                print(f"Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘: {self.output_dir}")
                vectorstore = Chroma.from_documents(
                    documents=documents,
                    embedding=embeddings,
                    persist_directory=self.output_dir
                )
                vectorstore.persist()
                
                print(f"âœ… Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")
                
                # ë²¡í„° ì €ì¥ì†Œ ì •ë³´ ì €ì¥
                vector_store_info = {
                    'created_at': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'document_count': len(documents),
                    'embed_model': self.model_name if not self.use_openai_embeddings else 'text-embedding-3-large',
                    'use_openai': self.use_openai_embeddings,
                    'vector_store_type': 'Chroma'
                }
                
                with open(os.path.join(self.output_dir, 'vector_store_info.json'), 'w', encoding='utf-8') as f:
                    json.dump(vector_store_info, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“Š ë²¡í„° ì €ì¥ì†Œ ì •ë³´: {len(documents)}ê°œ ë¬¸ì„œ ì„ë² ë”©")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    chunker = FSSAdaptiveChunker(
        input_json="./data/FSS_MANAGEMENT/fss_management_parsed.json",
        output_dir="./data/vector_db/fss_management",
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        use_openai_embeddings=False,  # OpenAI ì„ë² ë”© ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
        default_chunk_size=512,
        default_chunk_overlap=50,
        use_faiss=True  # FAISS ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
    )
    
    chunker.process() 