"""
ê¸ˆìœµê°ë…ì› ì œì¬/ê²½ì˜ìœ ì˜ì‚¬í•­ RAG íŒŒì´í”„ë¼ì¸
- ì½”í¼ìŠ¤ ë¡œë“œ ë° ì²­í‚¹
- ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
- ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ ì‹¤í–‰
"""

import os
import argparse
import json
from typing import Dict, List, Any, Optional
import shutil
from datetime import datetime
from dotenv import load_dotenv
from semantic_chunker import FSSSemanticChunker
from adaptive_chunker import FSSAdaptiveChunker
from rag_system import FSSRagSystem

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='ê¸ˆìœµê°ë…ì› ì œì¬/ê²½ì˜ìœ ì˜ì‚¬í•­ RAG íŒŒì´í”„ë¼ì¸')
    
    # ì…ë ¥ íŒŒì¼/ë””ë ‰í† ë¦¬ ì„¤ì •
    parser.add_argument('--sanctions-json', type=str, default='./data/fss_sanctions_parsed.json',
                        help='ì œì¬ ì •ë³´ íŒŒì‹±ëœ JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--management-json', type=str, default='./data/fss_management_parsed.json',
                        help='ê²½ì˜ìœ ì˜ì‚¬í•­ íŒŒì‹±ëœ JSON íŒŒì¼ ê²½ë¡œ')
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    parser.add_argument('--vector-db-dir', type=str, default='./data/vector_db',
                        help='ë²¡í„° ì €ì¥ì†Œ ë””ë ‰í† ë¦¬')
    
    # ì„ë² ë”© ì„¤ì •
    parser.add_argument('--embed-model', type=str, 
                        default='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                        help='ì„ë² ë”© ëª¨ë¸ ì´ë¦„')
    parser.add_argument('--use-openai-embeddings', action='store_true', default=True,
                        help='OpenAI ì„ë² ë”© API ì‚¬ìš© (í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)')
    parser.add_argument('--use-faiss', action='store_true', default=True,
                        help='Chroma ëŒ€ì‹  FAISS ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©')
    
    # ì²­í‚¹ ì„¤ì •
    parser.add_argument('--chunk-size', type=int, default=512,
                        help='ì²­í¬ í¬ê¸°')
    parser.add_argument('--chunk-overlap', type=int, default=50,
                        help='ì²­í¬ ê²¹ì¹¨ í¬ê¸°')
    
    # ì²˜ë¦¬ ë‹¨ê³„ ì„ íƒ
    parser.add_argument('--skip-sanctions', action='store_true',
                        help='ì œì¬ ì •ë³´ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-management', action='store_true',
                        help='ê²½ì˜ìœ ì˜ì‚¬í•­ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--interactive', action='store_true',
                        help='ì²˜ë¦¬ í›„ ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰')
    
    # ìƒ˜í”Œ ë°ì´í„° ì˜µì…˜
    parser.add_argument('--use-sample-data', action='store_true',
                        help='ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)')
    
    return parser.parse_args()


def create_sample_data(sanctions_json: str, management_json: str):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(sanctions_json), exist_ok=True)
    os.makedirs(os.path.dirname(management_json), exist_ok=True)
    
    # ì œì¬ ì •ë³´ ìƒ˜í”Œ ë°ì´í„°
    if not os.path.exists(sanctions_json):
        print(f"ğŸ”„ ìƒ˜í”Œ ì œì¬ ì •ë³´ ë°ì´í„° ìƒì„±: {sanctions_json}")
        sample_sanctions = {
            "metadata": {
                "source": "ìƒ˜í”Œ ë°ì´í„°",
                "created_at": datetime.now().isoformat()
            },
            "documents": [
                {
                    "doc_id": "SAMPLE_001",
                    "source_file": "sample_file_1.pdf",
                    "institution": "í…ŒìŠ¤íŠ¸ê¸ˆìœµì£¼ì‹íšŒì‚¬",
                    "sanction_date": "2023.01.15",
                    "content": {
                        "sanction_type": "ê³¼íƒœë£Œ",
                        "fine": {"amount": 5000000, "unit": "ì›", "text": "ê³¼íƒœë£Œ 5ë°±ë§Œì›"},
                        "executive_sanction": "CEO ì£¼ì˜ì  ê²½ê³ ",
                        "sanction_facts": [
                            {"title": "ì „ìê¸ˆìœµ ì•ˆì „ì¡°ì¹˜ ë¯¸í¡", "content": "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•¨"}
                        ],
                        "full_text": "í…ŒìŠ¤íŠ¸ê¸ˆìœµ(ì£¼)ëŠ” ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•˜ì˜€ìœ¼ë¯€ë¡œ ê³¼íƒœë£Œ 5ë°±ë§Œì›ì˜ ì œì¬ë¥¼ ê²°ì •í•¨"
                    },
                    "metadata": {
                        "doc_type": "ì œì¬ë‚´ìš©ê³µê°œ",
                        "char_count": 200,
                        "regulations": ["ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°", "ì „ìê¸ˆìœµê°ë…ê·œì • ì œ13ì¡°"]
                    },
                    "quality_score": 4
                },
                {
                    "doc_id": "SAMPLE_002",
                    "source_file": "sample_file_2.pdf",
                    "institution": "ìƒ˜í”Œì€í–‰",
                    "sanction_date": "2023.02.20",
                    "content": {
                        "sanction_type": "ê¸°ê´€ê²½ê³ ",
                        "fine": {"amount": 0, "unit": "ì›", "text": ""},
                        "executive_sanction": "CIO ë¬¸ì±…ê²½ê³ ",
                        "sanction_facts": [
                            {"title": "ì •ë³´ë³´í˜¸ ì¸ë ¥ ë¶€ì¡±", "content": "ì „ìê¸ˆìœµê°ë…ê·œì •ì— ë”°ë¥¸ ì •ë³´ë³´í˜¸ ì¸ë ¥ì„ ì¶©ë¶„íˆ í™•ë³´í•˜ì§€ ì•Šê³  ìš´ì˜í•¨"},
                            {"title": "ë³´ì•ˆ ì·¨ì•½ì  ë°©ì¹˜", "content": "ì•Œë ¤ì§„ ë³´ì•ˆ ì·¨ì•½ì ì— ëŒ€í•œ ì¡°ì¹˜ë¥¼ 6ê°œì›” ì´ìƒ ì§€ì—°í•˜ì—¬ ì¡°ì¹˜í•¨"}
                        ],
                        "full_text": "ìƒ˜í”Œì€í–‰ì€ ì „ìê¸ˆìœµê°ë…ê·œì •ì— ë”°ë¥¸ ì •ë³´ë³´í˜¸ ì¸ë ¥ì„ ì¶©ë¶„íˆ í™•ë³´í•˜ì§€ ì•Šê³  ìš´ì˜í•˜ì˜€ìœ¼ë©°, ì•Œë ¤ì§„ ë³´ì•ˆ ì·¨ì•½ì ì— ëŒ€í•œ ì¡°ì¹˜ë¥¼ 6ê°œì›” ì´ìƒ ì§€ì—°í•˜ì—¬ ì¡°ì¹˜í•œ ì‚¬ì‹¤ì´ í™•ì¸ë˜ì–´ ê¸°ê´€ê²½ê³  ì¡°ì¹˜í•¨"
                    },
                    "metadata": {
                        "doc_type": "ì œì¬ë‚´ìš©ê³µê°œ",
                        "char_count": 300,
                        "regulations": ["ì „ìê¸ˆìœµê°ë…ê·œì • ì œ36ì¡°", "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì˜2"]
                    },
                    "quality_score": 5
                }
            ]
        }
        
        with open(sanctions_json, 'w', encoding='utf-8') as f:
            json.dump(sample_sanctions, f, ensure_ascii=False, indent=2)
    
    # ê²½ì˜ìœ ì˜ì‚¬í•­ ìƒ˜í”Œ ë°ì´í„°
    if not os.path.exists(management_json):
        print(f"ğŸ”„ ìƒ˜í”Œ ê²½ì˜ìœ ì˜ì‚¬í•­ ë°ì´í„° ìƒì„±: {management_json}")
        sample_management = {
            "metadata": {
                "source": "ìƒ˜í”Œ ë°ì´í„°",
                "created_at": datetime.now().isoformat()
            },
            "documents": [
                {
                    "doc_id": "SAMPLE_MGMT_001",
                    "source_file": "sample_mgmt_1.pdf",
                    "institution": "í…ŒìŠ¤íŠ¸ê¸ˆìœµì£¼ì‹íšŒì‚¬",
                    "disclosure_date": "2023.01.15",
                    "is_relevant": True,
                    "found_keywords": ["ì „ìê¸ˆìœµ", "ì •ë³´ì²˜ë¦¬ìœ„íƒ"],
                    "content": {
                        "management_type": "ì „ìê¸ˆìœµ ê´€ë ¨ ê²½ì˜ìœ ì˜ì‚¬í•­",
                        "management_details": [
                            {
                                "title": "1. ì „ìê¸ˆìœµ ì•ˆì „ì¡°ì¹˜ ë¯¸í¡",
                                "content": "ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•¨. ê¸ˆìœµíšŒì‚¬ëŠ” ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„± í™•ë³´ë¥¼ ìœ„í•˜ì—¬ ì¸ë ¥, ì‹œì„¤, ì „ìì  ì¥ì¹˜ ë“±ì˜ ì •ë³´ê¸°ìˆ ë¶€ë¬¸, ì „ìê¸ˆìœµì—…ë¬´ ì˜ìœ„ë¥¼ ìœ„í•œ ë‚´ë¶€í†µì œì ˆì°¨ ë“±ì— ê´€í•˜ì—¬ ê¸ˆìœµìœ„ì›íšŒê°€ ì •í•˜ëŠ” ê¸°ì¤€ì„ ì¤€ìˆ˜í•´ì•¼ í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ë³´ì•ˆ ì·¨ì•½ì ì„ ë°©ì¹˜í•˜ì˜€ìŒ."
                            },
                            {
                                "title": "2. ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ ë¶€ì‹¤",
                                "content": "ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ê°€ ë¶€ì‹¤í•˜ì—¬ ìœ„íƒì—…ì²´ì— ëŒ€í•œ ê´€ë¦¬Â·ê°ë…ì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŒ. íŠ¹íˆ ì™¸ë¶€ ê°œë°œì—…ì²´ì— ëŒ€í•œ ë³´ì•ˆê´€ë¦¬ê°€ ë¯¸í¡í•˜ì—¬ ê°œë°œ ë‹¨ê³„ì—ì„œ ë³´ì•ˆ ì·¨ì•½ì ì´ ë°œìƒí–ˆìœ¼ë©°, ì´ì— ëŒ€í•œ ì¡°ì¹˜ê°€ ì§€ì—°ë˜ì—ˆìŒ."
                            }
                        ],
                        "full_text": "í…ŒìŠ¤íŠ¸ê¸ˆìœµì£¼ì‹íšŒì‚¬ ê²½ì˜ìœ ì˜ì‚¬í•­ ê³µì‹œ\n\n1. ì „ìê¸ˆìœµ ì•ˆì „ì¡°ì¹˜ ë¯¸í¡\nì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°ì— ë”°ë¥¸ ì•ˆì „ì¡°ì¹˜ ì˜ë¬´ë¥¼ ì†Œí™€íˆ í•˜ì—¬ ê³ ê°ì •ë³´ ìœ ì¶œ ìœ„í—˜ì„ ì´ˆë˜í•¨. ê¸ˆìœµíšŒì‚¬ëŠ” ì „ìê¸ˆìœµê±°ë˜ì˜ ì•ˆì „ì„± í™•ë³´ë¥¼ ìœ„í•˜ì—¬ ì¸ë ¥, ì‹œì„¤, ì „ìì  ì¥ì¹˜ ë“±ì˜ ì •ë³´ê¸°ìˆ ë¶€ë¬¸, ì „ìê¸ˆìœµì—…ë¬´ ì˜ìœ„ë¥¼ ìœ„í•œ ë‚´ë¶€í†µì œì ˆì°¨ ë“±ì— ê´€í•˜ì—¬ ê¸ˆìœµìœ„ì›íšŒê°€ ì •í•˜ëŠ” ê¸°ì¤€ì„ ì¤€ìˆ˜í•´ì•¼ í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ë³´ì•ˆ ì·¨ì•½ì ì„ ë°©ì¹˜í•˜ì˜€ìŒ.\n\n2. ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ ë¶€ì‹¤\nì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒ ê´€ë¦¬ê°€ ë¶€ì‹¤í•˜ì—¬ ìœ„íƒì—…ì²´ì— ëŒ€í•œ ê´€ë¦¬Â·ê°ë…ì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŒ. íŠ¹íˆ ì™¸ë¶€ ê°œë°œì—…ì²´ì— ëŒ€í•œ ë³´ì•ˆê´€ë¦¬ê°€ ë¯¸í¡í•˜ì—¬ ê°œë°œ ë‹¨ê³„ì—ì„œ ë³´ì•ˆ ì·¨ì•½ì ì´ ë°œìƒí–ˆìœ¼ë©°, ì´ì— ëŒ€í•œ ì¡°ì¹˜ê°€ ì§€ì—°ë˜ì—ˆìŒ."
                    },
                    "metadata": {
                        "doc_type": "ê²½ì˜ìœ ì˜ì‚¬í•­",
                        "char_count": 500,
                        "regulations": ["ì „ìê¸ˆìœµê±°ë˜ë²• ì œ21ì¡°", "ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì • ì œ7ì¡°"]
                    },
                    "quality_score": 4
                }
            ]
        }
        
        with open(management_json, 'w', encoding='utf-8') as f:
            json.dump(sample_management, f, ensure_ascii=False, indent=2)
    
    print("âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ")


def process_sanctions(args):
    """ì œì¬ ì •ë³´ ì²˜ë¦¬"""
    print("\nğŸ”„ ì œì¬ ì •ë³´ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    sanctions_db_dir = os.path.join(os.path.abspath(args.vector_db_dir), "fss_sanctions")
    os.makedirs(sanctions_db_dir, exist_ok=True)
    
    # ì‹œë§¨í‹± ì²­ì»¤ ì´ˆê¸°í™” ë° ì²˜ë¦¬
    sanctions_chunker = FSSSemanticChunker(
        input_json=os.path.abspath(args.sanctions_json),
        output_dir=sanctions_db_dir,
        model_name=args.embed_model,
        use_openai_embeddings=args.use_openai_embeddings,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        use_faiss=args.use_faiss  # FAISS ì‚¬ìš© ì—¬ë¶€ ì¶”ê°€
    )
    
    sanctions_chunker.process()
    
    print("âœ… ì œì¬ ì •ë³´ ì²˜ë¦¬ ì™„ë£Œ")


def process_management(args):
    """ê²½ì˜ìœ ì˜ì‚¬í•­ ì²˜ë¦¬"""
    print("\nğŸ”„ ê²½ì˜ìœ ì˜ì‚¬í•­ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    management_db_dir = os.path.join(os.path.abspath(args.vector_db_dir), "fss_management")
    os.makedirs(management_db_dir, exist_ok=True)
    
    # ì ì‘í˜• ì²­ì»¤ ì´ˆê¸°í™” ë° ì²˜ë¦¬
    management_chunker = FSSAdaptiveChunker(
        input_json=os.path.abspath(args.management_json),
        output_dir=management_db_dir,
        model_name=args.embed_model,
        use_openai_embeddings=args.use_openai_embeddings,
        default_chunk_size=args.chunk_size,
        default_chunk_overlap=args.chunk_overlap,
        use_faiss=args.use_faiss  # FAISS ì‚¬ìš© ì—¬ë¶€ ì¶”ê°€
    )
    
    management_chunker.process()
    
    print("âœ… ê²½ì˜ìœ ì˜ì‚¬í•­ ì²˜ë¦¬ ì™„ë£Œ")


def run_interactive_mode(args):
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print("\nğŸ¤– ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘...")
    
    # DB ìœ í˜• ì„ íƒ
    print("\nì‚¬ìš©í•  ë²¡í„° ì €ì¥ì†Œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì œì¬ ì •ë³´")
    print("2. ê²½ì˜ìœ ì˜ì‚¬í•­")
    choice = input("ì„ íƒ (ê¸°ë³¸ê°’: 1): ").strip() or "1"
    
    if choice == "2":
        db_path = os.path.join(os.path.abspath(args.vector_db_dir), "fss_management")
        print("ê²½ì˜ìœ ì˜ì‚¬í•­ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©")
    else:
        db_path = os.path.join(os.path.abspath(args.vector_db_dir), "fss_sanctions")
        print("ì œì¬ ì •ë³´ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©")
    
    # API ì„ íƒ
    print("\nì‚¬ìš©í•  LLM APIë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. OpenAI API (ê¸°ë³¸ê°’)")
    print("2. Anthropic Claude API")
    api_choice = input("ì„ íƒ (ê¸°ë³¸ê°’: 1): ").strip() or "1"
    
    use_anthropic = (api_choice == "2")
    use_openai = (api_choice == "1")
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    if use_anthropic:
        anthropic_api_key = os.getenv("ANTHROPIC_APIKEY")
        if not anthropic_api_key:
            anthropic_api_key = input("Anthropic API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        
        rag_system = FSSRagSystem(
            vector_db_path=db_path,
            embed_model_name=args.embed_model,
            use_openai_embeddings=args.use_openai_embeddings,
            use_anthropic=True,
            anthropic_api_key=anthropic_api_key,
            top_k=5,
            use_faiss=args.use_faiss,  # FAISS ì‚¬ìš© ì—¬ë¶€ ì¶”ê°€
            use_openai_llm=False
        )
    else:  # OpenAI API ì‚¬ìš© (ê¸°ë³¸ê°’)
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            print("âš ï¸ í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        rag_system = FSSRagSystem(
            vector_db_path=db_path,
            embed_model_name=args.embed_model,
            use_openai_embeddings=args.use_openai_embeddings,
            llm_model_name="gpt-3.5-turbo",  # OpenAI ëª¨ë¸ ì‚¬ìš©
            top_k=5,
            use_faiss=args.use_faiss,  # FAISS ì‚¬ìš© ì—¬ë¶€ ì¶”ê°€
            use_openai_llm=True  # OpenAI LLM ì‚¬ìš©
        )
    
    # ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
    rag_system.interactive_mode()


def create_vector_stores(args):
    """ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ì €ì¥"""
    print("\nğŸ”„ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹œì‘...")
    
    # OpenAI API í‚¤ í™•ì¸
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ì œì¬ ì •ë³´ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        if not args.skip_sanctions:
            sanctions_dir = os.path.join(args.vector_db_dir, "fss_sanctions")
            os.makedirs(sanctions_dir, exist_ok=True)
            
            # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            from rag_system import FSSRagSystem
            sanctions_rag = FSSRagSystem(
                vector_db_path=sanctions_dir,
                embed_model_name=args.embed_model,
                use_openai_embeddings=args.use_openai_embeddings,
                use_anthropic=False,
                use_faiss=args.use_faiss,
                create_from_json=args.sanctions_json
            )
            
            if not sanctions_rag.vector_store:
                print("âŒ ì œì¬ ì •ë³´ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨")
                return False
            
            # ë²¡í„° ì €ì¥ì†Œ ì •ë³´ ì €ì¥
            info_path = os.path.join(sanctions_dir, "vector_store_info.json")
            info = {
                "created_at": datetime.now().isoformat(),
                "embed_model": "text-embedding-3-large" if args.use_openai_embeddings else args.embed_model,
                "use_openai": args.use_openai_embeddings,
                "vector_store_type": "FAISS" if args.use_faiss else "Chroma"
            }
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(info, f, ensure_ascii=False, indent=2)
            
            print("âœ… ì œì¬ ì •ë³´ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")
        
        # ê²½ì˜ìœ ì˜ì‚¬í•­ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        if not args.skip_management:
            management_dir = os.path.join(args.vector_db_dir, "fss_management")
            os.makedirs(management_dir, exist_ok=True)
            
            # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            from rag_system import FSSRagSystem
            management_rag = FSSRagSystem(
                vector_db_path=management_dir,
                embed_model_name=args.embed_model,
                use_openai_embeddings=args.use_openai_embeddings,
                use_anthropic=False,
                use_faiss=args.use_faiss,
                create_from_json=args.management_json
            )
            
            if not management_rag.vector_store:
                print("âŒ ê²½ì˜ìœ ì˜ì‚¬í•­ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨")
                return False
            
            # ë²¡í„° ì €ì¥ì†Œ ì •ë³´ ì €ì¥
            info_path = os.path.join(management_dir, "vector_store_info.json")
            info = {
                "created_at": datetime.now().isoformat(),
                "embed_model": "text-embedding-3-large" if args.use_openai_embeddings else args.embed_model,
                "use_openai": args.use_openai_embeddings,
                "vector_store_type": "FAISS" if args.use_faiss else "Chroma"
            }
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(info, f, ensure_ascii=False, indent=2)
            
            print("âœ… ê²½ì˜ìœ ì˜ì‚¬í•­ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ")
        
        print("\nğŸ‰ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ!")
        print("ìƒì„±ëœ ë²¡í„° ì €ì¥ì†Œë¥¼ GitHubì— ì»¤ë°‹í•˜ì„¸ìš”.")
        return True
        
    except Exception as e:
        print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ê¸ˆìœµê°ë…ì› ì œì¬/ê²½ì˜ìœ ì˜ì‚¬í•­ RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    
    # ì¸ì íŒŒì‹±
    args = parse_arguments()
    
    # ê²½ë¡œ ì •ê·œí™”
    args.sanctions_json = os.path.abspath(args.sanctions_json)
    args.management_json = os.path.abspath(args.management_json)
    args.vector_db_dir = os.path.abspath(args.vector_db_dir)
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(args.sanctions_json), exist_ok=True)
    os.makedirs(os.path.dirname(args.management_json), exist_ok=True)
    os.makedirs(args.vector_db_dir, exist_ok=True)
    
    # OpenAI API í‚¤ í™•ì¸
    if args.use_openai_embeddings:
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            print("âš ï¸ í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë ¤ë©´ .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            print("HuggingFace ì„ë² ë”©ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            args.use_openai_embeddings = False
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í•„ìš”í•œ ê²½ìš°)
    if args.use_sample_data:
        create_sample_data(args.sanctions_json, args.management_json)
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    create_vector_stores(args)
    
    # ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰
    if args.interactive:
        run_interactive_mode(args)
    
    print("\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 