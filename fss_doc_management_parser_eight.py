"""
ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ ë¬¸ì„œ íŒŒì‹± ëª¨ë“ˆ
- PDF, HWP, HWPX íŒŒì¼ íŒŒì‹±
- ê²½ì˜ìœ ì˜ ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜
- ì§€ì •ëœ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë§Œ ì½”í¼ìŠ¤ì— í¬í•¨
"""

import os
import json
import re
import pathlib
import shutil
import tempfile
import zipfile
import subprocess
import fitz 
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    # íŠ¹ìˆ˜ë¬¸ì ì¤‘ í•„ìš”í•œ ê²ƒë§Œ ìœ ì§€
    text = re.sub(r'[^\wê°€-í£ .,?!:;()\-\[\]%Â·â€»â—‹â—â–¡â– â—‡â—â–³â–²â–½â–¼\n]', '', text)
    return text.strip()


def extract_text_from_hwp_mgmt(hwp_file_path: str) -> str:
    """hwp5txt ëª…ë ¹ì¤„ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ HWP íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        # ì„ì‹œ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        temp_txt_path = hwp_file_path + '.txt'
        
        # hwp5txt ëª…ë ¹ ì‹¤í–‰
        command = f'hwp5txt "{hwp_file_path}" --output "{temp_txt_path}"'
        result = os.system(command)
        
        if result == 0 and os.path.exists(temp_txt_path):
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
            with open(temp_txt_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_txt_path)
            
            if text_content and text_content.strip():
                print(f"âœ… hwp5txtë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(text_content)} ë¬¸ì")
                return text_content.strip()
            else:
                print(f"âš ï¸ hwp5txt ê²°ê³¼ ë¹„ì–´ìˆìŒ, ëŒ€ì²´ ë°©ë²• ì‹œë„")
                return extract_text_from_hwp_alternative_mgmt(hwp_file_path)
        else:
            print(f"âš ï¸ hwp5txt ëª…ë ¹ ì‹¤íŒ¨ (ì½”ë“œ: {result}), ëŒ€ì²´ ë°©ë²• ì‹œë„")
            return extract_text_from_hwp_alternative_mgmt(hwp_file_path)
            
    except Exception as e:
        print(f"âš ï¸ hwp5txt íŒŒì‹± ì‹¤íŒ¨: {e}")
        # ëŒ€ì²´ ë°©ë²• ì‹œë„
        return extract_text_from_hwp_alternative_mgmt(hwp_file_path)


def extract_text_from_hwp_alternative_mgmt(hwp_file_path: str) -> str:
    """olefileì„ ì‚¬ìš©í•œ HWP íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ëŒ€ì²´ ë°©ë²•"""
    try:
        import olefile
        
        # OLE íŒŒì¼ë¡œ HWP íŒŒì¼ ì—´ê¸°
        ole = olefile.OleFileIO(hwp_file_path)
        
        # í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ ì°¾ê¸°
        text_streams = []
        for stream_name in ole.listdir():
            if isinstance(stream_name, list) and len(stream_name) > 0:
                if 'BodyText' in stream_name[0] or 'Section' in stream_name[0]:
                    text_streams.append(stream_name)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = []
        for stream in text_streams:
            try:
                with ole.open(stream) as stream_data:
                    # ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì½ê¸°
                    data = stream_data.read()
                    
                    # ìœ ë‹ˆì½”ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    try:
                        # UTF-16 ì‹œë„
                        text = data.decode('utf-16le', errors='ignore')
                        # ì œì–´ ë¬¸ì ì œê±°
                        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                        if text.strip():
                            extracted_text.append(text.strip())
                    except:
                        # CP949 ì‹œë„
                        try:
                            text = data.decode('cp949', errors='ignore')
                            text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                            if text.strip():
                                extracted_text.append(text.strip())
                        except:
                            pass
            except:
                continue
        
        ole.close()
        
        full_text = '\n'.join(extracted_text)
        if full_text.strip():
            return full_text
        else:
            return "HWP íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"
            
    except Exception as e:
        print(f"âš ï¸ olefile íŒŒì‹±ë„ ì‹¤íŒ¨: {e}")
        return "HWP íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨"


def hwp2html(hwp_file_path: str, html_file_dir: str) -> bool:
    """HWP íŒŒì¼ì„ HTMLë¡œ ë³€í™˜"""
    try:
        # hwp5html ëª…ë ¹ì–´ ì‹œë„
        result = os.system(f'hwp5html --output "{html_file_dir}" "{hwp_file_path}"')
        return result == 0
    except:
        return False


def is_electronic_finance_related(content: str, filename: str = "", institution: str = "") -> bool:
    """ì§€ì •ëœ í‚¤ì›Œë“œë¡œ ì „ìê¸ˆìœµ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨"""
    keywords = [
        'ì „ìê¸ˆìœµ',
        'ì •ë³´ì²˜ë¦¬ìœ„íƒ',
        'ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì •',
        'ì‹ ìš©ì •ë³´ë²•',
        'ì‹ ìš©ì •ë³´ì˜ ì´ìš© ë° ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ',
        'ì‹ ìš©ì •ë³´ì—…ê°ë…ê·œì •',
        'ì‹ ìš©ì •ë³´ì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™'
    ]
    
    # ë¬¸ì„œ ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    for keyword in keywords:
        if keyword in content:
            return True
    
    # íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    for keyword in keywords:
        if keyword in filename:
            return True
    
    # ê¸°ê´€ëª…ì—ì„œ í‚¤ì›Œë“œ í™•ì¸
    for keyword in keywords:
        if keyword in institution:
            return True
    
    return False


def extract_found_keywords(content: str, filename: str = "", institution: str = "") -> List[str]:
    """ë°œê²¬ëœ í‚¤ì›Œë“œ ëª©ë¡ ë°˜í™˜"""
    keywords = [
        'ì „ìê¸ˆìœµ',
        'ì •ë³´ì²˜ë¦¬ìœ„íƒ',
        'ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì •',
        'ì‹ ìš©ì •ë³´ë²•',
        'ì‹ ìš©ì •ë³´ì˜ ì´ìš© ë° ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ',
        'ì‹ ìš©ì •ë³´ì—…ê°ë…ê·œì •',
        'ì‹ ìš©ì •ë³´ì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™'
    ]
    
    found_keywords = []
    
    # ë¬¸ì„œ ë‚´ìš©, íŒŒì¼ëª…, ê¸°ê´€ëª…ì—ì„œ í‚¤ì›Œë“œ ì°¾ê¸°
    all_text = f"{content} {filename} {institution}"
    
    for keyword in keywords:
        if keyword in all_text:
            found_keywords.append(keyword)
    
    return found_keywords


class FSSManagementParser:
    """ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ ë¬¸ì„œ íŒŒì‹± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.management_patterns = {
            'institution': [
                r'íšŒì‚¬ëª…\s*[:ï¼š]\s*([^\n]+)',
                r'ê¸°ê´€ëª…\s*[:ï¼š]\s*([^\n]+)',
                r'ê¸ˆìœµíšŒì‚¬ëª…\s*[:ï¼š]\s*([^\n]+)',
                r'ëŒ€ìƒê¸°ê´€\s*[:ï¼š]\s*([^\n]+)'
            ],
            'date': [
                r'ê³µì‹œì¼\s*[:ï¼š]\s*(\d{4}[\.\-\s]*\d{1,2}[\.\-\s]*\d{1,2})',
                r'ê³µì‹œì¼ì\s*[:ï¼š]\s*(\d{4}[\.\-\s]*\d{1,2}[\.\-\s]*\d{1,2})',
                r'ì‘ì„±ì¼\s*[:ï¼š]\s*(\d{4}[\.\-\s]*\d{1,2}[\.\-\s]*\d{1,2})',
                r'(\d{4}[\.\-\s]*\d{1,2}[\.\-\s]*\d{1,2})'
            ],
            'management_type': [
                r'ê²½ì˜ìœ ì˜ì‚¬í•­\s*[:ï¼š]?\s*([^\n]+)',
                r'ìœ ì˜ì‚¬í•­\s*[:ï¼š]?\s*([^\n]+)',
                r'ë‚´ìš©\s*[:ï¼š]?\s*([^\n]+)'
            ],
        }
    
    def extract_from_pdf(self, file_path: str) -> Dict[str, Any]:
        """PDF íŒŒì¼ì—ì„œ ê²½ì˜ìœ ì˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            doc = fitz.open(file_path)
            full_text = ""
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                text = page.get_text()
                full_text += text + "\n"
            
            doc.close()
            
            return self.parse_management_content(full_text, file_path)
            
        except Exception as e:
            print(f"âŒ PDF íŒŒì‹± ì—ëŸ¬ {file_path}: {e}")
            return self.create_empty_result(file_path)
    
    def extract_from_hwp(self, file_path: str) -> Dict[str, Any]:
        """HWP íŒŒì¼ì—ì„œ ê²½ì˜ìœ ì˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            print(f"ğŸ”„ HWP íŒŒì¼ íŒŒì‹± ì‹œì‘: {os.path.basename(file_path)}")
            
            # hwp5txtë¥¼ ì‚¬ìš©í•œ ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = extract_text_from_hwp_mgmt(file_path)
            
            if full_text and full_text.strip() and "íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨" not in full_text:
                print(f"âœ… HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(full_text)} ë¬¸ì")
                return self.parse_management_content(full_text, file_path)
            else:
                print(f"âš ï¸ HWP ì§ì ‘ íŒŒì‹± ì‹¤íŒ¨, HTML ë³€í™˜ ì‹œë„")
                
                # HTML ë³€í™˜ ë°©ë²• ì‹œë„
                hwp_file_dir = os.path.dirname(file_path)
                hwp_file_name = os.path.basename(file_path)
                html_file_dir = os.path.join(hwp_file_dir, hwp_file_name.split('.')[0])
                html_file_path = os.path.join(html_file_dir, 'index.xhtml')
                
                conversion_success = hwp2html(file_path, html_file_dir)
                
                if conversion_success and os.path.exists(html_file_path):
                    # HTML íŒŒì‹±
                    with open(html_file_path, 'rt', encoding='utf-8') as f:
                        page = BeautifulSoup(f.read(), 'html.parser')
                    
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
                    if os.path.exists(html_file_dir):
                        shutil.rmtree(html_file_dir)
                    
                    full_text = page.get_text(separator='\n')
                    print(f"âœ… HWP HTML ë³€í™˜ ì„±ê³µ: {len(full_text)} ë¬¸ì")
                    
                    return self.parse_management_content(full_text, file_path)
                else:
                    print(f"âš ï¸ HTML ë³€í™˜ë„ ì‹¤íŒ¨, ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ")
                    return self.extract_hwp_alternative(file_path)
            
        except Exception as e:
            print(f"âŒ HWP íŒŒì‹± ì—ëŸ¬ {file_path}: {e}")
            return self.extract_hwp_alternative(file_path)
    
    def extract_hwp_alternative(self, file_path: str) -> Dict[str, Any]:
        """HWP íŒŒì¼ ëŒ€ì²´ ì²˜ë¦¬ ë°©ë²•"""
        try:
            # íŒŒì¼ ì •ë³´ë§Œìœ¼ë¡œ ê¸°ë³¸ êµ¬ì¡° ìƒì„±
            file_name = os.path.basename(file_path)
            doc_id = os.path.splitext(file_name)[0]
            
            # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
            institution = "ë¯¸í™•ì¸"
            date = "ë¯¸í™•ì¸"
            
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
            date_match = re.search(r'(\d{8})', file_name)
            if date_match:
                date_str = date_match.group(1)
                date = f"{date_str[:4]}.{date_str[4:6]}.{date_str[6:8]}"
            
            # íŒŒì¼ëª…ì—ì„œ ê¸°ê´€ëª… ì¶”ì¶œ ì‹œë„
            inst_match = re.search(r'MGMT_\d+_([^_]+)_', file_name)
            if inst_match:
                institution = inst_match.group(1).replace('_', ' ')
            
            # ê¸°ë³¸ ê²°ê³¼ êµ¬ì„±
            result = {
                "doc_id": doc_id,
                "source_file": file_name,
                "institution": institution,
                "date": date,
                "is_relevant": False,
                "found_keywords": [],
                "content": {
                    "full_text": "HWP íŒŒì¼ ë³€í™˜ ë„êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                },
                "metadata": {
                    "doc_type": "ê²½ì˜ìœ ì˜ì‚¬í•­",
                    "char_count": 0,
                    "estimated_tokens": 0,
                    "created_at": datetime.now().strftime("%Y-%m-%d"),
                    "regulations": [],
                    "file_extension": pathlib.Path(file_path).suffix.lower(),
                    "conversion_status": "ë³€í™˜ ì‹¤íŒ¨"
                },
                "llm_metadata": {
                    "keywords": [],
                    "regulations": "",
                    "fines": "",
                    "executive_sanction": ""
                },
                "quality_score": 1,
                "status": "HWP ë³€í™˜ í•„ìš”"
            }
            
            print(f"ğŸ“„ HWP íŒŒì¼ ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ: {institution} ({date})")
            return result
            
        except Exception as e:
            print(f"âŒ HWP ëŒ€ì²´ ì²˜ë¦¬ ì—ëŸ¬ {file_path}: {e}")
            return self.create_empty_result(file_path)
    
    def extract_from_hwpx(self, file_path: str) -> Dict[str, Any]:
        """HWPX íŒŒì¼ì—ì„œ ê²½ì˜ìœ ì˜ ì •ë³´ ì¶”ì¶œ"""
        try:
            full_text = ""
            
            with tempfile.TemporaryDirectory() as temp_dir:
                # HWPX íŒŒì¼ì„ ZIPìœ¼ë¡œ ì••ì¶•í•´ì œ
                with zipfile.ZipFile(file_path, 'r') as zip_ref:
                    zip_ref.extractall(temp_dir)
                
                # section XML íŒŒì¼ë“¤ ì°¾ê¸°
                contents_dir = os.path.join(temp_dir, 'Contents')
                section_files = [f for f in os.listdir(contents_dir) if f.startswith('section') and f.endswith('.xml')]
                
                for section_file in sorted(section_files):
                    section_path = os.path.join(contents_dir, section_file)
                    
                    with open(section_path, 'r', encoding='utf-8') as f:
                        xml_content = f.read()
                    
                    # BeautifulSoupìœ¼ë¡œ XML íŒŒì‹±
                    soup = BeautifulSoup(xml_content, 'lxml-xml')
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text_elements = soup.find_all(['hp:t'])
                    for elem in text_elements:
                        text = elem.get_text(strip=True)
                        if text:
                            full_text += text + "\n"
            
            return self.parse_management_content(full_text, file_path)
            
        except Exception as e:
            print(f"âŒ HWPX íŒŒì‹± ì—ëŸ¬ {file_path}: {e}")
            return self.create_empty_result(file_path)
    
    def parse_management_content(self, text: str, file_path: str) -> Dict[str, Any]:
        """ê²½ì˜ìœ ì˜ ë‚´ìš© íŒŒì‹±"""
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        institution = self.extract_pattern(text, self.management_patterns['institution'])
        date = self.extract_pattern(text, self.management_patterns['date'])
        management_type = self.extract_pattern(text, self.management_patterns['management_type'])
        
        # ê´€ë ¨ ê·œì • ì¶”ì¶œ
        regulations = self.extract_regulations(text)
        
        # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
        file_name = os.path.basename(file_path)
        doc_id = os.path.splitext(file_name)[0]
        
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
        if not date:
            date_match = re.search(r'(\d{8})', file_name)
            if date_match:
                date_str = date_match.group(1)
                date = f"{date_str[:4]}.{date_str[4:6]}.{date_str[6:8]}"
        
        # íŒŒì¼ëª…ì—ì„œ ê¸°ê´€ëª… ì¶”ì¶œ ì‹œë„
        if not institution:
            inst_match = re.search(r'MGMT_\d+_([^_]+)_', file_name)
            if inst_match:
                institution = inst_match.group(1).replace('_', ' ')
        
        # ì „ìê¸ˆìœµ ê´€ë ¨ ì—¬ë¶€ í™•ì¸
        is_relevant = is_electronic_finance_related(text, file_name, institution or "")
        found_keywords = extract_found_keywords(text, file_name, institution or "")
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "doc_id": doc_id,
            "source_file": file_name,
            "institution": institution or "ë¯¸í™•ì¸",
            "date": date or "ë¯¸í™•ì¸",
            "is_relevant": is_relevant,
            "found_keywords": found_keywords,
            "content": {
                "full_text": clean_text(text)
            },
            "metadata": {
                "doc_type": "ê²½ì˜ìœ ì˜ì‚¬í•­",
                "char_count": len(text),
                "estimated_tokens": len(text.split()),
                "created_at": datetime.now().strftime("%Y-%m-%d"),
                "regulations": regulations,
                "file_extension": pathlib.Path(file_path).suffix.lower()
            },
            "llm_metadata": {
                "keywords": [],
                "regulations": "",
                "fines": "",
                "executive_sanction": ""
            },
            "quality_score": self.calculate_quality_score(institution, date, management_type),
            "status": "ì²˜ë¦¬ì™„ë£Œ"
        }
        
        return result
    
    def extract_pattern(self, text: str, patterns: List[str]) -> Optional[str]:
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ"""
        for pattern in patterns:
            match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)
            if match:
                return clean_text(match.group(1))
        return None
    
    def extract_management_details(self, text: str) -> List[Dict[str, str]]:
        """ê²½ì˜ìœ ì˜ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ"""
        details = []
        
        # ê²½ì˜ìœ ì˜ ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
        detail_patterns = [
            r'ê²½ì˜ìœ ì˜ì‚¬í•­\s*[:ï¼š]?\s*\n([^\n]+(?:\n(?![0-9]+\.|ê°€\.|ë‚˜\.|ë‹¤\.)[^\n]+)*)',
            r'ìœ ì˜ì‚¬í•­\s*[:ï¼š]?\s*\n([^\n]+(?:\n(?![0-9]+\.|ê°€\.|ë‚˜\.|ë‹¤\.)[^\n]+)*)',
            r'ë‚´ìš©\s*[:ï¼š]?\s*\n([^\n]+(?:\n(?![0-9]+\.|ê°€\.|ë‚˜\.|ë‹¤\.)[^\n]+)*)'
        ]
        
        # í•­ëª©ë³„ íŒ¨í„´ (ê°€. ë‚˜. ë‹¤. ë˜ëŠ” (1) (2) (3) í˜•ì‹)
        item_patterns = [
            r'([ê°€-í•˜]\.\s*[^\n]+(?:\n(?![ê°€-í•˜]\.|[0-9]+\.)[^\n]+)*)',
            r'(\([0-9]+\)\s*[^\n]+(?:\n(?!\([0-9]+\)|[ê°€-í•˜]\.)[^\n]+)*)',
            r'([0-9]+\.\s*[^\n]+(?:\n(?![0-9]+\.|[ê°€-í•˜]\.)[^\n]+)*)'
        ]
        
        for pattern in detail_patterns:
            match = re.search(pattern, text, re.MULTILINE | re.DOTALL)
            if match:
                detail_section = match.group(1)
                
                # í•­ëª©ë³„ë¡œ ë¶„ë¦¬
                for item_pattern in item_patterns:
                    items = re.findall(item_pattern, detail_section, re.MULTILINE | re.DOTALL)
                    for item in items:
                        # ì œëª©ê³¼ ë‚´ìš© ë¶„ë¦¬
                        lines = item.strip().split('\n', 1)
                        if lines:
                            title = clean_text(lines[0])
                            content = clean_text(lines[1]) if len(lines) > 1 else ""
                            
                            details.append({
                                "title": title,
                                "content": content
                            })
        
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        if not details:
            keywords = [
                'ì „ìê¸ˆìœµ',
                'ì •ë³´ì²˜ë¦¬ìœ„íƒ',
                'ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì •',
                'ì‹ ìš©ì •ë³´ë²•',
                'ì‹ ìš©ì •ë³´ì˜ ì´ìš© ë° ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ',
                'ì‹ ìš©ì •ë³´ì—…ê°ë…ê·œì •',
                'ì‹ ìš©ì •ë³´ì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™'
            ]
            
            for keyword in keywords:
                if keyword in text:
                    # í‚¤ì›Œë“œ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    idx = text.find(keyword)
                    start = max(0, idx - 200)
                    end = min(len(text), idx + 800)
                    snippet = text[start:end]
                    
                    details.append({
                        "title": f"{keyword} ê´€ë ¨ ë‚´ìš©",
                        "content": clean_text(snippet)
                    })
                    
                    if len(details) >= 5:  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ
                        break
        
        return details
    
    def extract_regulations(self, text: str) -> List[str]:
        """ê´€ë ¨ ê·œì • ì¶”ì¶œ"""
        regulations = []
        
        # ë²•ë ¹ íŒ¨í„´
        law_patterns = [
            r'ã€Œ([^ã€]+)ã€\s*ì œ(\d+)ì¡°',
            r'([ê°€-í£]+ë²•)\s*ì œ(\d+)ì¡°',
            r'([ê°€-í£]+ê·œì •)\s*ì œ(\d+)ì¡°',
            r'([ê°€-í£]+ê·œì¹™)\s*ì œ(\d+)ì¡°',
            r'([ê°€-í£]+ì‹œí–‰ì„¸ì¹™)\s*ì œ(\d+)ì¡°'
        ]
        
        for pattern in law_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                law_name = match[0]
                article = f"ì œ{match[1]}ì¡°"
                regulation = f"{law_name} {article}"
                if regulation not in regulations:
                    regulations.append(regulation)
        
        return regulations[:10]  # ìµœëŒ€ 10ê°œê¹Œì§€
    
    def calculate_quality_score(self, institution: str, date: str, management_type: str, details: List = None) -> int:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (1-5)"""
        score = 1
        
        if institution and institution != "ë¯¸í™•ì¸":
            score += 1
        if date and date != "ë¯¸í™•ì¸":
            score += 1
        if management_type and management_type != "ë¯¸í™•ì¸":
            score += 1
        if details and len(details) > 0:
            score += 1
        
        return min(score, 5)
    
    def create_empty_result(self, file_path: str) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        file_name = os.path.basename(file_path)
        doc_id = os.path.splitext(file_name)[0]
        
        return {
            "doc_id": doc_id,
            "source_file": file_name,
            "institution": "íŒŒì‹±ì‹¤íŒ¨",
            "date": "íŒŒì‹±ì‹¤íŒ¨",
            "is_relevant": False,
            "found_keywords": [],
            "content": {
                "full_text": ""
            },
            "metadata": {
                "doc_type": "ê²½ì˜ìœ ì˜ì‚¬í•­",
                "char_count": 0,
                "estimated_tokens": 0,
                "created_at": datetime.now().strftime("%Y-%m-%d"),
                "regulations": [],
                "file_extension": pathlib.Path(file_path).suffix.lower()
            },
            "llm_metadata": {
                "keywords": [],
                "regulations": "",
                "fines": "",
                "executive_sanction": ""
            },
            "quality_score": 0,
            "status": "íŒŒì‹±ì‹¤íŒ¨"
        }
    
    def process_directory(self, input_dir: str, output_json: str) -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ê²½ì˜ìœ ì˜ ë¬¸ì„œ ì²˜ë¦¬"""
        results = []
        relevant_results = []
        stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "relevant_files": 0,
            "file_types": {"pdf": 0, "hwp": 0, "hwpx": 0}
        }
        
        if not os.path.exists(input_dir):
            print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_dir}")
            return {"data": results, "stats": stats}
        
        # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        files = []
        for file_name in os.listdir(input_dir):
            if file_name.lower().endswith(('.pdf', '.hwp', '.hwpx')):
                files.append(os.path.join(input_dir, file_name))
        
        stats["total_files"] = len(files)
        print(f"\nğŸ“ ì´ {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        # ê° íŒŒì¼ ì²˜ë¦¬
        for file_path in sorted(files):
            file_name = os.path.basename(file_path)
            ext = pathlib.Path(file_path).suffix.lower()
            
            print(f"\nğŸ” ì²˜ë¦¬ ì¤‘: {file_name}")
            
            try:
                if ext == '.pdf':
                    result = self.extract_from_pdf(file_path)
                    stats["file_types"]["pdf"] += 1
                elif ext == '.hwp':
                    result = self.extract_from_hwp(file_path)
                    stats["file_types"]["hwp"] += 1
                elif ext == '.hwpx':
                    result = self.extract_from_hwpx(file_path)
                    stats["file_types"]["hwpx"] += 1
                else:
                    continue
                
                if result["status"] == "íŒŒì‹±ì‹¤íŒ¨":
                    stats["failed_files"] += 1
                else:
                    stats["processed_files"] += 1
                
                results.append(result)
                
                # ê´€ë ¨ ë¬¸ì„œë§Œ ë³„ë„ ì €ì¥
                if result.get("is_relevant", False):
                    relevant_results.append(result)
                    stats["relevant_files"] += 1
                    print(f"âœ… ê´€ë ¨ ë¬¸ì„œ: {result['institution']} ({result['date']}) - í‚¤ì›Œë“œ: {', '.join(result['found_keywords'])}")
                else:
                    print(f"â­ï¸  ì¼ë°˜ ë¬¸ì„œ: {result['institution']} ({result['date']})")
                
            except Exception as e:
                print(f"âŒ ì—ëŸ¬: {file_name} - {e}")
                stats["failed_files"] += 1
                results.append(self.create_empty_result(file_path))
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        all_output_data = {
            "metadata": {
                "source": "ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ì‚¬í•­ ê³µì‹œ",
                "processed_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_documents": len(results),
                "relevant_documents": len(relevant_results),
                "keywords": [
                    "ì „ìê¸ˆìœµ",
                    "ì •ë³´ì²˜ë¦¬ìœ„íƒ",
                    "ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì •",
                    "ì‹ ìš©ì •ë³´ë²•",
                    "ì‹ ìš©ì •ë³´ì˜ ì´ìš© ë° ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ",
                    "ì‹ ìš©ì •ë³´ì—…ê°ë…ê·œì •",
                    "ì‹ ìš©ì •ë³´ì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™"
                ]
            },
            "statistics": stats,
            "documents": results
        }
        
        # ê´€ë ¨ ë¬¸ì„œë§Œ ì½”í¼ìŠ¤ ì €ì¥
        corpus_output_data = {
            "metadata": {
                "source": "ê¸ˆìœµê°ë…ì› ê²½ì˜ìœ ì˜ì‚¬í•­ ê³µì‹œ",
                "processed_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "total_documents": len(relevant_results),
                "keywords": [
                    "ì „ìê¸ˆìœµ",
                    "ì •ë³´ì²˜ë¦¬ìœ„íƒ",
                    "ì •ë³´ì²˜ë¦¬ ì—…ë¬´ ìœ„íƒì— ê´€í•œ ê·œì •",
                    "ì‹ ìš©ì •ë³´ë²•",
                    "ì‹ ìš©ì •ë³´ì˜ ì´ìš© ë° ë³´í˜¸ì— ê´€í•œ ë²•ë¥ ",
                    "ì‹ ìš©ì •ë³´ì—…ê°ë…ê·œì •",
                    "ì‹ ìš©ì •ë³´ì—…ê°ë…ì—…ë¬´ì‹œí–‰ì„¸ì¹™"
                ]
            },
            "statistics": {
                "total_processed": stats["processed_files"],
                "relevant_documents": stats["relevant_files"],
                "relevance_rate": round(stats["relevant_files"] / max(stats["processed_files"], 1) * 100, 2)
            },
            "documents": relevant_results
        }
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        os.makedirs(os.path.dirname(output_json), exist_ok=True)
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(all_output_data, f, ensure_ascii=False, indent=2)
        
        # ì½”í¼ìŠ¤ ì €ì¥
        corpus_file = output_json.replace('.json', '_corpus.json')
        with open(corpus_file, 'w', encoding='utf-8') as f:
            json.dump(corpus_output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ‰ ì™„ë£Œ!")
        print(f"ğŸ“Š í†µê³„:")
        print(f"  - ì „ì²´ íŒŒì¼: {stats['total_files']}ê°œ")
        print(f"  - ì„±ê³µ: {stats['processed_files']}ê°œ")
        print(f"  - ì‹¤íŒ¨: {stats['failed_files']}ê°œ")
        print(f"  - ê´€ë ¨ ë¬¸ì„œ: {stats['relevant_files']}ê°œ")
        print(f"  - ê´€ë ¨ë„: {round(stats['relevant_files'] / max(stats['processed_files'], 1) * 100, 2)}%")
        print(f"  - PDF: {stats['file_types']['pdf']}ê°œ")
        print(f"  - HWP: {stats['file_types']['hwp']}ê°œ")
        print(f"  - HWPX: {stats['file_types']['hwpx']}ê°œ")
        print(f"\nğŸ’¾ ì „ì²´ ê²°ê³¼: {output_json}")
        print(f"ğŸ’¾ ì½”í¼ìŠ¤ ê²°ê³¼: {corpus_file}")
        
        return all_output_data


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    parser = FSSManagementParser()
    
    # ì…ë ¥ ë””ë ‰í† ë¦¬ì™€ ì¶œë ¥ íŒŒì¼ ì„¤ì •
    input_directory = "../../data/FSS_MANAGEMENT"
    output_file = "./data/fss_management_parsed.json"
    
    # ë””ë ‰í† ë¦¬ ì²˜ë¦¬
    result = parser.process_directory(input_directory, output_file) 