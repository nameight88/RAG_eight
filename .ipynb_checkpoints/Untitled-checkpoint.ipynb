{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976b56e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ì œì¬ í¬ë¡¤ë§ ì‹œì‘: 2025-07-13 21:19:13.888522 ===\n",
      "ê¸°ì¡´ ì œì¬ ì •ë³´: 0ê°œ\n",
      "\n",
      "ğŸ“„ 1í˜ì´ì§€ ì¡°íšŒ ì¤‘...\n",
      "ğŸ” ìš”ì²­ URL: https://www.fss.or.kr/fss/job/openInfo/list.do\n",
      "ğŸ” íŒŒë¼ë¯¸í„°: {'menuNo': '200476', 'pageIndex': 1, 'sdate': '2014-01-01', 'edate': '2025-07-13', 'searchCnd': '4', 'searchWrd': ''}\n",
      "ğŸ” ì‘ë‹µ ìƒíƒœ: 200\n",
      "ğŸ” HTML ê¸¸ì´: 261693\n",
      "ğŸ” Title: ê²€ì‚¬ê²°ê³¼ì œì¬(ëª©ë¡) | ì œì¬ê´€ë ¨ ê³µì‹œ | ê²€ì‚¬Â·ì œì¬ | ì—…ë¬´ìë£Œ | \n",
      "ğŸ” ì „ì²´ ê±´ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê³„ì† ì§„í–‰\n",
      "ğŸ” í…Œì´ë¸” í´ë˜ìŠ¤: None\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202000323&emOpenSeq=4, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202000323, emOpenSeq=4\n",
      "ğŸ” í•­ëª© ë°œê²¬: í•˜ë‚˜ì¦ê¶Œì£¼ì‹íšŒì‚¬ - ê¸ˆìœµíˆ¬ìê²€ì‚¬1êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300754&emOpenSeq=2, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202300754, emOpenSeq=2\n",
      "ğŸ” í•­ëª© ë°œê²¬: ì‹ í•œë¼ì´í”„ìƒëª…ë³´í—˜ì£¼ì‹íšŒì‚¬ - ë³´í—˜ê²€ì‚¬1êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300379&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202300379, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: ì¼€ì´ì—ìŠ¤ì‹ ìš©ì •ë³´ ì£¼ì‹íšŒì‚¬ - ì„œë¯¼ê¸ˆìœµë³´í˜¸êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300687&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202300687, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: ì£¼ì‹íšŒì‚¬ ë„¥ìŠ¤í…Œë¼íˆ¬ìì¼ì„ - ê¸ˆìœµíˆ¬ìê²€ì‚¬2êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300381&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202300381, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: ì„¸ì¼ì‹ ìš©ì •ë³´ ì£¼ì‹íšŒì‚¬ - ì„œë¯¼ê¸ˆìœµë³´í˜¸êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202400688&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202400688, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: ë™ì›ì œì¼ì €ì¶•ì€í–‰ - ì¤‘ì†Œê¸ˆìœµê²€ì‚¬1êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300504&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202300504, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: ëŒ€ë•ìì‚°ìš´ìš© ì£¼ì‹íšŒì‚¬ - ê¸ˆìœµíˆ¬ìê²€ì‚¬3êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202400617&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202400617, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: ìš°ë¦¬ìì‚°ì‹ íƒ - ê¸ˆìœµíˆ¬ìê²€ì‚¬3êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202000041&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202000041, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: ì œì´ë¹„ìì‚°ìš´ìš© - ê¸ˆìœµíˆ¬ìê²€ì‚¬2êµ­\n",
      "ğŸ” TD ê°œìˆ˜: 6\n",
      "ğŸ” ë§í¬ ì •ë³´: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202200487&emOpenSeq=1, onclick=None\n",
      "ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo=202200487, emOpenSeq=1\n",
      "ğŸ” í•­ëª© ë°œê²¬: í”¼ì—í”„ì”¨í…Œí¬ë†€ë¡œì§€ìŠ¤ ì£¼ì‹íšŒì‚¬ - ì „ìê¸ˆìœµê²€ì‚¬êµ­\n",
      "  - 10ê°œ í•­ëª© ë°œê²¬\n",
      "\n",
      "âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬: NameError - name 'content' is not defined\n",
      "\n",
      "âœ¨ ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ì œì¬ í¬ë¡¤ë§ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/ly32t5wx3p5b36cvk238t1840000gn/T/ipykernel_64586/235809900.py:88: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  total_elem = soup.find(text=re.compile(r'ì´\\s*\\d+\\s*ê±´'))\n"
     ]
    }
   ],
   "source": [
    "# ê¸ˆìœµê°ë…ì› í¬ë¡¤ë§\n",
    "\"\"\"\n",
    "ê¸ˆìœµê°ë…ì› ê²€ì‚¬ê²°ê³¼ì œì¬ ì „ìê¸ˆìœµ ê´€ë ¨ ë¬¸ì„œ í¬ë¡¤ëŸ¬\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "\n",
    "def is_electronic_finance_related(department: str, content: str = \"\") -> bool:\n",
    "    \"\"\"ê´€ë ¨ë¶€ì„œëª…ê³¼ ë‚´ìš©ìœ¼ë¡œ ì „ìê¸ˆìœµ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨\"\"\"\n",
    "    # ì „ìê¸ˆìœµ ê´€ë ¨ ë¶€ì„œ\n",
    "    dept_keywords = [\n",
    "        'ì „ìê¸ˆìœµê²€ì‚¬êµ­', 'ì „ìê¸ˆìœµ', 'ITê²€ì‚¬', 'ì •ë³´ê¸°ìˆ ', 'IT', 'ì‚¬ì´ë²„'\n",
    "    ]\n",
    "    \n",
    "    # ì „ìê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ\n",
    "    content_keywords = [\n",
    "        'ì „ìê¸ˆìœµ', 'í•€í…Œí¬', 'ì˜¨ë¼ì¸', 'ëª¨ë°”ì¼', 'ì¸í„°ë„·ë±…í‚¹', 'ì „ìì§€ê¸‰',\n",
    "        'ì „ìê²°ì œ', 'API', 'ì˜¤í”ˆë±…í‚¹', 'ë§ˆì´ë°ì´í„°', 'ì „ìì„œëª…', 'ì •ë³´ë³´í˜¸',\n",
    "        'ê°œì¸ì •ë³´', 'ì‚¬ì´ë²„', 'í•´í‚¹', 'ë³´ì•ˆ', 'ì „ì‚°', 'IT', 'ì‹œìŠ¤í…œ', 'ë„¤íŠ¸ì›Œí¬'\n",
    "    ]\n",
    "    \n",
    "    # ë¶€ì„œëª… í™•ì¸\n",
    "    for keyword in dept_keywords:\n",
    "        if keyword in department:\n",
    "            return True\n",
    "    \n",
    "    # ë‚´ìš© í™•ì¸\n",
    "    if content:\n",
    "        for keyword in content_keywords:\n",
    "            if keyword in content:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "class FSSCrawler:\n",
    "    \"\"\"ê¸ˆìœµê°ë…ì› í¬ë¡¤ëŸ¬\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"../../\"):\n",
    "        self.base_url = \"https://www.fss.or.kr\"\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "    def get_sanction_list(self, page: int = 1) -> Dict:\n",
    "        \"\"\"ê²€ì‚¬ê²°ê³¼ì œì¬ ëª©ë¡ ì¡°íšŒ\"\"\"\n",
    "        url = f\"{self.base_url}/fss/job/openInfo/list.do\"\n",
    "        params = {\n",
    "            'menuNo': '200476',\n",
    "            'pageIndex': page,\n",
    "            'sdate': '2014-01-01',\n",
    "            'edate': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'searchCnd': '4',\n",
    "            'searchWrd': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ” ìš”ì²­ URL: {url}\")\n",
    "            print(f\"ğŸ” íŒŒë¼ë¯¸í„°: {params}\")\n",
    "            \n",
    "            resp = requests.get(url, params=params, headers=self.headers, timeout=30)\n",
    "            print(f\"ğŸ” ì‘ë‹µ ìƒíƒœ: {resp.status_code}\")\n",
    "            \n",
    "            if resp.status_code != 200:\n",
    "                print(f\"âŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: ìƒíƒœì½”ë“œ {resp.status_code}\")\n",
    "                return {}\n",
    "                \n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            # ë””ë²„ê¹…: HTML ì¼ë¶€ ì¶œë ¥\n",
    "            print(f\"ğŸ” HTML ê¸¸ì´: {len(resp.text)}\")\n",
    "            print(f\"ğŸ” Title: {soup.find('title').text if soup.find('title') else 'No title'}\")\n",
    "            \n",
    "            # ì „ì²´ ê±´ìˆ˜ ì¶”ì¶œ\n",
    "            total_text = soup.find('span', class_='total')\n",
    "            total_count = 0\n",
    "            if total_text:\n",
    "                match = re.search(r'(\\d+)ê±´', total_text.text)\n",
    "                if match:\n",
    "                    total_count = int(match.group(1))\n",
    "                print(f\"ğŸ” ì „ì²´ ê±´ìˆ˜: {total_count}\")\n",
    "            else:\n",
    "                # ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì „ì²´ ê±´ìˆ˜ ì°¾ê¸°\n",
    "                total_elem = soup.find(text=re.compile(r'ì´\\s*\\d+\\s*ê±´'))\n",
    "                if total_elem:\n",
    "                    match = re.search(r'(\\d+)', total_elem)\n",
    "                    if match:\n",
    "                        total_count = int(match.group(1))\n",
    "                        print(f\"ğŸ” ì „ì²´ ê±´ìˆ˜(íŒ¨í„´2): {total_count}\")\n",
    "                else:\n",
    "                    # í˜ì´ì§• ì •ë³´ì—ì„œ ì¶”ì¶œ\n",
    "                    paging = soup.find('div', class_='paging')\n",
    "                    if paging:\n",
    "                        last_page = paging.find_all('a')[-1]\n",
    "                        if last_page:\n",
    "                            onclick = last_page.get('onclick', '')\n",
    "                            match = re.search(r'goPage\\((\\d+)\\)', onclick)\n",
    "                            if match:\n",
    "                                last_page_num = int(match.group(1))\n",
    "                                total_count = last_page_num * 10  # ëŒ€ëµì ì¸ ì¶”ì •\n",
    "                                print(f\"ğŸ” ì „ì²´ ê±´ìˆ˜(í˜ì´ì§• ì¶”ì •): ì•½ {total_count}ê±´\")\n",
    "                    \n",
    "                    if total_count == 0:\n",
    "                        print(\"ğŸ” ì „ì²´ ê±´ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê³„ì† ì§„í–‰\")\n",
    "                        total_count = 999999  # ì¶©ë¶„íˆ í° ìˆ˜ë¡œ ì„¤ì •\n",
    "            \n",
    "            # í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "            items = []\n",
    "            table = soup.find('table', class_='tbl_list')\n",
    "            if not table:\n",
    "                # ë‹¤ë¥¸ í´ë˜ìŠ¤ëª… ì‹œë„\n",
    "                table = soup.find('table', class_='list')\n",
    "                if not table:\n",
    "                    table = soup.find('table')\n",
    "                    print(f\"ğŸ” í…Œì´ë¸” í´ë˜ìŠ¤: {table.get('class') if table else 'No table found'}\")\n",
    "            \n",
    "            if table:\n",
    "                tbody = table.find('tbody')\n",
    "                if tbody:\n",
    "                    for tr in tbody.find_all('tr'):\n",
    "                        tds = tr.find_all('td')\n",
    "                        print(f\"ğŸ” TD ê°œìˆ˜: {len(tds)}\")\n",
    "                        if len(tds) >= 6:\n",
    "                            # ë²ˆí˜¸, ì œì¬ëŒ€ìƒê¸°ê´€, ì œì¬ì¡°ì¹˜ìš”êµ¬ì¼, ì œì¬ì¡°ì¹˜ìš”êµ¬ë‚´ìš©(ë§í¬), ê´€ë ¨ë¶€ì„œ, ì¡°íšŒìˆ˜\n",
    "                            item = {\n",
    "                                'no': tds[0].get_text(strip=True),\n",
    "                                'institution': tds[1].get_text(strip=True),\n",
    "                                'date': tds[2].get_text(strip=True),\n",
    "                                'department': tds[4].get_text(strip=True),\n",
    "                                'views': tds[5].get_text(strip=True)\n",
    "                            }\n",
    "                            \n",
    "                            # ìƒì„¸ ë§í¬ ì¶”ì¶œ\n",
    "                            link_tag = tds[3].find('a')\n",
    "                            if link_tag:\n",
    "                                # href ì†ì„± í™•ì¸\n",
    "                                href = link_tag.get('href')\n",
    "                                onclick = link_tag.get('onclick')\n",
    "                                print(f\"ğŸ” ë§í¬ ì •ë³´: href={href}, onclick={onclick}\")\n",
    "                                \n",
    "                                if onclick:\n",
    "                                    # JavaScript í•¨ìˆ˜ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ\n",
    "                                    match = re.search(r\"fnView\\('([^']+)','([^']+)'\\)\", onclick)\n",
    "                                    if match:\n",
    "                                        item['examMgmtNo'] = match.group(1)\n",
    "                                        item['emOpenSeq'] = match.group(2)\n",
    "                                        print(f\"ğŸ” ì¶”ì¶œëœ ID: examMgmtNo={item['examMgmtNo']}, emOpenSeq={item['emOpenSeq']}\")\n",
    "                                    else:\n",
    "                                        # ë‹¤ë¥¸ íŒ¨í„´ ì‹œë„\n",
    "                                        match = re.search(r\"fnView\\(([^,]+),([^)]+)\\)\", onclick)\n",
    "                                        if match:\n",
    "                                            item['examMgmtNo'] = match.group(1).strip(\"'\\\"\")\n",
    "                                            item['emOpenSeq'] = match.group(2).strip(\"'\\\"\")\n",
    "                                            print(f\"ğŸ” ì¶”ì¶œëœ ID(íŒ¨í„´2): examMgmtNo={item['examMgmtNo']}, emOpenSeq={item['emOpenSeq']}\")\n",
    "                                elif href and 'examMgmtNo' in href:\n",
    "                                    # URL íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ\n",
    "                                    import urllib.parse\n",
    "                                    parsed = urllib.parse.urlparse(href)\n",
    "                                    params = urllib.parse.parse_qs(parsed.query)\n",
    "                                    item['examMgmtNo'] = params.get('examMgmtNo', [''])[0]\n",
    "                                    item['emOpenSeq'] = params.get('emOpenSeq', [''])[0]\n",
    "                                    print(f\"ğŸ” URLì—ì„œ ì¶”ì¶œ: examMgmtNo={item['examMgmtNo']}, emOpenSeq={item['emOpenSeq']}\")\n",
    "                            \n",
    "                            items.append(item)\n",
    "                            print(f\"ğŸ” í•­ëª© ë°œê²¬: {item['institution']} - {item['department']}\")\n",
    "                else:\n",
    "                    print(\"ğŸ” tbodyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "            else:\n",
    "                print(\"ğŸ” í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "                # í˜ì´ì§€ì— ìˆëŠ” ëª¨ë“  í…Œì´ë¸” í™•ì¸\n",
    "                all_tables = soup.find_all('table')\n",
    "                print(f\"ğŸ” í˜ì´ì§€ì˜ ì „ì²´ í…Œì´ë¸” ìˆ˜: {len(all_tables)}\")\n",
    "                for i, t in enumerate(all_tables):\n",
    "                    print(f\"   í…Œì´ë¸” {i}: class={t.get('class')}\")\n",
    "            \n",
    "            return {\n",
    "                'total_count': total_count,\n",
    "                'items': items,\n",
    "                'page': page\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª©ë¡ ì¡°íšŒ ì—ëŸ¬ (í˜ì´ì§€ {page}): {type(e).__name__} - {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {}\n",
    "    \n",
    "    def get_sanction_detail(self, examMgmtNo: str, emOpenSeq: str) -> Optional[Dict]:\n",
    "        \"\"\"ì œì¬ ìƒì„¸ ì •ë³´ ì¡°íšŒ\"\"\"\n",
    "        url = f\"{self.base_url}/fss/job/openInfo/view.do\"\n",
    "        params = {\n",
    "            'menuNo': '200476',\n",
    "            'examMgmtNo': examMgmtNo,\n",
    "            'emOpenSeq': emOpenSeq\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=self.headers, timeout=30)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"âŒ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: ìƒíƒœì½”ë“œ {resp.status_code}\")\n",
    "                return None\n",
    "                \n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ\n",
    "            detail = {\n",
    "                'examMgmtNo': examMgmtNo,\n",
    "                'emOpenSeq': emOpenSeq\n",
    "            }\n",
    "            \n",
    "            # í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ\n",
    "            for table in soup.find_all('table', class_='tbl_view'):\n",
    "                for tr in table.find_all('tr'):\n",
    "                    th = tr.find('th')\n",
    "                    td = tr.find('td')\n",
    "                    if th and td:\n",
    "                        key = th.get_text(strip=True)\n",
    "                        value = td.get_text(' ', strip=True)\n",
    "                        \n",
    "                        if key == 'ì œì¬ëŒ€ìƒê¸°ê´€':\n",
    "                            detail['institution'] = value\n",
    "                        elif key == 'ì œì¬ì¡°ì¹˜ìš”êµ¬ì¼':\n",
    "                            detail['date'] = value\n",
    "                        elif key == 'ê´€ë ¨ë¶€ì„œ':\n",
    "                            detail['department'] = value\n",
    "                        elif key == 'ì œì¬ì¡°ì¹˜ìš”êµ¬ë‚´ìš©':\n",
    "                            detail['content'] = value\n",
    "                        elif key == 'ì²¨ë¶€íŒŒì¼':\n",
    "                            # ì²¨ë¶€íŒŒì¼ ë§í¬ ì¶”ì¶œ\n",
    "                            files = []\n",
    "                            for a in td.find_all('a'):\n",
    "                                file_info = {\n",
    "                                    'name': a.get_text(strip=True),\n",
    "                                    'url': self.base_url + a.get('href', '')\n",
    "                                }\n",
    "                                files.append(file_info)\n",
    "                            detail['files'] = files\n",
    "            \n",
    "            return detail\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ìƒì„¸ ì¡°íšŒ ì—ëŸ¬ ({examMgmtNo}): {type(e).__name__} - {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_sanction_detail(self, detail: Dict, save_dir: str = \"data/FSS_SANCTION\") -> Dict:\n",
    "        \"\"\"ì œì¬ ì •ë³´ ì €ì¥\"\"\"\n",
    "        save_path = self.base_dir / save_dir\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # íŒŒì¼ëª… ìƒì„±\n",
    "        date_str = detail.get('date', '')\n",
    "        if date_str:\n",
    "            # ë‚ ì§œ í˜•ì‹ ì •ê·œí™” (ì˜ˆ: 2024.06.17 -> 20240617)\n",
    "            date_str = re.sub(r'[^\\d]', '', date_str)[:8]\n",
    "        else:\n",
    "            date_str = datetime.now().strftime('%Y%m%d')\n",
    "            \n",
    "        institution = re.sub(r'[^\\w\\s-]', '', detail.get('institution', 'unknown'))\n",
    "        institution = institution.replace(' ', '_')[:30]  # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ\n",
    "        \n",
    "        base_filename = f\"SANCTION_{date_str}_{institution}_{detail['examMgmtNo']}\"\n",
    "        \n",
    "        try:\n",
    "            # ì²¨ë¶€íŒŒì¼ì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ\n",
    "            if detail.get('files'):\n",
    "                for i, file_info in enumerate(detail['files']):\n",
    "                    try:\n",
    "                        file_url = file_info['url']\n",
    "                        file_name = file_info['name']\n",
    "                        ext = os.path.splitext(file_name)[-1] if '.' in file_name else '.pdf'\n",
    "                        \n",
    "                        if i == 0:\n",
    "                            save_filename = f\"{base_filename}{ext}\"\n",
    "                        else:\n",
    "                            save_filename = f\"{base_filename}_{i+1}{ext}\"\n",
    "                        \n",
    "                        file_path = save_path / save_filename\n",
    "                        \n",
    "                        if file_path.exists():\n",
    "                            print(f\"ğŸ“„ ì´ë¯¸ ì¡´ì¬: {file_path}\")\n",
    "                            detail['saved_path'] = str(file_path)\n",
    "                            continue\n",
    "                        \n",
    "                        resp = requests.get(file_url, headers=self.headers, timeout=60)\n",
    "                        if resp.status_code == 200:\n",
    "                            with open(file_path, 'wb') as f:\n",
    "                                f.write(resp.content)\n",
    "                            print(f\"ğŸ’¾ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_path}\")\n",
    "                            detail['saved_path'] = str(file_path)\n",
    "                            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
    "                        else:\n",
    "                            print(f\"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {resp.status_code}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì—ëŸ¬: {type(e).__name__} - {e}\")\n",
    "            \n",
    "            # ìƒì„¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥\n",
    "            json_path = save_path / f\"{base_filename}.json\"\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(detail, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"ğŸ“ ì •ë³´ ì €ì¥ ì™„ë£Œ: {json_path}\")\n",
    "            \n",
    "            if 'saved_path' not in detail:\n",
    "                detail['saved_path'] = str(json_path)\n",
    "            \n",
    "            return detail\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì €ì¥ ì—ëŸ¬: {type(e).__name__} - {e}\")\n",
    "            detail['saved_path'] = None\n",
    "            return detail\n",
    "    \n",
    "    def get_existing_sanctions(self, save_dir: str = \"data/FSS_SANCTION\") -> set:\n",
    "        \"\"\"ê¸°ì¡´ ì €ì¥ëœ ì œì¬ ì •ë³´ í™•ì¸\"\"\"\n",
    "        existing = set()\n",
    "        save_path = self.base_dir / save_dir\n",
    "        \n",
    "        if save_path.exists():\n",
    "            for file in save_path.glob(\"*.json\"):\n",
    "                if file.name.startswith(\"SANCTION_\"):\n",
    "                    # íŒŒì¼ëª…ì—ì„œ examMgmtNo ì¶”ì¶œ\n",
    "                    match = re.search(r'_(\\d{9,})\\.json', file.name)\n",
    "                    if match:\n",
    "                        existing.add(match.group(1))\n",
    "        \n",
    "        return existing\n",
    "    \n",
    "    def crawl_electronic_finance_sanctions(self) -> List[Dict]:\n",
    "        \"\"\"ì „ìê¸ˆìœµ ê´€ë ¨ ì œì¬ ì •ë³´ í¬ë¡¤ë§\"\"\"\n",
    "        new_files = []\n",
    "        \n",
    "        print(f\"=== ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ì œì¬ í¬ë¡¤ë§ ì‹œì‘: {datetime.now()} ===\")\n",
    "        \n",
    "        try:\n",
    "            # ê¸°ì¡´ íŒŒì¼ í™•ì¸\n",
    "            existing = self.get_existing_sanctions()\n",
    "            print(f\"ê¸°ì¡´ ì œì¬ ì •ë³´: {len(existing)}ê°œ\")\n",
    "            \n",
    "            page = 1\n",
    "            total_electronic = 0\n",
    "            \n",
    "            while True:\n",
    "                print(f\"\\nğŸ“„ {page}í˜ì´ì§€ ì¡°íšŒ ì¤‘...\")\n",
    "                result = self.get_sanction_list(page)\n",
    "                \n",
    "                if not result or not result.get('items'):\n",
    "                    break\n",
    "                \n",
    "                items = result['items']\n",
    "                print(f\"  - {len(items)}ê°œ í•­ëª© ë°œê²¬\")\n",
    "                \n",
    "                for item in items:\n",
    "                    # ì´ë¯¸ ì €ì¥ëœ í•­ëª©ì€ ê±´ë„ˆë›°ê¸°\n",
    "                    if item.get('examMgmtNo') and item.get('examMgmtNo') in existing:\n",
    "                        continue\n",
    "                    \n",
    "                    # ì „ìê¸ˆìœµ ê´€ë ¨ ì—¬ë¶€ í™•ì¸\n",
    "                    department = item.get('department', '')\n",
    "                    if is_electronic_finance_related(department):\n",
    "                        print(f\"\\nğŸ” ì „ìê¸ˆìœµ ê´€ë ¨ ë°œê²¬: {item['institution']} ({item['date']})\")\n",
    "                        \n",
    "                        # examMgmtNoê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "                        if not item.get('examMgmtNo') or not item.get('emOpenSeq'):\n",
    "                            print(f\"âš ï¸ ìƒì„¸ ë§í¬ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # ìƒì„¸ ì •ë³´ ì¡°íšŒ\n",
    "                        detail = self.get_sanction_detail(\n",
    "                            item['examMgmtNo'], \n",
    "                            item['emOpenSeq']\n",
    "                        )\n",
    "                        \n",
    "                        if detail:\n",
    "                            # ìƒì„¸ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ í•œë²ˆ í™•ì¸\n",
    "                            content = detail.get('content', '')\n",
    "                            if is_electronic_finance_related(department, content):\n",
    "                                # ì €ì¥\n",
    "                                saved = self.save_sanction_detail(detail)\n",
    "                                \n",
    "                                file_info = {\n",
    "                                    'type': 'ê²€ì‚¬ê²°ê³¼ì œì¬',\n",
    "                                    'examMgmtNo': item['examMgmtNo'],\n",
    "                                    'institution': detail.get('institution', ''),\n",
    "                                    'date': detail.get('date', ''),\n",
    "                                    'department': department,\n",
    "                                    'file_path': saved.get('saved_path'),\n",
    "                                    'timestamp': datetime.now().isoformat()\n",
    "                                }\n",
    "                                new_files.append(file_info)\n",
    "                                total_electronic += 1\n",
    "                                \n",
    "                                time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€\n",
    "                \n",
    "                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸\n",
    "                total_count = result.get('total_count', 0)\n",
    "                if page * 10 >= total_count:\n",
    "                    print(f\"\\nâœ… ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ ì™„ë£Œ (ì´ {total_count}ê±´)\")\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(1)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°\n",
    "                \n",
    "                # ì•ˆì „ì¥ì¹˜\n",
    "                if page > 500:  # ìµœëŒ€ 500í˜ì´ì§€ê¹Œì§€ë§Œ\n",
    "                    print(\"âš ï¸ 500í˜ì´ì§€ ì´ˆê³¼, ì•ˆì „ ì¤‘ë‹¨\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\n=== í¬ë¡¤ë§ ì™„ë£Œ ===\")\n",
    "            print(f\"ğŸ”µ ìƒˆë¡œìš´ ì „ìê¸ˆìœµ ì œì¬ ì •ë³´: {len(new_files)}ê°œ\")\n",
    "            print(f\"ğŸ“Š ì „ì²´ ì „ìê¸ˆìœµ ì œì¬: {total_electronic}ê°œ\")\n",
    "            \n",
    "            return new_files\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nâŒ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "            return new_files\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬: {type(e).__name__} - {e}\")\n",
    "            return new_files\n",
    "    \n",
    "    def save_crawl_log(self, new_files: List[Dict]) -> None:\n",
    "        \"\"\"í¬ë¡¤ë§ ê²°ê³¼ ë¡œê·¸ ì €ì¥\"\"\"\n",
    "        if not new_files:\n",
    "            return\n",
    "        \n",
    "        log_dir = self.base_dir / \"crawl_logs\"\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        today = datetime.now().strftime(\"%Y%m%d\")\n",
    "        log_file = log_dir / f\"fss_sanctions_{today}.json\"\n",
    "        \n",
    "        with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(new_files, f, ensure_ascii=False, indent=2, default=str)\n",
    "        \n",
    "        print(f\"ğŸ“‹ í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥: {log_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    crawler = FSSCrawler()\n",
    "    \n",
    "    # ì „ìê¸ˆìœµ ê´€ë ¨ ì œì¬ ì •ë³´ í¬ë¡¤ë§\n",
    "    new_files = crawler.crawl_electronic_finance_sanctions()\n",
    "    \n",
    "    # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥\n",
    "    crawler.save_crawl_log(new_files)\n",
    "    \n",
    "    print(\"\\nâœ¨ ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ì œì¬ í¬ë¡¤ë§ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc2dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
