{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "976b56e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 금융감독원 전자금융 제재 크롤링 시작: 2025-07-13 21:19:13.888522 ===\n",
      "기존 제재 정보: 0개\n",
      "\n",
      "📄 1페이지 조회 중...\n",
      "🔍 요청 URL: https://www.fss.or.kr/fss/job/openInfo/list.do\n",
      "🔍 파라미터: {'menuNo': '200476', 'pageIndex': 1, 'sdate': '2014-01-01', 'edate': '2025-07-13', 'searchCnd': '4', 'searchWrd': ''}\n",
      "🔍 응답 상태: 200\n",
      "🔍 HTML 길이: 261693\n",
      "🔍 Title: 검사결과제재(목록) | 제재관련 공시 | 검사·제재 | 업무자료 | \n",
      "🔍 전체 건수를 찾을 수 없음 - 계속 진행\n",
      "🔍 테이블 클래스: None\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202000323&emOpenSeq=4, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202000323, emOpenSeq=4\n",
      "🔍 항목 발견: 하나증권주식회사 - 금융투자검사1국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300754&emOpenSeq=2, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202300754, emOpenSeq=2\n",
      "🔍 항목 발견: 신한라이프생명보험주식회사 - 보험검사1국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300379&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202300379, emOpenSeq=1\n",
      "🔍 항목 발견: 케이에스신용정보 주식회사 - 서민금융보호국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300687&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202300687, emOpenSeq=1\n",
      "🔍 항목 발견: 주식회사 넥스테라투자일임 - 금융투자검사2국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300381&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202300381, emOpenSeq=1\n",
      "🔍 항목 발견: 세일신용정보 주식회사 - 서민금융보호국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202400688&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202400688, emOpenSeq=1\n",
      "🔍 항목 발견: 동원제일저축은행 - 중소금융검사1국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202300504&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202300504, emOpenSeq=1\n",
      "🔍 항목 발견: 대덕자산운용 주식회사 - 금융투자검사3국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202400617&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202400617, emOpenSeq=1\n",
      "🔍 항목 발견: 우리자산신탁 - 금융투자검사3국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202000041&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202000041, emOpenSeq=1\n",
      "🔍 항목 발견: 제이비자산운용 - 금융투자검사2국\n",
      "🔍 TD 개수: 6\n",
      "🔍 링크 정보: href=/fss/job/openInfo/view.do?menuNo=200476&sdate=2014-01-01&edate=2025-07-13&searchCnd=4&searchWrd=&pageIndex=1&examMgmtNo=202200487&emOpenSeq=1, onclick=None\n",
      "🔍 URL에서 추출: examMgmtNo=202200487, emOpenSeq=1\n",
      "🔍 항목 발견: 피에프씨테크놀로지스 주식회사 - 전자금융검사국\n",
      "  - 10개 항목 발견\n",
      "\n",
      "❌ 크롤링 중 예기치 못한 에러: NameError - name 'content' is not defined\n",
      "\n",
      "✨ 금융감독원 전자금융 제재 크롤링 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sl/ly32t5wx3p5b36cvk238t1840000gn/T/ipykernel_64586/235809900.py:88: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  total_elem = soup.find(text=re.compile(r'총\\s*\\d+\\s*건'))\n"
     ]
    }
   ],
   "source": [
    "# 금융감독원 크롤링\n",
    "\"\"\"\n",
    "금융감독원 검사결과제재 전자금융 관련 문서 크롤러\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "\n",
    "def is_electronic_finance_related(department: str, content: str = \"\") -> bool:\n",
    "    \"\"\"관련부서명과 내용으로 전자금융 관련 여부 판단\"\"\"\n",
    "    # 전자금융 관련 부서\n",
    "    dept_keywords = [\n",
    "        '전자금융검사국', '전자금융', 'IT검사', '정보기술', 'IT', '사이버'\n",
    "    ]\n",
    "    \n",
    "    # 전자금융 관련 키워드\n",
    "    content_keywords = [\n",
    "        '전자금융', '핀테크', '온라인', '모바일', '인터넷뱅킹', '전자지급',\n",
    "        '전자결제', 'API', '오픈뱅킹', '마이데이터', '전자서명', '정보보호',\n",
    "        '개인정보', '사이버', '해킹', '보안', '전산', 'IT', '시스템', '네트워크'\n",
    "    ]\n",
    "    \n",
    "    # 부서명 확인\n",
    "    for keyword in dept_keywords:\n",
    "        if keyword in department:\n",
    "            return True\n",
    "    \n",
    "    # 내용 확인\n",
    "    if content:\n",
    "        for keyword in content_keywords:\n",
    "            if keyword in content:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "class FSSCrawler:\n",
    "    \"\"\"금융감독원 크롤러\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir: str = \"../../\"):\n",
    "        self.base_url = \"https://www.fss.or.kr\"\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "    def get_sanction_list(self, page: int = 1) -> Dict:\n",
    "        \"\"\"검사결과제재 목록 조회\"\"\"\n",
    "        url = f\"{self.base_url}/fss/job/openInfo/list.do\"\n",
    "        params = {\n",
    "            'menuNo': '200476',\n",
    "            'pageIndex': page,\n",
    "            'sdate': '2014-01-01',\n",
    "            'edate': datetime.now().strftime('%Y-%m-%d'),\n",
    "            'searchCnd': '4',\n",
    "            'searchWrd': ''\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"🔍 요청 URL: {url}\")\n",
    "            print(f\"🔍 파라미터: {params}\")\n",
    "            \n",
    "            resp = requests.get(url, params=params, headers=self.headers, timeout=30)\n",
    "            print(f\"🔍 응답 상태: {resp.status_code}\")\n",
    "            \n",
    "            if resp.status_code != 200:\n",
    "                print(f\"❌ 목록 조회 실패: 상태코드 {resp.status_code}\")\n",
    "                return {}\n",
    "                \n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            # 디버깅: HTML 일부 출력\n",
    "            print(f\"🔍 HTML 길이: {len(resp.text)}\")\n",
    "            print(f\"🔍 Title: {soup.find('title').text if soup.find('title') else 'No title'}\")\n",
    "            \n",
    "            # 전체 건수 추출\n",
    "            total_text = soup.find('span', class_='total')\n",
    "            total_count = 0\n",
    "            if total_text:\n",
    "                match = re.search(r'(\\d+)건', total_text.text)\n",
    "                if match:\n",
    "                    total_count = int(match.group(1))\n",
    "                print(f\"🔍 전체 건수: {total_count}\")\n",
    "            else:\n",
    "                # 다른 방법으로 전체 건수 찾기\n",
    "                total_elem = soup.find(text=re.compile(r'총\\s*\\d+\\s*건'))\n",
    "                if total_elem:\n",
    "                    match = re.search(r'(\\d+)', total_elem)\n",
    "                    if match:\n",
    "                        total_count = int(match.group(1))\n",
    "                        print(f\"🔍 전체 건수(패턴2): {total_count}\")\n",
    "                else:\n",
    "                    # 페이징 정보에서 추출\n",
    "                    paging = soup.find('div', class_='paging')\n",
    "                    if paging:\n",
    "                        last_page = paging.find_all('a')[-1]\n",
    "                        if last_page:\n",
    "                            onclick = last_page.get('onclick', '')\n",
    "                            match = re.search(r'goPage\\((\\d+)\\)', onclick)\n",
    "                            if match:\n",
    "                                last_page_num = int(match.group(1))\n",
    "                                total_count = last_page_num * 10  # 대략적인 추정\n",
    "                                print(f\"🔍 전체 건수(페이징 추정): 약 {total_count}건\")\n",
    "                    \n",
    "                    if total_count == 0:\n",
    "                        print(\"🔍 전체 건수를 찾을 수 없음 - 계속 진행\")\n",
    "                        total_count = 999999  # 충분히 큰 수로 설정\n",
    "            \n",
    "            # 테이블에서 데이터 추출\n",
    "            items = []\n",
    "            table = soup.find('table', class_='tbl_list')\n",
    "            if not table:\n",
    "                # 다른 클래스명 시도\n",
    "                table = soup.find('table', class_='list')\n",
    "                if not table:\n",
    "                    table = soup.find('table')\n",
    "                    print(f\"🔍 테이블 클래스: {table.get('class') if table else 'No table found'}\")\n",
    "            \n",
    "            if table:\n",
    "                tbody = table.find('tbody')\n",
    "                if tbody:\n",
    "                    for tr in tbody.find_all('tr'):\n",
    "                        tds = tr.find_all('td')\n",
    "                        print(f\"🔍 TD 개수: {len(tds)}\")\n",
    "                        if len(tds) >= 6:\n",
    "                            # 번호, 제재대상기관, 제재조치요구일, 제재조치요구내용(링크), 관련부서, 조회수\n",
    "                            item = {\n",
    "                                'no': tds[0].get_text(strip=True),\n",
    "                                'institution': tds[1].get_text(strip=True),\n",
    "                                'date': tds[2].get_text(strip=True),\n",
    "                                'department': tds[4].get_text(strip=True),\n",
    "                                'views': tds[5].get_text(strip=True)\n",
    "                            }\n",
    "                            \n",
    "                            # 상세 링크 추출\n",
    "                            link_tag = tds[3].find('a')\n",
    "                            if link_tag:\n",
    "                                # href 속성 확인\n",
    "                                href = link_tag.get('href')\n",
    "                                onclick = link_tag.get('onclick')\n",
    "                                print(f\"🔍 링크 정보: href={href}, onclick={onclick}\")\n",
    "                                \n",
    "                                if onclick:\n",
    "                                    # JavaScript 함수에서 파라미터 추출\n",
    "                                    match = re.search(r\"fnView\\('([^']+)','([^']+)'\\)\", onclick)\n",
    "                                    if match:\n",
    "                                        item['examMgmtNo'] = match.group(1)\n",
    "                                        item['emOpenSeq'] = match.group(2)\n",
    "                                        print(f\"🔍 추출된 ID: examMgmtNo={item['examMgmtNo']}, emOpenSeq={item['emOpenSeq']}\")\n",
    "                                    else:\n",
    "                                        # 다른 패턴 시도\n",
    "                                        match = re.search(r\"fnView\\(([^,]+),([^)]+)\\)\", onclick)\n",
    "                                        if match:\n",
    "                                            item['examMgmtNo'] = match.group(1).strip(\"'\\\"\")\n",
    "                                            item['emOpenSeq'] = match.group(2).strip(\"'\\\"\")\n",
    "                                            print(f\"🔍 추출된 ID(패턴2): examMgmtNo={item['examMgmtNo']}, emOpenSeq={item['emOpenSeq']}\")\n",
    "                                elif href and 'examMgmtNo' in href:\n",
    "                                    # URL 파라미터에서 추출\n",
    "                                    import urllib.parse\n",
    "                                    parsed = urllib.parse.urlparse(href)\n",
    "                                    params = urllib.parse.parse_qs(parsed.query)\n",
    "                                    item['examMgmtNo'] = params.get('examMgmtNo', [''])[0]\n",
    "                                    item['emOpenSeq'] = params.get('emOpenSeq', [''])[0]\n",
    "                                    print(f\"🔍 URL에서 추출: examMgmtNo={item['examMgmtNo']}, emOpenSeq={item['emOpenSeq']}\")\n",
    "                            \n",
    "                            items.append(item)\n",
    "                            print(f\"🔍 항목 발견: {item['institution']} - {item['department']}\")\n",
    "                else:\n",
    "                    print(\"🔍 tbody를 찾을 수 없음\")\n",
    "            else:\n",
    "                print(\"🔍 테이블을 찾을 수 없음\")\n",
    "                # 페이지에 있는 모든 테이블 확인\n",
    "                all_tables = soup.find_all('table')\n",
    "                print(f\"🔍 페이지의 전체 테이블 수: {len(all_tables)}\")\n",
    "                for i, t in enumerate(all_tables):\n",
    "                    print(f\"   테이블 {i}: class={t.get('class')}\")\n",
    "            \n",
    "            return {\n",
    "                'total_count': total_count,\n",
    "                'items': items,\n",
    "                'page': page\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 목록 조회 에러 (페이지 {page}): {type(e).__name__} - {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {}\n",
    "    \n",
    "    def get_sanction_detail(self, examMgmtNo: str, emOpenSeq: str) -> Optional[Dict]:\n",
    "        \"\"\"제재 상세 정보 조회\"\"\"\n",
    "        url = f\"{self.base_url}/fss/job/openInfo/view.do\"\n",
    "        params = {\n",
    "            'menuNo': '200476',\n",
    "            'examMgmtNo': examMgmtNo,\n",
    "            'emOpenSeq': emOpenSeq\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(url, params=params, headers=self.headers, timeout=30)\n",
    "            if resp.status_code != 200:\n",
    "                print(f\"❌ 상세 조회 실패: 상태코드 {resp.status_code}\")\n",
    "                return None\n",
    "                \n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            \n",
    "            # 상세 정보 추출\n",
    "            detail = {\n",
    "                'examMgmtNo': examMgmtNo,\n",
    "                'emOpenSeq': emOpenSeq\n",
    "            }\n",
    "            \n",
    "            # 테이블에서 정보 추출\n",
    "            for table in soup.find_all('table', class_='tbl_view'):\n",
    "                for tr in table.find_all('tr'):\n",
    "                    th = tr.find('th')\n",
    "                    td = tr.find('td')\n",
    "                    if th and td:\n",
    "                        key = th.get_text(strip=True)\n",
    "                        value = td.get_text(' ', strip=True)\n",
    "                        \n",
    "                        if key == '제재대상기관':\n",
    "                            detail['institution'] = value\n",
    "                        elif key == '제재조치요구일':\n",
    "                            detail['date'] = value\n",
    "                        elif key == '관련부서':\n",
    "                            detail['department'] = value\n",
    "                        elif key == '제재조치요구내용':\n",
    "                            detail['content'] = value\n",
    "                        elif key == '첨부파일':\n",
    "                            # 첨부파일 링크 추출\n",
    "                            files = []\n",
    "                            for a in td.find_all('a'):\n",
    "                                file_info = {\n",
    "                                    'name': a.get_text(strip=True),\n",
    "                                    'url': self.base_url + a.get('href', '')\n",
    "                                }\n",
    "                                files.append(file_info)\n",
    "                            detail['files'] = files\n",
    "            \n",
    "            return detail\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 상세 조회 에러 ({examMgmtNo}): {type(e).__name__} - {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_sanction_detail(self, detail: Dict, save_dir: str = \"data/FSS_SANCTION\") -> Dict:\n",
    "        \"\"\"제재 정보 저장\"\"\"\n",
    "        save_path = self.base_dir / save_dir\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 파일명 생성\n",
    "        date_str = detail.get('date', '')\n",
    "        if date_str:\n",
    "            # 날짜 형식 정규화 (예: 2024.06.17 -> 20240617)\n",
    "            date_str = re.sub(r'[^\\d]', '', date_str)[:8]\n",
    "        else:\n",
    "            date_str = datetime.now().strftime('%Y%m%d')\n",
    "            \n",
    "        institution = re.sub(r'[^\\w\\s-]', '', detail.get('institution', 'unknown'))\n",
    "        institution = institution.replace(' ', '_')[:30]  # 파일명 길이 제한\n",
    "        \n",
    "        base_filename = f\"SANCTION_{date_str}_{institution}_{detail['examMgmtNo']}\"\n",
    "        \n",
    "        try:\n",
    "            # 첨부파일이 있으면 다운로드\n",
    "            if detail.get('files'):\n",
    "                for i, file_info in enumerate(detail['files']):\n",
    "                    try:\n",
    "                        file_url = file_info['url']\n",
    "                        file_name = file_info['name']\n",
    "                        ext = os.path.splitext(file_name)[-1] if '.' in file_name else '.pdf'\n",
    "                        \n",
    "                        if i == 0:\n",
    "                            save_filename = f\"{base_filename}{ext}\"\n",
    "                        else:\n",
    "                            save_filename = f\"{base_filename}_{i+1}{ext}\"\n",
    "                        \n",
    "                        file_path = save_path / save_filename\n",
    "                        \n",
    "                        if file_path.exists():\n",
    "                            print(f\"📄 이미 존재: {file_path}\")\n",
    "                            detail['saved_path'] = str(file_path)\n",
    "                            continue\n",
    "                        \n",
    "                        resp = requests.get(file_url, headers=self.headers, timeout=60)\n",
    "                        if resp.status_code == 200:\n",
    "                            with open(file_path, 'wb') as f:\n",
    "                                f.write(resp.content)\n",
    "                            print(f\"💾 다운로드 완료: {file_path}\")\n",
    "                            detail['saved_path'] = str(file_path)\n",
    "                            time.sleep(1)  # 서버 부하 방지\n",
    "                        else:\n",
    "                            print(f\"❌ 다운로드 실패: HTTP {resp.status_code}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ 파일 다운로드 에러: {type(e).__name__} - {e}\")\n",
    "            \n",
    "            # 상세 정보를 JSON으로 저장\n",
    "            json_path = save_path / f\"{base_filename}.json\"\n",
    "            with open(json_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(detail, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"📝 정보 저장 완료: {json_path}\")\n",
    "            \n",
    "            if 'saved_path' not in detail:\n",
    "                detail['saved_path'] = str(json_path)\n",
    "            \n",
    "            return detail\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 저장 에러: {type(e).__name__} - {e}\")\n",
    "            detail['saved_path'] = None\n",
    "            return detail\n",
    "    \n",
    "    def get_existing_sanctions(self, save_dir: str = \"data/FSS_SANCTION\") -> set:\n",
    "        \"\"\"기존 저장된 제재 정보 확인\"\"\"\n",
    "        existing = set()\n",
    "        save_path = self.base_dir / save_dir\n",
    "        \n",
    "        if save_path.exists():\n",
    "            for file in save_path.glob(\"*.json\"):\n",
    "                if file.name.startswith(\"SANCTION_\"):\n",
    "                    # 파일명에서 examMgmtNo 추출\n",
    "                    match = re.search(r'_(\\d{9,})\\.json', file.name)\n",
    "                    if match:\n",
    "                        existing.add(match.group(1))\n",
    "        \n",
    "        return existing\n",
    "    \n",
    "    def crawl_electronic_finance_sanctions(self) -> List[Dict]:\n",
    "        \"\"\"전자금융 관련 제재 정보 크롤링\"\"\"\n",
    "        new_files = []\n",
    "        \n",
    "        print(f\"=== 금융감독원 전자금융 제재 크롤링 시작: {datetime.now()} ===\")\n",
    "        \n",
    "        try:\n",
    "            # 기존 파일 확인\n",
    "            existing = self.get_existing_sanctions()\n",
    "            print(f\"기존 제재 정보: {len(existing)}개\")\n",
    "            \n",
    "            page = 1\n",
    "            total_electronic = 0\n",
    "            \n",
    "            while True:\n",
    "                print(f\"\\n📄 {page}페이지 조회 중...\")\n",
    "                result = self.get_sanction_list(page)\n",
    "                \n",
    "                if not result or not result.get('items'):\n",
    "                    break\n",
    "                \n",
    "                items = result['items']\n",
    "                print(f\"  - {len(items)}개 항목 발견\")\n",
    "                \n",
    "                for item in items:\n",
    "                    # 이미 저장된 항목은 건너뛰기\n",
    "                    if item.get('examMgmtNo') and item.get('examMgmtNo') in existing:\n",
    "                        continue\n",
    "                    \n",
    "                    # 전자금융 관련 여부 확인\n",
    "                    department = item.get('department', '')\n",
    "                    if is_electronic_finance_related(department):\n",
    "                        print(f\"\\n🔍 전자금융 관련 발견: {item['institution']} ({item['date']})\")\n",
    "                        \n",
    "                        # examMgmtNo가 없는 경우 처리\n",
    "                        if not item.get('examMgmtNo') or not item.get('emOpenSeq'):\n",
    "                            print(f\"⚠️ 상세 링크 정보가 없습니다. 건너뜁니다.\")\n",
    "                            continue\n",
    "                        \n",
    "                        # 상세 정보 조회\n",
    "                        detail = self.get_sanction_detail(\n",
    "                            item['examMgmtNo'], \n",
    "                            item['emOpenSeq']\n",
    "                        )\n",
    "                        \n",
    "                        if detail:\n",
    "                            # 상세 내용으로 다시 한번 확인\n",
    "                            content = detail.get('content', '')\n",
    "                            if is_electronic_finance_related(department, content):\n",
    "                                # 저장\n",
    "                                saved = self.save_sanction_detail(detail)\n",
    "                                \n",
    "                                file_info = {\n",
    "                                    'type': '검사결과제재',\n",
    "                                    'examMgmtNo': item['examMgmtNo'],\n",
    "                                    'institution': detail.get('institution', ''),\n",
    "                                    'date': detail.get('date', ''),\n",
    "                                    'department': department,\n",
    "                                    'file_path': saved.get('saved_path'),\n",
    "                                    'timestamp': datetime.now().isoformat()\n",
    "                                }\n",
    "                                new_files.append(file_info)\n",
    "                                total_electronic += 1\n",
    "                                \n",
    "                                time.sleep(2)  # 서버 부하 방지\n",
    "                \n",
    "                # 다음 페이지 확인\n",
    "                total_count = result.get('total_count', 0)\n",
    "                if page * 10 >= total_count:\n",
    "                    print(f\"\\n✅ 모든 페이지 조회 완료 (총 {total_count}건)\")\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                time.sleep(1)  # 페이지 간 대기\n",
    "                \n",
    "                # 안전장치\n",
    "                if page > 500:  # 최대 500페이지까지만\n",
    "                    print(\"⚠️ 500페이지 초과, 안전 중단\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\n=== 크롤링 완료 ===\")\n",
    "            print(f\"🔵 새로운 전자금융 제재 정보: {len(new_files)}개\")\n",
    "            print(f\"📊 전체 전자금융 제재: {total_electronic}개\")\n",
    "            \n",
    "            return new_files\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n❌ 사용자에 의해 크롤링이 중단되었습니다.\")\n",
    "            return new_files\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 크롤링 중 예기치 못한 에러: {type(e).__name__} - {e}\")\n",
    "            return new_files\n",
    "    \n",
    "    def save_crawl_log(self, new_files: List[Dict]) -> None:\n",
    "        \"\"\"크롤링 결과 로그 저장\"\"\"\n",
    "        if not new_files:\n",
    "            return\n",
    "        \n",
    "        log_dir = self.base_dir / \"crawl_logs\"\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        today = datetime.now().strftime(\"%Y%m%d\")\n",
    "        log_file = log_dir / f\"fss_sanctions_{today}.json\"\n",
    "        \n",
    "        with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(new_files, f, ensure_ascii=False, indent=2, default=str)\n",
    "        \n",
    "        print(f\"📋 크롤링 로그 저장: {log_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 크롤러 인스턴스 생성\n",
    "    crawler = FSSCrawler()\n",
    "    \n",
    "    # 전자금융 관련 제재 정보 크롤링\n",
    "    new_files = crawler.crawl_electronic_finance_sanctions()\n",
    "    \n",
    "    # 크롤링 로그 저장\n",
    "    crawler.save_crawl_log(new_files)\n",
    "    \n",
    "    print(\"\\n✨ 금융감독원 전자금융 제재 크롤링 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc2dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
