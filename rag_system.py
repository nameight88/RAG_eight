"""
ê¸ˆìœµê°ë…ì› ì œì¬/ê²½ì˜ìœ ì˜ì‚¬í•­ ì½”í¼ìŠ¤ë¥¼ í™œìš©í•œ RAG ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
- ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
- LLMì„ ì´ìš©í•œ ì§ˆì˜ì‘ë‹µ
"""

import os
import json
import re
import torch
import pickle  # pickle ëª¨ë“ˆ ì¶”ê°€
import numpy as np  # numpy ëª¨ë“ˆ ì¶”ê°€
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS, Chroma
from langchain_huggingface import HuggingFaceEmbeddings
# OpenAI ì˜ì¡´ì„± ë³µì›
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.llms import HuggingFacePipeline
from langchain_anthropic import ChatAnthropic
from langchain.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# FAISS ê´€ë ¨ ì„í¬íŠ¸
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    print("âš ï¸ FAISSë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install faiss-cpuë¥¼ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    FAISS_AVAILABLE = False

# Pydantic í˜¸í™˜ì„±ì„ ìœ„í•œ ì»¤ìŠ¤í…€ Unpickler í´ë˜ìŠ¤ ì¶”ê°€
class PydanticCompatibleUnpickler(pickle.Unpickler):
    """Pydantic v1/v2 í˜¸í™˜ì„±ì„ ìœ„í•œ ì»¤ìŠ¤í…€ Unpickler"""
    def find_class(self, module, name):
        try:
            return super().find_class(module, name)
        except (ImportError, AttributeError):
            # Pydantic ê´€ë ¨ í´ë˜ìŠ¤ ì²˜ë¦¬
            if module == "pydantic.main" and name == "BaseModel":
                import pydantic
                return pydantic.BaseModel
            elif module == "langchain.schema" and name == "Document":
                from langchain_core.documents import Document
                return Document
            elif module == "langchain.docstore.document" and name == "Document":
                from langchain_core.documents import Document
                return Document
            elif module == "langchain.docstore.in_memory" and name == "InMemoryDocstore":
                from langchain_community.docstore.in_memory import InMemoryDocstore
                return InMemoryDocstore
            else:
                # ê¸°íƒ€ í´ë˜ìŠ¤ëŠ” ë™ì ìœ¼ë¡œ ì²˜ë¦¬
                try:
                    import importlib
                    mod = importlib.import_module(module)
                    return getattr(mod, name)
                except:
                    # ìµœí›„ì˜ ìˆ˜ë‹¨: ë¹ˆ í´ë˜ìŠ¤ ë°˜í™˜
                    class DummyClass:
                        def __init__(self, *args, **kwargs):
                            pass
                        def __getstate__(self):
                            return {}
                        def __setstate__(self, state):
                            pass
                    return DummyClass

    def persistent_load(self, pid):
        raise pickle.UnpicklingError("unsupported persistent object")

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class FSSRagSystem:
    """ê¸ˆìœµê°ë…ì› ì œì¬/ê²½ì˜ìœ ì˜ì‚¬í•­ RAG ì‹œìŠ¤í…œ"""
    
    # ë²¡í„° ì €ì¥ì†Œ ìºì‹œ - ê²½ë¡œë³„ë¡œ ì €ì¥
    _vector_store_cache = {}
    # ì„ë² ë”© ëª¨ë¸ ìºì‹œ
    _embeddings_cache = {}
    
    def __init__(
        self,
        vector_db_path: str = "./data/vector_db/fss_sanctions",
        embed_model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        top_k: int = 5,
        use_anthropic: bool = False,
        use_openai_embeddings: bool = True,
        use_faiss: bool = True,
        use_openai_llm: bool = True,
    ) -> None:
        """FSS RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        
        # ì„¤ì • ì €ì¥
        self.vector_db_path = os.path.abspath(vector_db_path)
        self.embed_model_name = embed_model_name
        self.top_k = top_k
        self.use_anthropic = use_anthropic  # Anthropic Claude ì‚¬ìš© ì—¬ë¶€
        self.anthropic_api_key = os.getenv("ANTHROPIC_APIKEY")
        self.use_openai_llm = use_openai_llm  # OpenAI LLM ì‚¬ìš© ì—¬ë¶€
        self.use_openai_embeddings = use_openai_embeddings  # OpenAI ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€
        self.use_faiss = use_faiss  # FAISS ì‚¬ìš© ì—¬ë¶€ (Falseë©´ Chroma ì‚¬ìš©)
        
        # ì œì¬ ë°ì´í„°ì¸ì§€ ê²½ì˜ìœ ì˜ì¸ì§€ íŒë‹¨
        if "sanctions" in vector_db_path:
            self.db_type = "sanctions"
        elif "management" in vector_db_path:
            self.db_type = "management"
        else:
            self.db_type = "unknown"
            
        print(f"ğŸ”„ DB íƒ€ì…: {self.db_type}")
        
        # OpenAI ì„¤ì •
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.llm_model_name = "gpt-3.5-turbo"  # ê¸°ë³¸ ëª¨ë¸
        
        # ì´ˆê¸°í™”
        self.embeddings = None
        self.vector_store = None
        self.llm = None
        self.qa_chain = None
        
        # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹œë„
        self.load_vector_store()
    
    def get_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)"""
        cache_key = f"openai_{self.openai_api_key}" if self.use_openai_embeddings else self.embed_model_name
        
        # ìºì‹œì— ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if cache_key in FSSRagSystem._embeddings_cache:
            print(f"ğŸ“š ìºì‹œëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©: {cache_key}")
            return FSSRagSystem._embeddings_cache[cache_key]
        
        try:
            # OpenAI API ì‚¬ìš©
            if self.use_openai_embeddings and self.openai_api_key:
                print(f"ğŸ§  OpenAI ì„ë² ë”© API ì´ˆê¸°í™” ì¤‘...")
                embeddings = OpenAIEmbeddings(
                    model="text-embedding-3-small", 
                    openai_api_key=self.openai_api_key
                )
                print(f"âœ… OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ")
            # ë¡œì»¬/HuggingFace ëª¨ë¸ ì‚¬ìš©
            else:
                print(f"ğŸ§  HuggingFace ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘: {self.embed_model_name}")
                embeddings = HuggingFaceEmbeddings(
                    model_name=self.embed_model_name,
                    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
                    encode_kwargs={'normalize_embeddings': True}
                )
                print(f"âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
                
            # ìºì‹œì— ì €ì¥
            FSSRagSystem._embeddings_cache[cache_key] = embeddings
            
            return embeddings
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def load_faiss_from_local(self, local_path: str) -> Any:
        """ë¡œì»¬ ì €ì¥ì†Œì—ì„œ FAISS ë¡œë“œ"""
        try:
            print(f"âœ… ê¸°ì¡´ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {local_path}")
            
            # ë³´ì•ˆ ì˜µì…˜ ì¶”ê°€: allow_dangerous_deserialization=True
            faiss_vectorstore = FAISS.load_local(
                local_path,
                self.embeddings,
                allow_dangerous_deserialization=True  # ì•ˆì „í•˜ì§€ ì•Šì€ ì—­ì§ë ¬í™” í—ˆìš© (ì§ì ‘ ìƒì„±í•œ ì•ˆì „í•œ íŒŒì¼ì„)
            )
            return faiss_vectorstore
            
        except Exception as e:
            print(f"âŒ FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def load_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)"""
        try:
            print(f"ğŸ“š ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘: {self.vector_db_path}")

            # FAISS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
            if self.use_faiss and not FAISS_AVAILABLE:
                print("âš ï¸ FAISSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ Chromaë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                self.use_faiss = False

            # ë²¡í„° ì €ì¥ì†Œ ì •ë³´ íŒŒì¼ ê²½ë¡œ
            info_path = os.path.join(self.vector_db_path, 'vector_store_info.json')
            if not os.path.exists(info_path):
                print(f"âŒ 'vector_store_info.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {info_path}")
                print("ì˜¤ë¥˜: ë²¡í„° ì €ì¥ì†Œì˜ ë©”íƒ€ë°ì´í„°ê°€ ì—†ì–´ ì„ë² ë”© ëª¨ë¸ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ë°ì´í„° ìƒì„± íŒŒì´í”„ë¼ì¸ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ì¬ìƒì„±í•´ì£¼ì„¸ìš”.")
                return False

            with open(info_path, 'r', encoding='utf-8') as f:
                vs_info = json.load(f)
            
            use_openai = vs_info.get('use_openai', False)
            embed_model = vs_info.get('embed_model', 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            if isinstance(embed_model, str):
                embed_model = embed_model.replace("openai/", "")

            # OpenAI API í‚¤ í™•ì¸ ë° ì„¤ì •
            if use_openai:
                print(f"ğŸ§  OpenAI ì„ë² ë”© API ì´ˆê¸°í™” ì¤‘ ({embed_model})...")
                if not self.openai_api_key:
                    self.openai_api_key = os.getenv("OPENAI_API_KEY")
                    if not self.openai_api_key:
                        print("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        return False
                
                try:
                    from langchain_openai import OpenAIEmbeddings
                    self.embeddings = OpenAIEmbeddings(
                        model=embed_model,
                        openai_api_key=self.openai_api_key,
                        show_progress_bar=True,
                        request_timeout=60
                    )
                    # ì„ë² ë”© í…ŒìŠ¤íŠ¸
                    test_text = "í…ŒìŠ¤íŠ¸"
                    try:
                        test_embedding = self.embeddings.embed_query(test_text)
                        print(f"âœ… OpenAI ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ (ë²¡í„° í¬ê¸°: {len(test_embedding)})")
                    except Exception as e:
                        print(f"âŒ OpenAI ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                        return False
                except Exception as e:
                    print(f"âŒ OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                    return False
            else:
                print(f"ğŸ§  HuggingFace ì„ë² ë”© ì´ˆê¸°í™” ì¤‘: {embed_model}")
                from langchain_community.embeddings import HuggingFaceEmbeddings
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=embed_model,
                    model_kwargs={'device': 'cpu'},  # CUDA ì˜¤ë¥˜ ë°©ì§€
                    encode_kwargs={'normalize_embeddings': True}
                )

            # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (FAISS ë˜ëŠ” Chroma)
            vector_store_type = vs_info.get('vector_store_type', 'FAISS' if self.use_faiss else 'Chroma').upper()

            if vector_store_type == 'FAISS' and FAISS_AVAILABLE:
                faiss_path = os.path.join(self.vector_db_path, "faiss")
                index_path = os.path.join(faiss_path, "index.faiss")
                docstore_path = os.path.join(faiss_path, "index.pkl")
                
                if not os.path.exists(index_path) or not os.path.exists(docstore_path):
                    print(f"âŒ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {faiss_path}")
                    return False
                
                try:
                    print(f"âœ… ê¸°ì¡´ FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {faiss_path}")
                    
                    # ë°©ë²• 1: í‘œì¤€ load_local ì‹œë„
                    try:
                        from langchain_community.vectorstores import FAISS
                        self.vector_store = FAISS.load_local(
                            faiss_path,
                            self.embeddings,
                            allow_dangerous_deserialization=True
                        )
                        print("âœ… FAISS ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ (í‘œì¤€ ë°©ë²•)")
                    except (KeyError, AttributeError) as e:
                        if '__fields_set__' in str(e) or 'pydantic' in str(e).lower():
                            print("âš ï¸ Pydantic í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€, ì»¤ìŠ¤í…€ ë¡œë” ì‚¬ìš©...")
                            
                            # ë°©ë²• 2: ì»¤ìŠ¤í…€ ë¡œë” ì‚¬ìš©
                            import faiss
                            from langchain_community.docstore.in_memory import InMemoryDocstore
                            from langchain_community.vectorstores import FAISS
                            
                            # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                            index = faiss.read_index(index_path)
                            
                            # JSON íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ
                            json_filename = "fss_sanctions_parsed.json" if "sanctions" in self.vector_db_path else "fss_management_parsed.json"
                            json_path = os.path.join(self.vector_db_path, json_filename)
                            
                            if os.path.exists(json_path):
                                print(f"ğŸ“„ JSON íŒŒì¼ì—ì„œ ë¬¸ì„œ ë¡œë“œ ì¤‘: {json_path}")
                                with open(json_path, 'r', encoding='utf-8') as f:
                                    data = json.load(f)
                                
                                # ë¬¸ì„œ ìƒì„±
                                from langchain_core.documents import Document
                                documents = []
                                
                                # ë°ì´í„° êµ¬ì¡° í™•ì¸
                                if isinstance(data, dict) and 'documents' in data:
                                    docs_list = data['documents']
                                elif isinstance(data, list):
                                    docs_list = data
                                else:
                                    print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” JSON ë°ì´í„° êµ¬ì¡°")
                                    return False
                                
                                for doc in docs_list:
                                    if not isinstance(doc, dict):
                                        continue
                                        
                                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                    content = doc.get('content', {})
                                    if isinstance(content, dict):
                                        # ì œì¬ ì •ë³´ì˜ ê²½ìš°
                                        full_text = content.get('full_text', '')
                                        if not full_text:
                                            # ìƒì„¸ ë‚´ìš© êµ¬ì„±
                                            sanction_facts = content.get('sanction_facts', [])
                                            facts_text = ""
                                            for fact in sanction_facts:
                                                if isinstance(fact, dict):
                                                    facts_text += f"\n- {fact.get('title', '')}: {fact.get('content', '')}"
                                            
                                            fine_info = content.get('fine', {})
                                            if isinstance(fine_info, dict):
                                                fine_text = fine_info.get('text', '')
                                            else:
                                                fine_text = str(fine_info)
                                            
                                            full_text = f"ì œì¬ì‚¬ì‹¤:\n{facts_text}\n\nì œì¬ë‚´ìš©: {content.get('sanction_type', '')}\n{fine_text}\n{content.get('executive_sanction', '')}"
                                        text = full_text
                                    else:
                                        text = str(content)
                                    
                                    # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                                    metadata = {
                                        'institution': doc.get('institution', ''),
                                        'doc_id': doc.get('doc_id', ''),
                                    }
                                    
                                    # ë¬¸ì„œ íƒ€ì… ì„¤ì •
                                    if "sanctions" in self.vector_db_path:
                                        metadata['doc_type'] = 'ì œì¬ì •ë³´'
                                        if isinstance(content, dict):
                                            metadata['sanction_type'] = content.get('sanction_type', '')
                                    else:
                                        metadata['doc_type'] = 'ê²½ì˜ìœ ì˜ì‚¬í•­'
                                        if isinstance(content, dict):
                                            metadata['management_type'] = content.get('management_type', '')
                                    
                                    # ë‚ ì§œ í•„ë“œ ì¶”ê°€
                                    if 'sanction_date' in doc:
                                        metadata['sanction_date'] = doc['sanction_date']
                                        metadata['date'] = doc['sanction_date']
                                    elif 'disclosure_date' in doc:
                                        metadata['disclosure_date'] = doc['disclosure_date']
                                        metadata['date'] = doc['disclosure_date']
                                    
                                    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                                    doc_metadata = doc.get('metadata', {})
                                    if isinstance(doc_metadata, dict):
                                        # ê·œì • ì •ë³´ ì¶”ê°€
                                        if 'regulations' in doc_metadata:
                                            metadata['regulations'] = doc_metadata['regulations']
                                        
                                        # ê¸°íƒ€ ë©”íƒ€ë°ì´í„° ë³µì‚¬
                                        for key, value in doc_metadata.items():
                                            if key not in metadata and value:
                                                metadata[key] = value
                                    
                                    if text.strip():  # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                                        print(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ: {metadata['institution']} ({metadata['date']})")
                                        documents.append(Document(page_content=text, metadata=metadata))
                                
                                print(f"ğŸ“„ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                                
                                # ë¬¸ì„œ ì„ë² ë”© ìƒì„±
                                print("ğŸ”„ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
                                texts = [doc.page_content for doc in documents]
                                metadatas = [doc.metadata for doc in documents]
                                
                                # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
                                self.vector_store = FAISS.from_texts(
                                    texts,
                                    self.embeddings,
                                    metadatas=metadatas
                                )
                                print("âœ… ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„± ì™„ë£Œ")
                            else:
                                print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
                                return False
                        else:
                            raise e
                    
                    # ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸
                    try:
                        test_query = "í…ŒìŠ¤íŠ¸"
                        test_results = self.vector_store.similarity_search(test_query, k=1)
                        if test_results:
                            print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ (ê²°ê³¼ ìˆ˜: {len(test_results)})")
                            # ì²« ë²ˆì§¸ ê²°ê³¼ì˜ ë©”íƒ€ë°ì´í„° ì¶œë ¥
                            print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {test_results[0].metadata}")
                        else:
                            print("âš ï¸ ë²¡í„° ì €ì¥ì†Œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    except Exception as test_error:
                        print(f"âš ï¸ ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {test_error}")
                        import traceback
                        traceback.print_exc()
                    
                    return True
                    
                except Exception as e:
                    print(f"âŒ FAISS ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    return False
                    
            elif vector_store_type == 'CHROMA' or (vector_store_type == 'FAISS' and not FAISS_AVAILABLE):
                chroma_path = self.vector_db_path
                print(f"âœ… ê¸°ì¡´ Chroma ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {chroma_path}")
                try:
                    from langchain_community.vectorstores import Chroma
                    self.vector_store = Chroma(
                        persist_directory=chroma_path,
                        embedding_function=self.embeddings
                    )
                except Exception as e:
                    print(f"âŒ Chroma ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    return False
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ë²¡í„° ì €ì¥ì†Œ íƒ€ì…ì…ë‹ˆë‹¤: {vector_store_type}")
                return False

            print(f"âœ… ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì™„ë£Œ")
            self.check_vector_store()
            return True

        except Exception as e:
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def check_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ìƒíƒœ ë° ê¸°ëŠ¥ í™•ì¸"""
        try:
            print("ğŸ” ë²¡í„° ì €ì¥ì†Œ ìƒíƒœ í™•ì¸ ì¤‘...")
            
            # ë©”ì„œë“œ í™•ì¸
            methods = [
                method for method in dir(self.vector_store)
                if callable(getattr(self.vector_store, method)) and not method.startswith("_")
            ]
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ: {', '.join(methods[:5])}... (ì´ {len(methods)}ê°œ)")
            
            # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            try:
                print("ğŸ” ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì¤‘...")
                test_query = "ê¸ˆìœµ"
                if hasattr(self.vector_store, "similarity_search") and callable(getattr(self.vector_store, "similarity_search")):
                    results = self.vector_store.similarity_search(test_query, k=1)
                    if results:
                        print(f"âœ… í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ê²°ê³¼")
                        # ì²« ë²ˆì§¸ ê²°ê³¼ ë©”íƒ€ë°ì´í„° í™•ì¸
                        if results[0].metadata:
                            print(f"ğŸ“„ ë©”íƒ€ë°ì´í„° í‚¤: {', '.join(list(results[0].metadata.keys()))}")
                    else:
                        print("âš ï¸ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                else:
                    print("âš ï¸ similarity_search ë©”ì„œë“œ ì—†ìŒ")
            except Exception as e:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def initialize_llm(self) -> None:
        """LLM ì´ˆê¸°í™”"""
        try:
            # Anthropic Claude API ì‚¬ìš©
            if self.use_anthropic:
                try:
                    # Anthropic API í‚¤ í™•ì¸
                    anthropic_api_key = self.anthropic_api_key or os.getenv("ANTHROPIC_APIKEY")
                    if not anthropic_api_key:
                        print("âŒ Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        return
                    
                    print("ğŸ§  Anthropic Claude API ì´ˆê¸°í™” ì¤‘...")
                    try:
                        # ì‹ ê·œ ë²„ì „ import ì‹œë„
                        try:
                            from langchain_anthropic import ChatAnthropic
                            
                            # LLM ì´ˆê¸°í™”
                            self.llm = ChatAnthropic(
                                model="claude-3-opus-20240229",  # ìµœì‹  Claude ëª¨ë¸
                                anthropic_api_key=anthropic_api_key
                            )
                        except ImportError:
                            # ê¸°ì¡´ ë²„ì „ fallback
                            print("âš ï¸ langchain_anthropic ì„í¬íŠ¸ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‹œë„...")
                            from langchain.chat_models import ChatAnthropic
                            self.llm = ChatAnthropic(
                                model_name="claude-3-opus-20240229",  # ìµœì‹  Claude ëª¨ë¸
                                anthropic_api_key=anthropic_api_key
                            )
                        
                        print("âœ… Anthropic Claude API ì´ˆê¸°í™” ì™„ë£Œ")
                    except Exception as anthro_error:
                        print(f"âŒ Anthropic ì´ˆê¸°í™” ì‹¤íŒ¨: {anthro_error}")
                        return
                    
                except Exception as e:
                    print(f"âŒ Anthropic API ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    return
            # OpenAI API ì‚¬ìš©
            else:
                try:
                    openai_api_key = os.getenv("OPENAI_API_KEY")
                    if not openai_api_key:
                        print("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                        return
                    
                    print(f"ğŸ§  OpenAI API ì´ˆê¸°í™” ì¤‘: {self.llm_model_name}...")
                    
                    # ëª¨ë¸ ì´ë¦„ í˜¸í™˜ì„± í™•ì¸
                    model_name = self.llm_model_name
                    if model_name == "gpt-3.5-turbo":
                        print("âš ï¸ 'gpt-3.5-turbo'ëŠ” ë ˆê±°ì‹œ ì´ë¦„ì…ë‹ˆë‹¤. 'gpt-3.5-turbo-0125'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.")
                        model_name = "gpt-3.5-turbo-0125"
                    
                    # ì„í¬íŠ¸ ì‹œë„
                    try:
                        # ì‹ ê·œ ë²„ì „ import
                        from langchain_openai import ChatOpenAI
                        
                        # LLM ì´ˆê¸°í™”
                        self.llm = ChatOpenAI(
                            model=model_name,
                            temperature=0.3,
                            openai_api_key=openai_api_key
                        )
                    except ImportError:
                        # ê¸°ì¡´ ë²„ì „ fallback
                        print("âš ï¸ langchain_openai ì„í¬íŠ¸ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‹œë„...")
                        from langchain.chat_models import ChatOpenAI
                        self.llm = ChatOpenAI(
                            model_name=model_name,
                            temperature=0.3,
                            openai_api_key=openai_api_key
                        )
                    
                    print("âœ… OpenAI API ì´ˆê¸°í™” ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"âŒ OpenAI API ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    return
            
            # QA ì²´ì¸ ì„¤ì •
            self.setup_qa_chain()
            
        except Exception as e:
            print(f"âŒ LLM ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()

    def setup_qa_chain(self) -> None:
        """QA ì²´ì¸ ì„¤ì •"""
        try:
            # ë²¡í„° ì €ì¥ì†Œì™€ LLMì´ ëª¨ë‘ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not self.vector_store:
                print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            if not self.llm:
                print("âŒ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            # QA ì²´ì¸ì„ ì§ì ‘ êµ¬ì„±í•˜ì§€ ì•Šê³  ê²€ìƒ‰ ê³¼ì •ì„ ë³„ë„ë¡œ ê´€ë¦¬
            print("âœ… QA ì²´ì¸ ìƒì„± ì™„ë£Œ")
            self.qa_chain = True  # ë”ë¯¸ ê°’, QA ì²´ì¸ì´ ì¤€ë¹„ë˜ì—ˆë‹¤ëŠ” í‘œì‹œìš©
            
        except Exception as e:
            print(f"âŒ QA ì²´ì¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            self.qa_chain = None
    
    def _match_filters(self, metadata: Dict[str, Any], filters: Dict[str, Any]) -> bool:
        """ë©”íƒ€ë°ì´í„°ê°€ í•„í„° ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸"""
        if not filters:
            return True
        
        # ê¸°ê´€ ìœ í˜• í•„í„°ë§
        if 'institution_types' in filters and filters['institution_types']:
            institution = metadata.get('institution', '').lower()
            found_match = False
            for inst_type in filters['institution_types']:
                if inst_type.lower() in institution:
                    found_match = True
                    break
            if not found_match:
                return False
        
        # ë‚ ì§œ í•„í„°ë§
        if 'date_filter' in filters and 'date_value' in filters:
            date_str = metadata.get('date', '')
            if not date_str:
                # ë‹¤ë¥¸ ë‚ ì§œ ê´€ë ¨ í•„ë“œ í™•ì¸
                date_str = metadata.get('sanction_date', '')
                if not date_str:
                    date_str = metadata.get('disclosure_date', '')
                
                # ì—¬ì „íˆ ë‚ ì§œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
                if not date_str:
                    print(f"âš ï¸ ë‚ ì§œ ì •ë³´ ì—†ìŒ: {metadata}")
                    return False
                
            # ë‚ ì§œ í˜•ì‹ ì •ê·œí™” (YYYY.MM.DD ë˜ëŠ” YYYY-MM-DD)
            date_str = date_str.replace('-', '.').strip()
            
            # ì—°ë„ë§Œ ì¶”ì¶œ
            year_match = re.search(r'(20\d{2})', date_str)
            if not year_match:
                print(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ì¸ì‹ ë¶ˆê°€: {date_str}")
                return False
                
            document_year = year_match.group(1)
            filter_year = filters['date_value']
            
            # ìµœê·¼ 1ë…„ í•„í„°ë§ (ì˜ˆ: 2023ë…„ ì´ìƒ)
            if len(filter_year) == 4 and filter_year.isdigit():
                if int(document_year) < int(filter_year):
                    return False
            
            print(f"âœ… ë‚ ì§œ ë§¤ì¹­: ë¬¸ì„œ={document_year}, í•„í„°={filter_year}")
        
        return True

    def preprocess_query(self, query: str) -> Tuple[str, Dict[str, Any]]:
        """ì§ˆë¬¸ ì „ì²˜ë¦¬ ë° í•„í„° ì¶”ì¶œ"""
        processed_query = query
        
        # í•„í„° ì´ˆê¸°í™”
        filters = {}
        
        # ì€í–‰/ë³´í—˜ì‚¬/ì¦ê¶Œì‚¬ í•„í„°ë§
        institution_types = []
        if 'ì€í–‰' in query:
            institution_types.append('ì€í–‰')
        if 'ë³´í—˜' in query:
            institution_types.append('ë³´í—˜')
        if 'ì¦ê¶Œ' in query:
            institution_types.append('ì¦ê¶Œ')
        if 'ì¹´ë“œ' in query:
            institution_types.append('ì¹´ë“œ')
        if 'ê¸ˆìœµ' in query:
            institution_types.append('ê¸ˆìœµ')
        
        if institution_types:
            filters['institution_types'] = institution_types
        
        # ë‚ ì§œ í•„í„°ë§ (ìµœê·¼ 1ë…„, ì˜¬í•´, 2023ë…„ ë“±)
        date_filter = None
        if 'ìµœê·¼ 1ë…„' in query or 'ì§€ë‚œ 1ë…„' in query:
            date_filter = 'date'
            # í˜„ì¬ ì—°ë„ë¥¼ ì‚¬ìš©
            current_year = datetime.now().year
            date_value = str(current_year - 1)  # 1ë…„ ì „ë¶€í„°
            filters['date_filter'] = date_filter
            filters['date_value'] = date_value
            print(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {date_value}ë…„ë¶€í„°")
        elif 'ì˜¬í•´' in query:
            date_filter = 'date'
            date_value = str(datetime.now().year)
            filters['date_filter'] = date_filter
            filters['date_value'] = date_value
            print(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {date_value}ë…„")
        else:
            # ì—°ë„ ì¶”ì¶œ (YYYYë…„)
            year_match = re.search(r'(20\d{2})ë…„', query)
            if year_match:
                date_filter = 'date'
                date_value = year_match.group(1)
                filters['date_filter'] = date_filter
                filters['date_value'] = date_value
                print(f"ğŸ“… ë‚ ì§œ í•„í„°ë§: {date_value}ë…„")
        
        # ë¬¸ì„œ ìœ í˜• í•„í„°ë§
        doc_type_filter = None
        if 'ê²½ì˜ìœ ì˜' in query or 'ê²½ì˜ ìœ ì˜' in query:
            doc_type_filter = 'management'
        elif 'ì œì¬' in query or 'ì§•ê³„' in query or 'ê³¼íƒœë£Œ' in query or 'ê³¼ì§•ê¸ˆ' in query:
            doc_type_filter = 'sanctions'

        if doc_type_filter:
            filters['doc_type'] = doc_type_filter
        
        return processed_query, filters
    
    def answer_question(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ë‹µë³€"""
        try:
            # ë²¡í„° ì €ì¥ì†Œ ì²´í¬
            if not self.vector_store:
                return {
                    "answer": "ë²¡í„° ì €ì¥ì†Œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.",
                    "sources": []
                }
                
            # LLM ì²´í¬
            if not self.llm:
                return {
                    "answer": "LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'LLM ì´ˆê¸°í™”' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.",
                    "sources": []
                }
                
            # ì§ˆë¬¸ ì „ì²˜ë¦¬
            print(f"â“ ì§ˆë¬¸ ì²˜ë¦¬: '{question}'")
            processed_query, filters = self.preprocess_query(question)
            
            # ë¬¸ì„œ ìœ í˜• í•„í„° í™•ì¸
            if 'doc_type' in filters and filters['doc_type'] != self.db_type:
                if filters['doc_type'] == 'management':
                    return {
                        "answer": "í˜„ì¬ ì œì¬ DBê°€ ì„ íƒë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê²½ì˜ìœ ì˜ì‚¬í•­ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹œë ¤ë©´ DBë¥¼ ë³€ê²½í•´ì£¼ì„¸ìš”.",
                        "sources": []
                    }
                else:
                    return {
                        "answer": "í˜„ì¬ ê²½ì˜ìœ ì˜ DBê°€ ì„ íƒë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì œì¬ ì •ë³´ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹œë ¤ë©´ DBë¥¼ ë³€ê²½í•´ì£¼ì„¸ìš”.",
                        "sources": []
                    }
            
            if filters:
                print(f"ğŸ” ì¶”ì¶œëœ í•„í„°: {filters}")
            
            # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰
            search_results = self.search_documents(processed_query, k=5)
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            if not search_results:
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return {
                    "answer": "ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                    "sources": []
                }
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = ""
            sources = []
            
            for idx, doc in enumerate(search_results):
                try:
                    metadata = doc.get("metadata", {})
                    content = doc.get("content", "")
                    
                    # ìœ íš¨í•œ ë©”íƒ€ë°ì´í„° í™•ì¸
                    institution = metadata.get("institution", "")
                    if not institution:
                        institution = "ë¯¸ìƒ"
                        
                    # ë‚ ì§œ í•„ë“œ í™•ì¸
                    date = metadata.get("date", "")
                    if not date:
                        date = metadata.get("sanction_date", "")
                    if not date:
                        date = metadata.get("disclosure_date", "")
                    if not date:
                        date = "ë‚ ì§œ ë¯¸ìƒ"
                    
                    # ë¬¸ì„œ íƒ€ì… í™•ì¸
                    doc_type = metadata.get("doc_type", "")
                    if not doc_type and self.db_type == "sanctions":
                        doc_type = "ì œì¬ì •ë³´"
                    elif not doc_type:
                        doc_type = "ê²½ì˜ìœ ì˜ì‚¬í•­"
                    
                    context += f"[ë¬¸ì„œ {idx+1}]\n"
                    context += f"ê¸°ê´€: {institution}\n"
                    context += f"ë‚ ì§œ: {date}\n"
                    context += f"ìœ í˜•: {doc_type}\n"
                    context += f"ë‚´ìš©:\n{content}\n\n"
                    
                    sources.append({
                        "content": content,
                        "metadata": metadata
                    })
                except Exception as doc_error:
                    print(f"âš ï¸ ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {doc_error}")
                    continue
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            try:
                # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if len(context) > 12000:
                    print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤: {len(context)}ì â†’ 12000ìë¡œ ìë¦…ë‹ˆë‹¤")
                    context = context[:12000] + "..."
                    
                prompt = f"""ë‹¹ì‹ ì€ ê¸ˆìœµê°ë…ì› ì œì¬ ë° ê²½ì˜ìœ ì˜ ì •ë³´ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì¼ë¶€ì…ë‹ˆë‹¤. 
ë‹¤ìŒì€ ê²€ìƒ‰ëœ ê¸ˆìœµê°ë…ì› ê´€ë ¨ ìë£Œì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {question}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
1. ìë£Œì— ë‚˜ì˜¤ì§€ ì•ŠëŠ” ë‚´ìš©ì´ë©´ "ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
2. ê¸°ê´€ëª…, ë‚ ì§œ, ì œì¬ ìœ í˜•, ê¸ˆì•¡ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
3. ìë£Œì˜ ì¶œì²˜ë¥¼ ëª…í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”.
4. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

                print("ğŸ§  LLMì— ë‹µë³€ ìš”ì²­ ì¤‘...")
                answer = ""
                
                # ë‹¤ì–‘í•œ LLM í˜¸ì¶œ ë°©ì‹ ì‹œë„
                try:
                    # ë°©ì‹ 1: invoke ë©”ì„œë“œ (ìµœì‹  LangChain)
                    if hasattr(self.llm, "invoke"):
                        result = self.llm.invoke(prompt)
                        if hasattr(result, "content"):
                            answer = result.content
                        else:
                            answer = str(result)
                    # ë°©ì‹ 2: __call__ ë©”ì„œë“œ (êµ¬ LangChain)
                    else:
                        answer = str(self.llm(prompt))
                        
                    print("âœ… LLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
                    
                except Exception as llm_error:
                    print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {llm_error}")
                    # ê¸°ë³¸ ì‘ë‹µ ìƒì„±
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤. LLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n"
                    for idx, doc in enumerate(search_results[:3]):
                        metadata = doc.get("metadata", {})
                        institution = metadata.get('institution', 'N/A')
                        date = metadata.get('date', metadata.get('sanction_date', metadata.get('disclosure_date', 'N/A')))
                        answer += f"{idx+1}. {institution} ({date})\n"
                
                return {
                    "answer": answer,
                    "sources": sources
                }
                
            except Exception as e:
                print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                import traceback
                traceback.print_exc()
                
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
                answer = f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:\n\n"
                for i, doc in enumerate(search_results):
                    metadata = doc.get("metadata", {})
                    institution = metadata.get('institution', 'N/A')
                    date = metadata.get('date', metadata.get('sanction_date', metadata.get('disclosure_date', 'N/A')))
                    answer += f"{i+1}. {institution} ({date})\n"
                
                return {
                    "answer": answer,
                    "sources": sources
                }
                
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            
            return {
                "answer": "ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                "sources": []
            }
    
    def _apply_filters(self, docs, filters):
        """ì¶”ì¶œëœ í•„í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ í•„í„°ë§"""
        if not filters:
            return docs
        
        filtered_docs = []
        
        for doc in docs:
            metadata = doc.metadata
            include_doc = True
            
            # ë‚ ì§œ í•„í„° ì ìš©
            if "date_filter" in filters and "date_value" in filters:
                date_field = filters["date_filter"]
                min_year = filters["date_value"]
                
                if date_field in metadata:
                    doc_date = metadata[date_field]
                    try:
                        # ë‚ ì§œ í˜•ì‹ ë‹¤ì–‘ì„± ì²˜ë¦¬ (YYYY.MM.DD ë˜ëŠ” YYYY-MM-DD)
                        doc_year = re.search(r"(\d{4})", doc_date).group(1)
                        if doc_year < min_year:
                            include_doc = False
                    except:
                        pass
            
            # ê¸°ê´€ ìœ í˜• í•„í„° ì ìš©
            if "institution_types" in filters and include_doc:
                institution = metadata.get("institution", "").lower()
                
                institution_match = False
                for inst_type in filters["institution_types"]:
                    if inst_type.lower() in institution:
                        institution_match = True
                        break
                
                if not institution_match:
                    include_doc = False
            
            # ì œì¬ ìœ í˜• í•„í„° ì ìš©
            if "sanction_types" in filters and include_doc:
                sanction_type = metadata.get("sanction_type", "").lower()
                management_type = metadata.get("management_type", "").lower()
                
                type_field = sanction_type if sanction_type else management_type
                
                sanction_match = False
                for sanc_type in filters["sanction_types"]:
                    if sanc_type.lower() in type_field or sanc_type.lower() in doc.page_content.lower():
                        sanction_match = True
                        break
                
                if not sanction_match:
                    include_doc = False
            
            # ë²•ê·œ í•„í„° ì ìš©
            if "regulations" in filters and include_doc:
                # ë©”íƒ€ë°ì´í„°ì— regulations í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                regulations = []
                if "regulations" in metadata and isinstance(metadata["regulations"], list):
                    regulations = metadata["regulations"]
                
                # ë³¸ë¬¸ ê²€ìƒ‰
                content_lower = doc.page_content.lower()
                
                reg_match = False
                for reg in filters["regulations"]:
                    # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
                    for doc_reg in regulations:
                        if reg.lower() in doc_reg.lower():
                            reg_match = True
                            break
                    
                    # ë³¸ë¬¸ ê²€ìƒ‰
                    if reg.lower() in content_lower:
                        reg_match = True
                        break
                
                if not reg_match:
                    include_doc = False
            
            # ë‚´ë¶€í†µì œ í•„í„° ì ìš©
            if "internal_control" in filters and filters["internal_control"] and include_doc:
                content_lower = doc.page_content.lower()
                
                internal_control_keywords = ["ë‚´ë¶€í†µì œ", "ë‚´ë¶€ í†µì œ", "í†µì œ", "ê´€ë¦¬ì²´ê³„", "ê´€ë¦¬ ì²´ê³„"]
                internal_control_match = any(keyword in content_lower for keyword in internal_control_keywords)
                
                if not internal_control_match:
                    include_doc = False
            
            # í•„í„°ë¥¼ ëª¨ë‘ í†µê³¼í•œ ë¬¸ì„œë§Œ ì¶”ê°€
            if include_doc:
                filtered_docs.append(doc)
        
        return filtered_docs
    
    def search_documents(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """ë²¡í„° ì €ì¥ì†Œì—ì„œ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            if not self.vector_store:
                print("âŒ ë²¡í„° ì €ì¥ì†Œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return []
            
            # ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ ìƒì„±
            processed_query, filters = self.preprocess_query(query)
            
            print(f"ğŸ” ê²€ìƒ‰ì–´: '{processed_query}', í•„í„°: {filters}")
            
            try:
                # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ì‹ ì‹œë„
                docs = None
                
                # 1. ë‹¨ìˆœ ê²€ìƒ‰ (ê°€ì¥ ì•ˆì •ì )
                try:
                    print("ğŸ“š ê²€ìƒ‰ ë°©ì‹ 1: similarity_search ì‹œë„...")
                    docs = self.vector_store.similarity_search(
                        processed_query, 
                        k=k*2  # í•„í„°ë§ í›„ ì¶©ë¶„í•œ ê²°ê³¼ í™•ë³´ë¥¼ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
                    )
                    print(f"âœ… ê²€ìƒ‰ ì„±ê³µ: {len(docs)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
                except Exception as e1:
                    print(f"âš ï¸ similarity_search ì‹¤íŒ¨: {str(e1)}")
                    
                    # 2. ê²€ìƒ‰ ë¬¸ì„œ ì§ì ‘ êµ¬ì„±
                    try:
                        print("ğŸ“š ê²€ìƒ‰ ë°©ì‹ 2: ì§ì ‘ ê²€ìƒ‰ ì‹œë„...")
                        if hasattr(self.vector_store, "_collection"):
                            # Chromaìš© ê²€ìƒ‰
                            from langchain_core.documents import Document
                            
                            # ì„ë² ë”© ìƒì„±
                            query_embedding = self.embeddings.embed_query(processed_query)
                            
                            # Chroma ì»¬ë ‰ì…˜ì—ì„œ ì§ì ‘ ê²€ìƒ‰
                            results = self.vector_store._collection.query(
                                query_embeddings=[query_embedding],
                                n_results=k*2
                            )
                            
                            # ë¬¸ì„œ êµ¬ì„±
                            docs = []
                            for i, (id, dist) in enumerate(zip(results['ids'][0], results['distances'][0])):
                                if i >= k*2:
                                    break
                                metadata = json.loads(results['metadatas'][0][i]) if results['metadatas'][0][i] else {}
                                content = results['documents'][0][i] if results['documents'][0][i] else ""
                                docs.append(Document(page_content=content, metadata=metadata))
                            
                            print(f"âœ… Chroma ì§ì ‘ ê²€ìƒ‰ ì„±ê³µ: {len(docs)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
                    except Exception as e2:
                        print(f"âš ï¸ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e2)}")
                        
                        # 3. ìµœí›„ì˜ ë°©ë²• - ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
                        try:
                            print("ğŸ“š ê²€ìƒ‰ ë°©ì‹ 3: ëª¨ë“  ë¬¸ì„œ ë°˜í™˜ ì‹œë„...")
                            if hasattr(self.vector_store, "docstore"):
                                # FAISSìš© ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
                                all_docs = []
                                for doc_id in list(self.vector_store.docstore._dict.values())[:k*2]:
                                    all_docs.append(doc_id)
                                docs = all_docs
                                print(f"âœ… ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰ ì„±ê³µ: {len(docs)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
                        except Exception as e3:
                            print(f"âŒ ëª¨ë“  ê²€ìƒ‰ ë°©ì‹ ì‹¤íŒ¨: {str(e3)}")
                            return []
                
                # ê²€ìƒ‰ ê²°ê³¼ ì—†ìœ¼ë©´ ì¢…ë£Œ
                if not docs or len(docs) == 0:
                    print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return []
                
                # í•„í„°ë§ ì ìš©
                filtered_results = []
                
                for doc in docs:
                    metadata = doc.metadata if hasattr(doc, "metadata") else {}
                    content = doc.page_content if hasattr(doc, "page_content") else str(doc)
                    
                    # í•„í„°ë§ ì ìš©
                    if filters and not self._match_filters(metadata, filters):
                        continue
                        
                    result = {
                        "content": content,
                        "metadata": metadata,
                        "score": 1.0  # ì ìˆ˜ ì •ë³´ ì—†ìŒ
                    }
                    filtered_results.append(result)
                
                print(f"âœ… í•„í„°ë§ í›„: {len(filtered_results)}ê°œ ë¬¸ì„œ ë‚¨ìŒ")
                
                # ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° í•„í„° ì—†ì´ ë°˜í™˜
                if not filtered_results and filters and docs:
                    print("âš ï¸ í•„í„°ë§ ê²°ê³¼ê°€ ì—†ì–´ í•„í„° ì—†ì´ ëª¨ë“  ê²°ê³¼ ë°˜í™˜")
                    filtered_results = [
                        {
                            "content": doc.page_content if hasattr(doc, "page_content") else str(doc),
                            "metadata": doc.metadata if hasattr(doc, "metadata") else {},
                            "score": 1.0
                        } for doc in docs[:k]
                    ]
                
                # ìµœëŒ€ kê°œ ë°˜í™˜
                return filtered_results[:k]
                
            except Exception as e:
                print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                import traceback
                traceback.print_exc()
                return []
        
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def _doc_passes_filters(self, doc, filters):
        """ë¬¸ì„œê°€ í•„í„° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸"""
        if not filters:
            return True
        
        metadata = doc.metadata
        
        # ë‚ ì§œ í•„í„° ì ìš©
        if "date_filter" in filters and "date_value" in filters:
            date_field = filters["date_filter"]
            min_year = filters["date_value"]
            
            if date_field in metadata:
                doc_date = metadata[date_field]
                try:
                    # ë‚ ì§œ í˜•ì‹ ë‹¤ì–‘ì„± ì²˜ë¦¬ (YYYY.MM.DD ë˜ëŠ” YYYY-MM-DD)
                    doc_year = re.search(r"(\d{4})", doc_date).group(1)
                    if doc_year < min_year:
                        return False
                except:
                    pass
        
        # ê¸°ê´€ ìœ í˜• í•„í„° ì ìš©
        if "institution_types" in filters:
            institution = metadata.get("institution", "").lower()
            
            institution_match = False
            for inst_type in filters["institution_types"]:
                if inst_type.lower() in institution:
                    institution_match = True
                    break
            
            if not institution_match:
                return False
        
        # ì œì¬ ìœ í˜• í•„í„° ì ìš©
        if "sanction_types" in filters:
            sanction_type = metadata.get("sanction_type", "").lower()
            management_type = metadata.get("management_type", "").lower()
            
            type_field = sanction_type if sanction_type else management_type
            
            sanction_match = False
            for sanc_type in filters["sanction_types"]:
                if sanc_type.lower() in type_field or sanc_type.lower() in doc.page_content.lower():
                    sanction_match = True
                    break
            
            if not sanction_match:
                return False
        
        # ë²•ê·œ í•„í„° ì ìš©
        if "regulations" in filters:
            # ë©”íƒ€ë°ì´í„°ì— regulations í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
            regulations = []
            if "regulations" in metadata and isinstance(metadata["regulations"], list):
                regulations = metadata["regulations"]
            
            # ë³¸ë¬¸ ê²€ìƒ‰
            content_lower = doc.page_content.lower()
            
            reg_match = False
            for reg in filters["regulations"]:
                # ë©”íƒ€ë°ì´í„° ê²€ìƒ‰
                for doc_reg in regulations:
                    if reg.lower() in doc_reg.lower():
                        reg_match = True
                        break
                
                # ë³¸ë¬¸ ê²€ìƒ‰
                if reg.lower() in content_lower:
                    reg_match = True
                    break
            
            if not reg_match:
                return False
        
        # ë‚´ë¶€í†µì œ í•„í„° ì ìš©
        if "internal_control" in filters and filters["internal_control"]:
            content_lower = doc.page_content.lower()
            
            internal_control_keywords = ["ë‚´ë¶€í†µì œ", "ë‚´ë¶€ í†µì œ", "í†µì œ", "ê´€ë¦¬ì²´ê³„", "ê´€ë¦¬ ì²´ê³„"]
            internal_control_match = any(keyword in content_lower for keyword in internal_control_keywords)
            
            if not internal_control_match:
                return False
        
        return True
    
    def interactive_mode(self) -> None:
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print("\nğŸ¤– ê¸ˆìœµ ì œì¬/ê²½ì˜ìœ ì˜ì‚¬í•­ RAG ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ì„ ì…ë ¥í•˜ì„¸ìš”.")
        print("ğŸ’¡ 'search:'ë¡œ ì‹œì‘í•˜ë©´ ê²€ìƒ‰ ëª¨ë“œ, ê·¸ ì™¸ì—ëŠ” ì§ˆì˜ì‘ë‹µ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        
        while True:
            user_input = input("\nâ“ ì…ë ¥: ")
            if user_input.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
                print("ğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # ê²€ìƒ‰ ëª¨ë“œ
            if user_input.lower().startswith("search:"):
                query = user_input[7:].strip()
                if not query:
                    print("âŒ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                print(f"ğŸ” ê²€ìƒ‰: '{query}'")
                results = self.search_documents(query)
                
                if not results:
                    print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                print("\nğŸ“š ê²€ìƒ‰ ê²°ê³¼:")
                for i, result in enumerate(results):
                    print(f"\nê²°ê³¼ #{i+1} (ì ìˆ˜: {result['score']:.4f})")
                    
                    # DB íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í•„ë“œ ì¶œë ¥
                    if self.db_type == "sanctions":
                        print(f"ê¸°ê´€: {result['metadata'].get('institution', 'N/A')}")
                        print(f"ì œì¬ì¼: {result['metadata'].get('sanction_date', 'N/A')}")
                        print(f"ìœ í˜•: {result['metadata'].get('sanction_type', 'N/A')}")
                    else:
                        print(f"ê¸°ê´€: {result['metadata'].get('institution', 'N/A')}")
                        print(f"ê³µì‹œì¼: {result['metadata'].get('disclosure_date', 'N/A')}")
                        print(f"ìœ í˜•: {result['metadata'].get('management_type', 'N/A')}")
                    
                    print(f"ë‚´ìš©: {result['content'][:200]}...")
            
            # ì§ˆì˜ì‘ë‹µ ëª¨ë“œ
            else:
                result = self.answer_question(user_input)
                
                print("\nğŸ¤– ë‹µë³€:")
                print(result["answer"])
                
                if result["sources"]:
                    print("\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
                    for i, source in enumerate(result["sources"][:3]):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                        print(f"\nì¶œì²˜ #{i+1}:")
                        
                        # DB íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í•„ë“œ ì¶œë ¥
                        if self.db_type == "sanctions":
                            print(f"ê¸°ê´€: {source['metadata'].get('institution', 'N/A')}")
                            print(f"ì œì¬ì¼: {source['metadata'].get('sanction_date', 'N/A')}")
                            print(f"ìœ í˜•: {source['metadata'].get('sanction_type', 'N/A')}")
                        else:
                            print(f"ê¸°ê´€: {source['metadata'].get('institution', 'N/A')}")
                            print(f"ê³µì‹œì¼: {source['metadata'].get('disclosure_date', 'N/A')}")
                            print(f"ìœ í˜•: {source['metadata'].get('management_type', 'N/A')}")
                        
                        print(f"ë‚´ìš©: {source['content']}")

    def _rebuild_vector_store_from_json(self):
        """JSON íŒŒì¼ì—ì„œ ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„±"""
        try:
            print("ğŸ”„ JSON íŒŒì¼ì—ì„œ ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„± ì¤‘...")
            
            # JSON íŒŒì¼ ê²½ë¡œ ê²°ì •
            json_filename = "fss_sanctions_parsed.json" if "sanctions" in self.vector_db_path else "fss_management_parsed.json"
            json_path = os.path.join(self.vector_db_path, json_filename)
            
            if not os.path.exists(json_path):
                print(f"âŒ ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
                return False
            
            # JSON ë°ì´í„° ë¡œë“œ
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë°ì´í„° êµ¬ì¡° í™•ì¸
            if isinstance(data, list):
                documents_list = data
            elif isinstance(data, dict) and 'documents' in data:
                documents_list = data['documents']
            else:
                print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” JSON ë°ì´í„° êµ¬ì¡°: {type(data)}")
                return False
            
            # ë¬¸ì„œ ìƒì„±
            from langchain_core.documents import Document
            documents = []
            
            for doc in documents_list:
                if not isinstance(doc, dict):
                    continue
                    
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                content = doc.get('content', {})
                if isinstance(content, dict):
                    # ì œì¬ ì •ë³´ì˜ ê²½ìš°
                    full_text = content.get('full_text', '')
                    if not full_text:
                        # ìƒì„¸ ë‚´ìš© êµ¬ì„±
                        sanction_facts = content.get('sanction_facts', [])
                        facts_text = ""
                        for fact in sanction_facts:
                            if isinstance(fact, dict):
                                facts_text += f"\n- {fact.get('title', '')}: {fact.get('content', '')}"
                        
                        fine_info = content.get('fine', {})
                        if isinstance(fine_info, dict):
                            fine_text = fine_info.get('text', '')
                        else:
                            fine_text = str(fine_info)
                        
                        full_text = f"ì œì¬ì‚¬ì‹¤:\n{facts_text}\n\nì œì¬ë‚´ìš©: {content.get('sanction_type', '')}\n{fine_text}\n{content.get('executive_sanction', '')}"
                    text = full_text
                else:
                    text = str(content)
                
                # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                metadata = {
                    'institution': doc.get('institution', ''),
                    'doc_id': doc.get('doc_id', ''),
                }
                
                # ë¬¸ì„œ íƒ€ì… ì„¤ì •
                if "sanctions" in self.vector_db_path:
                    metadata['doc_type'] = 'ì œì¬ì •ë³´'
                    if isinstance(content, dict):
                        metadata['sanction_type'] = content.get('sanction_type', '')
                else:
                    metadata['doc_type'] = 'ê²½ì˜ìœ ì˜ì‚¬í•­'
                    if isinstance(content, dict):
                        metadata['management_type'] = content.get('management_type', '')
                
                # ë‚ ì§œ í•„ë“œ ì¶”ê°€
                if 'sanction_date' in doc:
                    metadata['sanction_date'] = doc['sanction_date']
                    metadata['date'] = doc['sanction_date']
                elif 'disclosure_date' in doc:
                    metadata['disclosure_date'] = doc['disclosure_date']
                    metadata['date'] = doc['disclosure_date']
                
                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                doc_metadata = doc.get('metadata', {})
                if isinstance(doc_metadata, dict):
                    # ê·œì • ì •ë³´ ì¶”ê°€
                    if 'regulations' in doc_metadata:
                        metadata['regulations'] = doc_metadata['regulations']
                    
                    # ê¸°íƒ€ ë©”íƒ€ë°ì´í„° ë³µì‚¬
                    for key, value in doc_metadata.items():
                        if key not in metadata and value:
                            metadata[key] = value
                
                if text.strip():  # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                    print(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ: {metadata['institution']} ({metadata['date']})")
                    documents.append(Document(page_content=text, metadata=metadata))
            
            if not documents:
                print("âŒ ë¬¸ì„œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            print(f"ğŸ“„ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            
            # FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            from langchain_community.vectorstores import FAISS
            self.vector_store = FAISS.from_documents(
                documents,
                self.embeddings
            )
            
            print("âœ… ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ì €ì¥ì†Œ ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return False


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì‚¬ìš©í•  ë²¡í„° DB ì„ íƒ
    db_type = input("ì‚¬ìš©í•  ë²¡í„° DBë¥¼ ì„ íƒí•˜ì„¸ìš” (1: ì œì¬ì •ë³´, 2: ê²½ì˜ìœ ì˜ì‚¬í•­): ")
    
    if db_type == "2":
        vector_db_path = "./data/vector_db/fss_management"
        print("ê²½ì˜ìœ ì˜ì‚¬í•­ ë²¡í„° DBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    else:
        vector_db_path = "./data/vector_db/fss_sanctions"
        print("ì œì¬ì •ë³´ ë²¡í„° DBë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # LLM ì„ íƒ
    use_anthropic = input("Anthropic Claude APIë¥¼ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower() == 'y'
    
    if use_anthropic:
        # API í‚¤ ì…ë ¥
        anthropic_api_key = input("Anthropic API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
        
        rag_system = FSSRagSystem(
            vector_db_path=vector_db_path,
            embed_model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            use_anthropic=True,
            anthropic_api_key=anthropic_api_key,
            top_k=5,
            use_openai_embeddings=False,  # ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©
            use_openai_llm=False
        )
    else:
        rag_system = FSSRagSystem(
            vector_db_path=vector_db_path,
            embed_model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            llm_model_name="gpt-3.5-turbo",
            top_k=5,
            use_openai_embeddings=False,  # ë¡œì»¬ ì„ë² ë”© ì‚¬ìš©
            use_openai_llm=True
        )
    
    # ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘
    rag_system.interactive_mode() 