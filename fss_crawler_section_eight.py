# ê¸ˆìœµê°ë…ì› í¬ë¡¤ë§
"""
ê¸ˆìœµê°ë…ì› ê²€ì‚¬ê²°ê³¼ì œì¬ ì „ìê¸ˆìœµ ê´€ë ¨ ë¬¸ì„œ í¬ë¡¤ëŸ¬
"""

import requests
import time
import os
import json
import re
import urllib.parse
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


def is_electronic_finance_related( content: str = "") -> bool:
    """ì „ìê¸ˆìœµ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨"""
    # ì „ìê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ
    content_keywords = [
        'ì „ìê¸ˆìœµ', 'í•€í…Œí¬', 'ì˜¨ë¼ì¸', 'ëª¨ë°”ì¼', 'ì¸í„°ë„·ë±…í‚¹', 'ì „ìì§€ê¸‰',
        'ì „ìê²°ì œ', 'API', 'ì˜¤í”ˆë±…í‚¹', 'ë§ˆì´ë°ì´í„°', 'ì „ìì„œëª…', 'ì •ë³´ë³´í˜¸',
        'ê°œì¸ì •ë³´', 'ì‚¬ì´ë²„', 'í•´í‚¹', 'ë³´ì•ˆ', 'ì „ì‚°', 'IT', 'ì‹œìŠ¤í…œ', 'ë„¤íŠ¸ì›Œí¬'
    ]
    
    # ë‚´ìš© í™•ì¸
    if content:
        for keyword in content_keywords:
            if keyword in content:
                return True
    
    return False


class FSSCrawler:
    """ê¸ˆìœµê°ë…ì› í¬ë¡¤ëŸ¬"""
    
    def __init__(self, base_dir: str = "../data/FSS_SANCTION"):
        self.base_url = "https://www.fss.or.kr"
        self.base_dir = Path(base_dir)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.lock = threading.Lock()
        
    def download_file(self, file_url: str, filename: str, save_dir: Path) -> Optional[Path]:
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        save_dir.mkdir(parents=True, exist_ok=True)
        filepath = save_dir / filename
        
        if filepath.exists():
            print(f"ğŸ“ ì´ë¯¸ ë‹¤ìš´ë¡œë“œë¨: {filepath}")
            return filepath
        
        try:
            resp = self.session.get(file_url, timeout=20)
            if resp.status_code == 200:
                with open(filepath, 'wb') as f:
                    f.write(resp.content)
                with self.lock:
                    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filepath}")
                return filepath
            else:
                with self.lock:
                    print(f"âŒ íŒŒì¼ ìš”ì²­ ì‹¤íŒ¨: {resp.status_code}")
        except Exception as e:
            with self.lock:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None
        
    def get_sanction_list(self, page: int = 1) -> Dict:
        """ê²€ì‚¬ê²°ê³¼ì œì¬ ëª©ë¡ ì¡°íšŒ"""
        url = f"{self.base_url}/fss/job/openInfo/list.do"
        params = {
            'menuNo': '200476',
            'pageIndex': page,
            'sdate': '2014-01-01',
            'edate': datetime.now().strftime('%Y-%m-%d'),
            'searchCnd': '4',
            'searchWrd': ''
        }
        
        try:
            resp = self.session.get(url, params=params, timeout=20)
            if resp.status_code != 200:
                print(f"âŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: ìƒíƒœì½”ë“œ {resp.status_code}")
                return {}
                
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ì „ì²´ ê±´ìˆ˜ ì¶”ì¶œ
            total_count = 0
            total_elem = soup.find(text=re.compile(r'ì´\s*\d+\s*ê±´'))
            if total_elem:
                match = re.search(r'(\d+)', str(total_elem))
                if match:
                    total_count = int(match.group(1))
            
            # í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            items = []
            rows = soup.select('table tbody tr')
            
            for row in rows:
                tds = row.find_all('td')
                if len(tds) >= 6:
                    item = {
                        'no': tds[0].get_text(strip=True),
                        'institution': tds[1].get_text(strip=True),
                        'date': tds[2].get_text(strip=True),
                        'views': tds[5].get_text(strip=True)
                    }
                    
                    # ìƒì„¸ ë§í¬ ì¶”ì¶œ
                    link_tag = tds[3].find('a')
                    if link_tag:
                        href = link_tag.get('href')
                        if href:
                            # URLì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                            parsed = urllib.parse.urlparse(href)
                            params = urllib.parse.parse_qs(parsed.query)
                            item['examMgmtNo'] = params.get('examMgmtNo', [''])[0]
                            item['emOpenSeq'] = params.get('emOpenSeq', [''])[0]
                            item['detail_url'] = urllib.parse.urljoin(self.base_url, href)
                    
                    items.append(item)
            
            return {
                'total_count': total_count,
                'items': items,
                'page': page
            }
            
        except Exception as e:
            print(f"âŒ ëª©ë¡ ì¡°íšŒ ì—ëŸ¬ (í˜ì´ì§€ {page}): {type(e).__name__} - {e}")
            return {}
    
    def get_sanction_detail_and_download(self, item: Dict, save_dir: str = "data/FSS_SANCTION") -> Dict:
        """ì œì¬ ìƒì„¸ ì •ë³´ ì¡°íšŒ ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        detail_url = item.get('detail_url')
        if not detail_url:
            return item
            
        try:
            resp = self.session.get(detail_url, timeout=20)
            if resp.status_code != 200:
                print(f"âŒ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: ìƒíƒœì½”ë“œ {resp.status_code}")
                return item
                
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ìƒì„¸ ì •ë³´ ì¶”ì¶œ
            detail = item.copy()
            
            # í…Œì´ë¸”ì—ì„œ ì •ë³´ ì¶”ì¶œ
            for table in soup.find_all('table', class_='tbl_view'):
                for tr in table.find_all('tr'):
                    th = tr.find('th')
                    td = tr.find('td')
                    if th and td:
                        key = th.get_text(strip=True)
                        value = td.get_text(' ', strip=True)
                        
                        if key == 'ì œì¬ëŒ€ìƒê¸°ê´€':
                            detail['institution'] = value
                        elif key == 'ì œì¬ì¡°ì¹˜ìš”êµ¬ì¼':
                            detail['date'] = value
                        elif key == 'ì œì¬ì¡°ì¹˜ìš”êµ¬ë‚´ìš©':
                            detail['content'] = value
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            save_path = self.base_dir / save_dir
            date_str = re.sub(r'[^\d]', '', detail.get('date', ''))[:8] or datetime.now().strftime('%Y%m%d')
            institution = re.sub(r'[^\w\s-]', '', detail.get('institution', 'unknown'))
            institution = institution.replace(' ', '_')[:30]
            
            download_count = 0
            downloaded_files = []
            
            # ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
            for a in soup.find_all('a', href=True):
                file_href = a['href']
                
                # ë‹¤ìš´ë¡œë“œ ë§í¬ íŒ¨í„´ í™•ì¸
                if '/fss.hpdownload' in file_href or 'download' in file_href.lower():
                    full_url = urllib.parse.urljoin(self.base_url, file_href)
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ
                    name_tag = a.find('span', class_='name')
                    if name_tag:
                        filename = name_tag.get_text(strip=True)
                    else:
                        # URLì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ ì‹œë„
                        filename_match = re.search(r'file=([^&]+)', file_href)
                        if filename_match:
                            filename = urllib.parse.unquote(filename_match.group(1))
                        else:
                            filename = a.get_text(strip=True)
                    
                    # íŒŒì¼ëª… ì •ë¦¬
                    if not filename or filename == 'ë‚´ìš©ë³´ê¸°':
                        continue
                        
                    # í™•ì¥ì í™•ì¸
                    if not any(filename.lower().endswith(ext) for ext in ['.pdf', '.hwp', '.hwpx']):
                        # í™•ì¥ìê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                        if '.' not in filename:
                            filename += '.pdf'
                        else:
                            continue
                    
                    # íŒŒì¼ëª… ìƒì„±
                    safe_filename = f"SANCTION_{date_str}_{institution}_{detail.get('examMgmtNo', 'unknown')}_{download_count+1}_{filename}"
                    safe_filename = re.sub(r'[^\w\s.-]', '_', safe_filename)
                    
                    # ë‹¤ìš´ë¡œë“œ
                    filepath = self.download_file(full_url, safe_filename, save_path)
                    if filepath:
                        downloaded_files.append(str(filepath))
                        download_count += 1
                    
                    time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ (ë‹¨ì¶•)
            
            detail['downloaded_files'] = downloaded_files
            detail['download_count'] = download_count
            
            # ìƒì„¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
            json_filename = f"SANCTION_{date_str}_{institution}_{detail.get('examMgmtNo', 'unknown')}.json"
            json_path = save_path / json_filename
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(detail, f, ensure_ascii=False, indent=2)
            
            return detail
            
        except Exception as e:
            print(f"âŒ ìƒì„¸ ì¡°íšŒ ì—ëŸ¬: {type(e).__name__} - {e}")
            return item
    
    def get_existing_sanctions(self, save_dir: str = "data/FSS_SANCTION") -> set:
        """ê¸°ì¡´ ì €ì¥ëœ ì œì¬ ì •ë³´ í™•ì¸"""
        existing = set()
        save_path = self.base_dir / save_dir
        
        if save_path.exists():
            for file in save_path.glob("*.json"):
                if file.name.startswith("SANCTION_"):
                    # íŒŒì¼ëª…ì—ì„œ examMgmtNo ì¶”ì¶œ
                    match = re.search(r'_(\d{9,})\.json', file.name)
                    if match:
                        existing.add(match.group(1))
        
        return existing
    
    def process_item_parallel(self, item: Dict, existing: set) -> Optional[Dict]:
        """ê°œë³„ í•­ëª© ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        # ì´ë¯¸ ì €ì¥ëœ í•­ëª©ì€ ê±´ë„ˆë›°ê¸°
        if item.get('examMgmtNo') and item.get('examMgmtNo') in existing:
            return None
        
        with self.lock:
            print(f"\nğŸ” ì²˜ë¦¬ ì¤‘: {item['institution']} ({item['date']})")
        
        # ìƒì„¸ ì •ë³´ ì¡°íšŒ ë° ë‹¤ìš´ë¡œë“œ
        detail = self.get_sanction_detail_and_download(item)
        
        if detail.get('download_count', 0) > 0:
            with self.lock:
                print(f"ğŸ“¥ {detail['download_count']}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
        else:
            with self.lock:
                print("ğŸ“ ì²¨ë¶€íŒŒì¼ ì—†ìŒ")
        
        file_info = {
            'type': 'ê²€ì‚¬ê²°ê³¼ì œì¬',
            'examMgmtNo': item.get('examMgmtNo'),
            'institution': detail.get('institution', ''),
            'date': detail.get('date', ''),
            'downloaded_files': detail.get('downloaded_files', []),
            'timestamp': datetime.now().isoformat()
        }
        
        return file_info
    
    def crawl_electronic_finance_sanctions(self) -> List[Dict]:
        """ì „ìê¸ˆìœµ ê´€ë ¨ ì œì¬ ì •ë³´ í¬ë¡¤ë§"""
        new_files = []
        
        print(f"=== ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ì œì¬ í¬ë¡¤ë§ ì‹œì‘: {datetime.now()} ===")
        
        try:
            # ê¸°ì¡´ íŒŒì¼ í™•ì¸
            existing = self.get_existing_sanctions()
            print(f"ê¸°ì¡´ ì œì¬ ì •ë³´: {len(existing)}ê°œ")
            
            page = 1
            total_electronic = 0
            consecutive_empty = 0
            
            while True:
                print(f"\nğŸ“„ {page}í˜ì´ì§€ ì¡°íšŒ ì¤‘...")
                result = self.get_sanction_list(page)
                
                if not result or not result.get('items'):
                    consecutive_empty += 1
                    if consecutive_empty >= 3:
                        print("ğŸ“­ ì—°ì† 3í˜ì´ì§€ ë¹ˆ ê²°ê³¼ - í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                    page += 1
                    continue
                
                consecutive_empty = 0
                items = result['items']
                print(f"  - {len(items)}ê°œ í•­ëª© ë°œê²¬")
                
                page_electronic = 0
                
                # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í•­ëª©ë“¤ ì²˜ë¦¬
                with ThreadPoolExecutor(max_workers=3) as executor:
                    future_to_item = {
                        executor.submit(self.process_item_parallel, item, existing): item 
                        for item in items
                    }
                    
                    for future in as_completed(future_to_item):
                        try:
                            file_info = future.result()
                            if file_info:
                                new_files.append(file_info)
                                page_electronic += 1
                                total_electronic += 1
                        except Exception as e:
                            item = future_to_item[future]
                            print(f"âŒ í•­ëª© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ({item.get('institution', 'unknown')}): {e}")
                
                time.sleep(0.5)  # í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ ë‹¨ì¶•
                
                print(f"  â†’ ì´ í˜ì´ì§€ì—ì„œ {page_electronic}ê°œ ì „ìê¸ˆìœµ ì œì¬ ë°œê²¬")
                
                # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
                total_count = result.get('total_count', 0)
                if total_count > 0 and page * 10 >= total_count:
                    print(f"\nâœ… ëª¨ë“  í˜ì´ì§€ ì¡°íšŒ ì™„ë£Œ (ì´ {total_count}ê±´)")
                    break
                
                page += 1
                
                # ì•ˆì „ì¥ì¹˜
                if page > 500:  # ìµœëŒ€ 500í˜ì´ì§€ê¹Œì§€ë§Œ
                    print("âš ï¸ 500í˜ì´ì§€ ì´ˆê³¼, ì•ˆì „ ì¤‘ë‹¨")
                    break
            
            print(f"\n=== í¬ë¡¤ë§ ì™„ë£Œ ===")
            print(f"ğŸ”µ ìƒˆë¡œìš´ ì „ìê¸ˆìœµ ì œì¬ ì •ë³´: {len(new_files)}ê°œ")
            print(f"ğŸ“Š ì „ì²´ ì „ìê¸ˆìœµ ì œì¬: {total_electronic}ê°œ")
            
            return new_files
            
        except KeyboardInterrupt:
            print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return new_files
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬: {type(e).__name__} - {e}")
            return new_files
    
    def save_crawl_log(self, new_files: List[Dict]) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ ë¡œê·¸ ì €ì¥"""
        if not new_files:
            return
        
        log_dir = self.base_dir / "crawl_logs"
        log_dir.mkdir(exist_ok=True)
        
        today = datetime.now().strftime("%Y%m%d")
        log_file = log_dir / f"fss_sanctions_{today}.json"
        
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(new_files, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“‹ í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥: {log_file}")


if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = FSSCrawler()
    
    # ì „ìê¸ˆìœµ ê´€ë ¨ ì œì¬ ì •ë³´ í¬ë¡¤ë§
    new_files = crawler.crawl_electronic_finance_sanctions()
    
    # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
    crawler.save_crawl_log(new_files)
    
    print("\nâœ¨ ê¸ˆìœµê°ë…ì› ì „ìê¸ˆìœµ ì œì¬ í¬ë¡¤ë§ ì™„ë£Œ!")
